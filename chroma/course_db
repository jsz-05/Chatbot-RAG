{"raw_webcrawl_data/table_of_contents.html": {"0": "<h3 class=\"w3-text-teal\">Contents</h3>\n", "1": "<h4 class=\"w3-text-teal\">Introduction</h4>\n<ol>\n<li>\n<a href=\"index.html\">\n    This Website\n    </a>\n</li>\n</ol>\n", "2": "<h4 class=\"w3-text-teal\">Distributed Systems: Basics</h4>\n<ol>\n<li>\n<a href=\"DISTRIBUTED_SYSTEM_MODELS/Basics.html\">\n      Model and Notation\n      </a>\n<ol>\n<li>\n<a href=\"DISTRIBUTED_SYSTEM_MODELS/BasicsFAQ.html\">\n\tFAQ\n\t</a>\n</li>\n<li>\n<a href=\"DISTRIBUTED_SYSTEM_MODELS/BasicsReview.html\">\n\tReview\n\t</a>\n</li>\n</ol>\n</li>\n<li>\n<a href=\"DISTRIBUTED_SYSTEM_MODELS/Model.html\">\n      Timelines and Dataflow\n      </a>\n<ol>\n<li>\n<a href=\"DISTRIBUTED_SYSTEM_MODELS/ModelFAQ.html\">\n\tFAQ\n\t</a>\n</li>\n<li>\n<a href=\"DISTRIBUTED_SYSTEM_MODELS/ModelReview.html\">\n\tReview\n\t</a>\n</li>\n</ol>\n</li>\n<li>\n<a href=\"DISTRIBUTED_SYSTEM_MODELS/Computations.html\">\n      Computations\n      </a>\n<ol>\n<li>\n<a href=\"DISTRIBUTED_SYSTEM_MODELS/ComputationsFAQ.html\">\n\tFAQ\n\t</a>\n</li>\n<li>\n<a href=\"DISTRIBUTED_SYSTEM_MODELS/ComputationsReview.html\">\n\tReview\n\t</a>\n</li>\n</ol>\n</li>\n<li>\n<a href=\"DISTRIBUTED_SYSTEM_MODELS/Timelines.html\">\n      Past, Future and Cuts in Dataflow\n      </a>\n<ol>\n<li>\n<a href=\"DISTRIBUTED_SYSTEM_MODELS/TimelinesProofs.html\">\n\tProofs\n\t</a>\n</li>\n<li>\n<a href=\"DISTRIBUTED_SYSTEM_MODELS/TimelinesFAQ.html\">\n\tFAQ\n\t</a>\n</li>\n<li>\n<a href=\"DISTRIBUTED_SYSTEM_MODELS/TimelinesReview.html\">\n\tReview\n\t</a>\n</li>\n</ol>\n</li>\n</ol>\n", "3": "<h4 class=\"w3-text-teal\">Snapshots and Clocks</h4>\n<ol>\n<li>\n<a href=\"ChannelSnapshots/ChannelSnapshots.html\">\n      Global snapshots\n      </a>\n<ol>\n<li>\n<a href=\"ChannelSnapshots/ChannelSnapshotsDetails.html\">\n\tGlobal Snapshots Details\n\t</a>\n</li>\n<li>\n<a href=\"ChannelSnapshots/ChannelSnapshotsFAQ.html\">\n\tFAQ\n\t</a>\n</li>\n<li>\n<a href=\"ChannelSnapshots/ChannelSnapshotsReview.html\">\n\tReview\n\t</a>\n</li>\n</ol>\n</li>\n<li>\n<a href=\"ChannelSnapshots/LogicalClocks.html\">\n      Logical Clocks\n      </a>\n<ol>\n<li>\n<a href=\"ChannelSnapshots/LogicalClocksSnapshots.html\">\n\tGlobal Snapshot at Logical Time \\(t\\)\n\t</a>\n</li>\n</ol>\n</li>\n<li>\n<a href=\"ChannelSnapshots/TerminationDetection.html\"> \n      Detecting Termination\n      </a>\n</li>\n<li>\n<a href=\"ChannelSnapshots/DatabaseDeadlockDetection.html\"> \n      Detecting Database Deadlock\n      </a>\n</li>\n</ol>\n", "4": "<h4 class=\"w3-text-teal\">Consensus Algorithms</h4>\n<ol>\n<li>\n<a href=\"Paxos/ConsensusImpossible.html\">\n      Impossibility of asynchronous consensus with faulty agents\n      </a>\n</li>\n<li>\n<a href=\"Paxos/StableMajority.html\">\n      The Paxos algorithm: Part 1\n      </a>\n</li>\n<li>\n<a href=\"Paxos/ReadWriteLossyChannels.html\">\n      The Paxos algorithm: Part 2\n      </a>\n</li>\n<li>\n      The Raft algorithm\n      </li>\n</ol>\n", "5": "<h4 class=\"w3-text-teal\">Self stabilization</h4>\n<ol>\n<li>\n<a href=\"SelfStabilization/SelfStabilization.html\">\n      Self stabilization of a token ring\n      </a>\n</li>\n</ol>\n", "6": "<h4 class=\"w3-text-teal\">Byzantine faults</h4>\n    Algorithms dealing with Byzantine faults\n    <ol>\n<li>\n<a href=\"Byzantine/ByzantineWritten.html\">\n      Byzantine algorithms with written messages\n      </a>\n</li>\n<li>\n<a href=\"Byzantine/ByzantineOral.html\">\n      Byzantine algorithms with oral messages\n      </a>\n</li>\n</ol>\n", "7": "<h4 class=\"w3-text-teal\">Cryptocurrency</h4>\n<ol>\n<li>\n<a href=\"Crypto/CryptoCurrencyIntroduction.html\">\n      Introduction to cryptographic hash functions.\n      </a>\n</li>\n<li>\n<a href=\"Crypto/BitCoin.html\">\n      BitCoin algorithm\n      </a>\n</li>\n<li>\n      Etherium algorithm\n      </li>\n</ol>\n", "8": "<h4 class=\"w3-text-teal\">Agent knowledge</h4>\n<ol>\n<li>\n<a href=\"Knowledge/Knowledge.html\">\n      How processes learn\n      </a>\n</li>\n</ol>\n", "9": "<h4 class=\"w3-text-teal\">Diffusing and gossip computations</h4>\n<ol>\n<li>\n<a href=\"DiffusingComputations/DiffusingComputations.html\"> \n      The Dijkstra-Scholten diffusing computation algorithm\n      </a>\n</li>\n</ol>\n", "10": "<h4 class=\"w3-text-teal\">Distributed Collaboration</h4>\n<ol>\n<li>\n<a href=\"DistributedCollaboration/DiningPhilosophers/DiningPhilosophers.html\">\n      Mutual Exclusion: Distributed Dining philosophers\n      </a>\n</li>\n<li>\n<a href=\"DistributedCollaboration/DrinkingPhilosophers/DrinkingPhilosophers.html\">\n      Distributed Resource Management: Drinking philosophers\n      </a>\n</li>\n<li>\n      Rendezvous in Distributed Systems: Committee Coordination\n      </li>\n</ol>\n", "11": "<h4 class=\"w3-text-teal\">Clock synchronization</h4>\n<ol>\n<li>\n      Network and Precision Time Protocols\n      </li>\n<li>\n      Clock synchronization in distributed systems\n      </li>\n</ol>\n", "12": "<h4 class=\"w3-text-teal\">Cloud-computing algorithms</h4>\n<ol>\n<li>\n      Map-reduce\n      </li>\n<li>\n      Millwheel\n      </li>\n</ol>\n", "13": "<h4 class=\"w3-text-teal\">Stream algorithms</h4>\n<ol>\n<li>\n      Finding repeated elements in streams: Misra &amp; Gries\n      </li>\n<li>\n      Finding repeated elements in streams: Approximate algorithms\n      </li>\n</ol>\n", "14": "<h4 class=\"w3-text-teal\">Distributed data structures</h4>\n<ol>\n<li>\n      Distributed tables: hashing\n      </li>\n<li>\n      Distributed data: BigTable\n      </li>\n</ol>\n", "15": "<h4 class=\"w3-text-teal\">Continuous distributed systems</h4>\n<ol>\n<li>\n      Asynchronous averaging\n      </li>\n</ol>\n", "16": "<h4 class=\"w3-text-teal\">Review: Sequential Programming Basics</h4>\n    Review of basic concepts from sequential algorithms.\n    <ol>\n<li>\n<a href=\"ReviewOfBasics/Introduction/Introduction.html\">\n      Basics: Introduction\n      </a>\n</li>\n<li>\n<a href=\"ReviewOfBasics/NondeterministicIteration/NondeterministicIteration.html\">\n      Nondeterministic iteration\n      </a>\n</li>\n<li>\n<a href=\"ReviewOfBasics/Predicates/Predicates.html\">\n      Predicates\n      </a>\n</li>\n<li>\n<a href=\"ReviewOfBasics/HoareTriples/HoareTriples.html\">\n      Hoare triples\n      </a>\n</li>\n<li>\n<a href=\"ReviewOfBasics/Invariant/Invariant.html\">\n      States &amp; Invariant\n      </a>\n</li>\n<li>\n<a href=\"ReviewOfBasics/LoopTermination/LoopTermination.html\">\n      Proving termination.\n      </a>\n</li>\n<li>\n<a href=\"ReviewOfBasics/ExamplesCorrectnessProofs/ExamplesCorrectnessProofs.html\">\n      Examples: correctness.\n      </a>\n<ol>\n<li>\n<a href=\"ReviewOfBasics/ExamplesCorrectnessProofs/Sorting/Sorting.html\">\n      Sorting\n      </a>\n</li>\n<li>\n<a href=\"ReviewOfBasics/ExamplesCorrectnessProofs/ShortestPath/ShortestPath.html\">\n      Shortest paths in a graph\n      </a>\n</li>\n<li>\n<a href=\"ReviewOfBasics/ExamplesCorrectnessProofs/AverageOfSensors/AverageOfSensors.html\">\n      Distributed averaging\n      </a>\n</li>\n<li>\n<a href=\"ReviewOfBasics/ExamplesCorrectnessProofs/FlightFormation/FlightFormation.html\">\n      Agents forming patterns in flight\n      </a>\n</li>\n<li>\n<a href=\"ReviewOfBasics/ExamplesCorrectnessProofs/EarliestMeetingTime/EarliestMeetingTime.html\">\n      Earliest meeting time of concurrent agents\n      </a>\n</li>\n</ol>\n</li></ol>\n"}, "raw_webcrawl_data/cross_reference.html": {"0": "<h2 class=\"w3-text-teal\">Index</h2>\n"}, "raw_webcrawl_data/Crypto/CryptoCurrencyIntroduction.html": {"0": "<h1 class=\"w3-text-teal\">Introduction to Cryptography for\nCryptocurrency</h1>\n", "1": "<h4 class=\"w3-text-teal\">\nThis module contains a review of elementary cryptographic operations\nand introduces a simple cryptocurrency managed by a trusted agent.\n</h4>\n\nThe next module discusses the algorithm underlying Bitcoin; the\nBitcoin algorithm doesn't require agents to be trusted.\n    \n    <a href=\"https://www.lopp.net/pdf/princeton_bitcoin_book.pdf\">\n    This Princeton University book is a superb description of\nBitcoin.</a>\n", "2": "<h2 class=\"w3-text-teal\">Review: Cryptographic Hash Function</h2>\n    A  hash function, \\(H\\), maps input strings of\n    arbitrary size to outputs of fixed size.\n\n", "3": "<h3 class=\"w3-text-teal\">Collision Resistance</h3>\n<hr class=\"new2\"/>\n", "4": "<h5 style=\"color:blue;\">\n    Input values \\(x, y\\) of a hash function \\(H\\)\nare said to <i>collide</i> when \\(H(x) = H(y)\\).\nA hash function \\(H\\) is said to be <i>collision resistant</i> if \nthe only known ways of\nfinding collisions using the hash are intractable.\n</h5>\n<hr class=\"new2\"/>\n<p>\n    Let's look at the following problem: Given \\(H\\), find any\n    colliding pair \\(x, y\\).\n\n    </p><p>\n    Consider a hash function \\(H\\) that outputs \\(n\\)-bit numbers and\n    whose input is \\(m\\) bit strings. As a specific example lets\n    assume that \\(m\\) is a large number and \\(n = 4\\). We can\n    find a collision in the following way.\n    </p><p>\n    Let \\(D\\) be an\n    array of size \\(2^{n} = 16\\). Initially \\(D\\) contains null values. Repeat the following\n    iteration until a collision is found.\nPick a random input \\(x\\).\nIf \\(D[H(x)]\\) is null then set \\(D[H(x)] = x\\) else there is a collision between\n\\(H(x)\\) and \\(x\\). \n\n    </p><p>\n    By the pigeon-hole principle, we will find a collision in at most\n    \\(2^{n} + 1 = 17\\) iterations.\n\n    This  brute-force algorithm uses space \\(2^{n}\\) and finds a\n    collision in at most  \\(2^{n} + 1\\) steps.\n\nFrom the <a href=\"https://en.wikipedia.org/wiki/Birthday_problem\">\nBirthday Paradox</a> a collision will be found with high probability\nin \\(2^{n/2}\\) iterations though\nthe worse-case time is \\(2^{n}+1\\). If \\(n = 256\\) then a collision\nwill be found with high probability in \\(2^{128}\\) iterations; \nhowever, executing \\(2^{128}\\) steps is still intractable.\n\n</p><p>\nFor \\(H\\) to be\n    collision resistant the output of \\(H\\) \n    must be \\(n\\)-bits for large \\(n\\). For example \\(n = 256\\) in the\n    SHA-256 hash function. \n    \n\n    </p>", "5": "<h3 class=\"w3-text-teal\">Commitment using Hashes</h3>\n    You bet that your soccer friend, Megan, cannot predict the winner\n    of the 2022 World Cup. Megan puts the name of the predicted\n    winner, \\(W\\), in an envelope and\n    gives it to a trusted third party. After the World Cup is over,\n    the third party reveals Megan's prediction, and at that point you\n    can find out whether Megan's prediction was accurate.\n    <p>\n    The trusted third party provides two services:\n</p><ol>\n<li>\n<i>Hiding</i>: You can't find out Megan's prediction until the third party reveals it.\n  </li>\n<li>\n<i>Binding</i>: Megan can't change her prediction after giving it to the third\n  party.\n  </li>\n</ol>\n<p>\nCan we use a hash function instead of a trusted third party?\n\n</p>", "6": "<h3 class=\"w3-text-teal\">Hiding</h3>\n\nLet's try the following idea. Megan commits to \\(W\\) in the following\nway. She announces a hash function, \\(H\\),\nand the hash, \\(y\\), where \\(y = H(W)\\). You know \\(H\\) and \\(y\\). \nAfter the World Cup is over, she reveals her prediction, \\(W\\). At\nthis point you can verify that \\(y = H(W)\\).\n<p>\nDoes the hash function provide the services of the third party?\nCan you discover Megan's prediction before she reveals it?\n\n\n    </p><p>\nIt's easy. There are only 32 teams playing. Compute \\(H(x)\\) where\n    \\(x\\) runs over each of the 32 teams. One of those teams has to be\n\\(W\\). You can discover her prediction in at most 210 steps.\n\n</p><p>\nLet's try another algorithm.  Megan selects a secret value \\(r\\) which\nshe keeps to herself. Instead\nof giving you \\(H(W)\\), she gives you \\(y\\) where \\(y = H(r + W)\\) and\nwhere \\(+\\) indicates concatenation of strings. Can\nyou discover \\(W\\) from \\(H\\) and \\(y\\) without knowing \\(r\\)?\n\n</p><p>\nA brute-force solution is to try every combination of \\(r\\) and\n\\(W\\). If \\(r\\) is obtained from a distribution that is spread out,\nthen finding \\(W\\) without knowing \\(r\\)  take so\nmuch time that it is practically impossible.\n\n</p><hr class=\"new2\"/>\n", "7": "<h5 style=\"color:blue;\">\nHiding: Given \\(H, y\\), where \\(y = H(r + W)\\) and \\(r\\) is a secret,\ndiscovering \\(W\\) is intractable.\n</h5>\n<hr class=\"new2\"/>\n", "8": "<h3 class=\"w3-text-teal\">Binding</h3>\n<p>\nDoes the hash function \\(H\\) and the secret \\(r\\) provide both\nservices of the trusted third party? Is Megan bound to her prediction\nor can she change her \"prediction\" after knowing the winner of the\nWorld Cup? \n\n</p><p>\nSuppose Megan has values \\(r\\) and \\(r'\\) such that H(r + 'Brazil') =\nH(r' + 'Italy'). After the World Cup is over, she can announce that\nher secret is \\(r\\) if Brazil wins, and announce that it is \\(r'\\) if\nItaly wins.\n\n</p><hr class=\"new2\"/>\n", "9": "<h5 style=\"color:blue;\">\nA hash function \\(H\\) is <i>binding</i> if finding pairs \\((x,y)\\) and \\((x',y')\\)\nwhere \\(y \\neq y'\\) such that:\n\\(H(x + y) = H(x' + y')\\)\nis intractable.\n</h5>\n<hr class=\"new2\"/>\n<p>\nSuppose you give Megan a hash function that is binding. Then she cannot find\n  (in reasonable time) values \\(r_{j}\\) to match country \\(C_{j}\\)\nsuch that\n</p><p>\n\\(H(r_{0} + C_{0}) = H(r_{1} + C_{1})  = H(r_{2} + C_{2})  = \\ldots\\).\n</p><p>\nand so she can't wait for the winner \\(C_{j}\\) to be announced before announcing\nher secret \\(r_{j}\\).\n</p><p>\nIn summary, we can use a hash function that is hiding and binding to\nplay the role of a trusted third party in a commitment.\n\n</p>", "10": "<h3 class=\"w3-text-teal\">Puzzle Friendly</h3>\nThe concept of <i>puzzle friendly</i> is related to\n<i>hiding</i>.\nLet \\(r\\) be a value picked from a spread-out distribution. Let\n\\(H\\) map arbitrary length strings to \\(n\\)-bit strings.\nConsider the following problem:\n    Given \\(H\\), \\(r\\), and an \\(n\\)-bit value \\(y\\), compute any \\(x\\) such that\n\\(H(r+x) = y\\).\n<p>\nIn this problem, as opposed to the <i>hiding</i> problem, we are given \\(r\\) and not \\(x\\),\n</p><p>\nThe hash function \\(H\\) is said to be <i>puzzle friendly</i>\n    exactly when any algorithm to solve this problem is about as slow\nas a brute-force algorithm which checks \\(H(r+x) = y\\) for random values\nof \\(x\\).\nThe number of steps taken by any\nalgorithm that solves this problem is not significantly lower than\n\\(2^{n}\\).\n\n</p><p>\nNow, let's look at the following related problem.  Given \\(H\\), \\(r\\),\nand a set \\(Y\\) of \\(n\\)-bit strings, compute any \\(x\\) such that\n\\(H(r+x) \\in Y\\).  If \\(Y\\) consists of a single element \\(y\\) then\nthis problem is the same as that in the previous paragraph.  If \\(Y\\)\nis a set of all \\(n\\)-bit strings then this problem is trivial because\nany \\(x\\) solves the problem.  The probability that a random value\nhashes to an element of \\(Y\\) is proportional to the cardinality of\n\\(Y\\).  The cardinality of \\(Y\\) controls the expected time to\nsolve the puzzle.\n\n</p><hr class=\"new2\"/>\n", "11": "<h5 style=\"color:blue;\">\nThe hash function \\(H\\) is <i>puzzle friendly</i> when\ngiven \\(H\\), \\(r\\), and a set \\(Y\\) of \\(n\\)-bit values, the time\nrequired to compute any \\(x\\) such that \\(H(r+x) \\in Y\\) is\nnot significantly lower than \\(2^{n} / |Y|\\), where \\(|Y|\\) is the\ncardinality of \\(Y\\).\n</h5>\n<hr class=\"new2\"/>\n", "12": "<h3 class=\"w3-text-teal\">A Cryptographic Hash Function</h3>\n<hr class=\"new2\"/>\n", "13": "<h5 style=\"color:blue;\">\nA cryptographic hash function is one that is collision resistant,\nhiding and puzzle-friendly.\n</h5>\n<hr class=\"new2\"/>\n", "14": "<h3 class=\"w3-text-teal\">Hashing Inputs of Arbitrary Length</h3>\nLet \\(f\\) be a function that operates on input strings\nof fixed length and produces output strings of fixed length. Let the\ninput and output strings of \\(f\\) have lengths \\(M + N\\) and \\(M\\),\nrespectively. We look at functions where \\(N &gt; 0\\), and since the\noutput is smaller than the input, \\(f\\) is called a <i>compression\nfunction.</i>\n<p>\nWe can use function \\(f\\) to define a function \\(g\\) whose inputs are strings of arbitrary\nlengths and whose outputs are strings of length \\(M\\).\nExample code for \\(g\\) is given below where <code>InitialValue</code>\nis a given constant string of length \\(M\\).\n\n</p><pre>\ndef g(y):\n    output = InitialValue\n\n    // pad y so that it's length is a multiple of N\n    if len(y)%N &gt; 0:  y = y + \"0\"*(N - len(y)%N)\n\n    // partition y into blocks of size N\n    blocks = [y[i: i+N] for i in range(0, len(y), N)]\n\n    // Apply function f to the concatenation of the\n    // previous output (length M) with each block\n    // (length N) to get the next output (length M).\n    for block in blocks: output = f(output+block)\n    return output\n</pre>\n", "15": "<h3 class=\"w3-text-teal\">Hash Pointers</h3>\n<hr class=\"new2\"/>\n", "16": "<h5 style=\"color:blue;\">\nA hash pointer to an item \\(D\\) of data is a pair \\((ptr, H(D))\\)\nwhere \\(ptr\\) is a pointer that \npoints to \\(D\\), and \\(H\\) is a cryptographic hash function.\n</h5>\n<hr class=\"new2\"/>\n\n\nAny data structure with pointers can be converted into a\ndata structure with hash pointers: merely replace a pointer \\(ptr\\) to\n\\(D\\) by \\((ptr, H(D))\\).\n\n", "17": "<h1 class=\"w3-text-teal\">Tamper-Evident Data Structures</h1>\n<p class=\"w3-text-teal\">Single Block</p>\nA simple example of a tamper-evident structure is a single block of\ndata D which is pointed to by a hash pointer consisting of a regular\npointer and a hash H(D).\n\n    <figure>\n<img alt=\"Fig1\" src=\"raw_webcrawl_data/Crypto/CryptoCurrencyIntroduction/Slide1.jpg\" style=\"width:100%\"/>\n<figcaption>Fig.1: Hash Pointer points to a Tamper-Evident Block\n    of Data</figcaption>\n</figure>\nAssume that a malicious agent cannot modify <i>both</i> the hash\npointer and the data that it points to. If an \nagent\n    changes D to D' then the tampering can be discovered because the\n    hash pointer won't match the data that it is pointing to:\n    \\(H(D') \\neq H(D)\\).\n\n", "18": "<h4 class=\"w3-text-teal\">Tamper-Evident Linked List</h4>\nLet's look at linear linked list to which elements can be appended but\nnot deleted. The \\(i\\)-th element appended to the list points to the\n\\((i-1)\\)-th element. \nLet's replace the pointers in the list by hashed\npointers.\n\n    <figure>\n<img alt=\"Fig1\" src=\"raw_webcrawl_data/Crypto/CryptoCurrencyIntroduction/Slide2.jpg\" style=\"width:100%\"/>\n<figcaption>Fig.2: Hash Pointer points to a Tamper-Evident List\n    of Data</figcaption>\n</figure>\n<p>\nThe \\(j\\)-th element of the list, \\(j &gt; 0\\), consists of data,\n\\(D_{j}\\), and a hash pointer that points to the \\(j - 1\\)-th\n  element of the list. The hash pointer in the \\(j\\)-th element\nconsists of a regular pointer, \\(ptr_{j}\\), which points to the \\(j - 1\\)-th\n  element of the list, and a hashed value, \\(HA_{j}\\), which is a cryptographic hash of the\n  <i>entire</i> \\(j-1\\)th element consisting  of \\(D_{j-1}\\), \\(ptr_{j-1}\\)\n  and \\(HA_{j-1}\\).\n\nThe \\(0\\)-th element is called the <i>genesis</i> element and has\ndefault values. The list is accessed by a hash pointer that points to\nthe last element of the list; let this pointer be \\(ptr_{n}, HA_{n}\\).\n\n</p><p>\nAssume that agents cannot modify \\(ptr_{n}, HA_{n}\\). Then,\ncan any agent determine whether the list has been tampered with?\n\n</p><p>\nSuppose the agent modifies \\(D_{j}\\), \\(ptr_{j}\\) or\n\\(HA_{j}\\) for any \\(j &lt; n\\). Any agent can detect this tampering because the\nhash value \\(HA_{j+1}\\) will no longer match the \\(j\\)-th element of\nthe list. If\nthe malicious agent also modifies \\(HA_{j+1}\\) then \nhash value \\(HA_{j+2}\\) will no longer match the \\(j+1\\)-th element.\n\n</p><p>By induction on \\(j\\), any agent can detect tampering with the list\nprovided malicious agents do not also modify the hash pointer to the\nlast element of the last.\n\n\n</p>", "19": "<h5 class=\"w3-text-teal\">Tamper-Evident Acyclic Graphs and Merkle\nTrees</h5>\nThe idea described in the previous paragraph to convert linear linked\nlists can be used to convert directed acyclic graphs, in which nodes are\nconnected by pointers, into tamper-evident graphs. A specific case of\na directed acyclic graph is a rooted tree.\n\n<p>\nA <i>Merkle tree</i> is a special case of a binary balanced tree in which\ndata items are stored only in the leaves.\nNodes that are not leaves \ncontain only hash pointers to nodes in the next level down.\nTo prove that an element at the leaf is a member of the tree we need only the\n\\(log_{2}(n)\\) hash pointers on the path from the root to that\nleaf. By contrast, to prove that an element is a member of a linear\nlist we need to inspect \\(O(n)\\) elements, on average.\n\n</p>", "20": "<h3 class=\"w3-text-teal\">Keys and Signed Messages</h3>\nYou can create a random public-key, private-key pair \n<a href=\"https://en.wikipedia.org/wiki/Ssh-keygen\">\nby calling a function on your computer.</a>\nWith high probability, nobody else has this\nspecific pair of keys. Each individual's private key is a secret held\nby that individual. Public keys are accessible by everybody.\n\n<p class=\"w3-text-teal\">Sending messages securely</p>\nKeys are used to send messages securely.\nKamala sends a secure message to Joe by encrypting the message with\nJoe's public key; Joe decrypts the message using Joe's private key.\nAn agent cannot decrypt the encrypted message without Joe's private key.\n\n<p class=\"w3-text-teal\">Signing messages</p>\nSuppose Kamala needs to send a signed message to Joe while\nensuring that nobody can forge her signature. She encrypts the message\n<i>M</i> with her private key to get an encrypted message <i>M'</i>,\nand sends the pair <i>(M, M')</i> securely to Joe, i.e., she encrypts\n<i>(M, M')</i> with Joe's public key, and sends the resulting\nencrypted message to Joe.  When Joe receives the message, Joe decrypts\nit using his private key to get <i>(M, M')</i>. Then Joe decrypts\n<i>M'</i> using Kamala's public key to get the decrypted message\n<i>M''</i>. If <i>M'' = M</i> then Joe knows that Kamala sent\n<i>M</i> because only an agent with Kamala's private signature could\nhave sent that message.\n\n\n\n", "21": "<h2 class=\"w3-text-teal\">Cryptocurrency Managed by a Trusted\nAgent</h2>\nLet's start with a digital currency managed by a trusted agent that we will\ncall a bank. Later, we will\nlook at a consensus algorithm --- very different from Paxos and Byzantine\nGenerals --- which will allow cryptocurrencies without trusted\nagent.\n\n\n<hr class=\"new2\"/>\n", "22": "<h5 style=\"color:blue;\">\nThe bank maintains a tamper-evident linear list\n<code>L</code> of transactions that we call a\ntamper-evident <i>ledger</i>. \nAny agent can get a copy of the ledger.\nThis tamper-evident ledger is the foundation of the currency.\n</h5>\n<hr class=\"new2\"/>\n<p>\nA transaction is one of\ntwo types: <i>create</i> or <i>pay</i>.\nIn a pay transaction, <i>payers</i> give coins that they possess to\n<i>payees</i>. An agent can be both a \npayer and payee of the same transaction.\nIn a <i>create</i> transaction the bank creates coins that it gives to\nagents --- the payees of the transaction; the bank acts as the payer.\n\n</p><p>\nFor this system to be trusted the bank must follow some protocol that determines\nhow and when the bank creates coins. We won't discuss these protocols.\n\n</p><p>\nA <i>pay</i> transaction is signed by all payers of the transaction.\nA <i>create</i> transaction is signed by the bank.\nWe discussed digital signatures and keys earlier.\n\n</p><p>\nEach element of the tamper-evident ledger has:\n</p><ol>\n<li>\n  a unique id;\n  </li>\n<li>\n  the type of the transaction, either create or pay;\n  </li>\n<li>\n<i>list of payers</i>: only for pay transactions --- a list\n  indicating the agents who pay coins into the transaction and the\n  amounts that they put in;\n  </li>\n<li>\n<i>array of value-payee pairs</i>: for both create and pay transactions --- an\n  array of pairs <code>(value, payee public key)</code>, where each\n  pair in the array indicates that coins of the specified value are\n  given to the payee with the specified public key.\n  </li>\n</ol>\n", "23": "<h3 style=\"color:red;\">Example of a create transaction</h3>\n An example of a <i>create</i> transaction is:\n<pre>\n(3146, create,  [(2.1, 7xxxx...), (3.2, 8xxxx)]).\n</pre>\nThe id of this transaction is 3146, the type of the transaction is\ncreate, and the array of value-payee pairs is\n<pre>[(2.1, 7xxxx...), (3.2, 8xxx)]\n</pre>\n\nIn this transaction the bank creates a coin of value 2.1 and gives it\nto the agent with public key 7xxxx..., and the bank also\ncreates a coin of value 3.2 and gives it\nto the agent with public key 8xxxx...\n\n<p>\nThe pair:\n</p><pre>\n(transaction id, index into array of value-payee pairs)\n</pre>\nuniquely identifies a (value, payee) tuple.\n<p>\nFor example (3146, 0) --- \ntransaction id 3146, and array index 0 --- identifies value-payee[0] of\ntransaction 3146 which is specified by the 2-tuple: (2.1, 7xxxx...). So, the\ntransaction id and index, (3146, 0), tells everybody that the agent\nwith public key 7xxxx received 2.1 units of coin in transaction 3146.\n\n</p><p>\nWhen this\ntransaction is in a tamper-evident ledger, every agent from that point\nonwards knows that agent 7xxxx received 2.1 coins. Any modifications of this\nrecord can be detected.\n\n</p><p>\nLikewise, (3146, 1) --- \ntransaction id 3146, and array index 1 --- identifies value-payee[1] which\nis the 2-tuple (3.2, 8xxxx...).\n\n\n</p>", "24": "<h3 class=\"w3-text-teal\">Pay transaction</h3>\nCoins are transferred from payers to payees in a pay transaction.\n\n", "25": "<h3 class=\"w3-text-teal\">Coins flowing into a pay transaction from payers</h3>\nThe payers are identified by a list of 2-tuples, where each 2-tuple is\n\n<pre>\n(transaction id, index into array of value-payee pairs)\n</pre>\n\nwhere <code>transaction id</code> is the id of the transaction in the\ntamper-evident ledger. As we said earlier, this pair uniquely identifies an agent and a\nvalue that this agent acquired in this transaction.\nFor example the pair --- transaction id, index --- such as (3146, 0)\nidentifies the 2-tuple (2.1, 7xxxx...); this 2-tuple asserts\nthat the agent\nwith public key 7xxxx received 2.1 units of coin in \ntransaction 3146. The <i>entire amount</i> specified in the 2-tuple (2.1 in our\nexample) is value that flows into this <i>pay</i> transaction from the agent\nwith public key 7xxxx.\n\n<p>\nSuppose the payers in a pay transaction are identified by the list:\n</p><pre>\n[(3146, 0), (7359, 3)]\n</pre>\nand suppose pair 3 in transaction 7359 is (3.2, 8xxxx). Then the total\namount of coins flowing into this pay transaction is 2.1 + 3.2, and\nthis amount is disbursed to payees.\n\n\n    <figure>\n<img alt=\"Fig1\" src=\"raw_webcrawl_data/Crypto/CryptoCurrencyIntroduction/Slide3.jpg\" style=\"width:100%\"/>\n<figcaption>Fig.3: Coins flowing into and out of a pay transaction.\n</figcaption>\n</figure>\n", "26": "<h3 class=\"w3-text-teal\">Coins flowing out of a pay transaction to payees</h3>\nThe outflow of coins is specified by an array of value-payee pairs,\nexactly as in a create transaction.\nThe transaction clears all coins: the total inflow from payers is\nequal to the total outflow to payees in a transaction.\n\n<p>\nThe system may provide incentives, such as payment of coins, to\nmanagers (e.g. banks) of cryptocurrencies. In this case, one of the\npayees is the bank itself.\n\n</p>", "27": "<h5 class=\"w3-text-teal\">Managing amounts spent in a transaction</h5>\nA transaction-id, index pair --- such as (3146, 0)\nidentifies a 2-tuple such as (2.1, 7xxxx...); this 2-tuple asserts\nthat the agent\nwith public key 7xxxx received 2.1 units of coin in \ntransaction 3146. The <i>entire amount</i> specified in the 2-tuple (2.1 in our\nexample) is value that flows into the transaction. What should this\nagent do if it\nwants to put in more than 2.1 coins into the transaction? Or less than\n2.1 coins?\n\n<p>\nTo put in more value, the bank identifies other transaction-id, index\npairs in which this agent received coins. For example, say that (4539, 2)\nidentifies a 2-tuple (3.2, 7xxxx), and assume that the payers in this transaction\nare specified by the pairs (3146, 0) and (4539, 2). The pair (3146, 0)\nasserts that agent (7xxxx) received 2.1 coins and the pair (4539, 2)\nasserts that the same agent received 3.2 coins. So the total amount of \ncoins flowing into this\ntransaction from this agent (7xxxx) includes 2.1 + 3.2.\n\n</p><p>\nTo put in less value, the agent acts as both payer and payee; the net\nvalue that this agent pays out to other agents is the difference\nbetween the amount that this agent puts in and takes out. For example,\nif agent with public key 7xxxx wants to put in 1.9 coins into this\ntransaction its payer information can be given by the transaction-id,\nindex pair (3146, 0) which asserts that the agent received 2.1 coins\nand the same agent is a payee that withdraws 0.2 coins.\n\n\n</p>", "28": "<h3 class=\"w3-text-teal\">Preventing Double Spending</h3>\nHow does the system prevent an agent from using the\nsame coin twice?\n\n<p>\nFor example, the transaction id, index pair\n(3146, 0) identifies the 2-tuple (2.1, 7xxxx...); this tuple\nasserts that the agent\nwith public key 7xxxx received 2.1 units of coin in \ntransaction 3146. Why can't the agent with public key 7xxxx use the\n2.1 coins that it received to buy items from Amazon and later use\nthe same 2.1 coins to buy more items from Walmart?\n\n</p><p>\nThe bank checks that the agent hasn't already spent the coin that it\nis putting into a transaction.\n\n</p><p>\nBefore permitting the transaction that double-spends the 2.1 coins\nwith Walmart, the bank inspects the tamper-evident ledger\nfor all transactions after transaction 3146 and before the Walmart\ntransaction to ensure that the agent (7xxxx) hasn't already spent the\n2.1 coins that it got in transaction 3146. The transaction that spends\nthe coins at Amazon will show up in the ledger, and so the transaction\nwith Walmart will not be allowed.\n\n</p><p>\nEvery agent that has the bank's hash pointer to the end of the\ntamper-evident ledger can inspect the ledger to check that\ndouble-spending hasn't occurred.\n\n</p><p>\nThe bank signs a valid transaction and appends it to the\ntamper-evident ledger. All agents can see the bank's signature and\nverify that nobody (not even the bank) has tampered with the\ntamper-evident ledger.\n\n</p>", "29": "<h4 style=\"color:red;\">Example of a pay transaction</h4>\n An example of the specification of a pay transaction is:\n<pre>\n(9431, pay,\n  [(3146, 0), (4731, 2)],\n  [(0.7, 7xxxx...), (4.6, 9xxxx...)]\n).\n</pre>\nThe id of this transaction is 9431; the type of the transaction is\npay; the payers into the transaction are identified by the pairs of\n(transaction-id, index): (3146, 0), and (4731, 2); and the\npayee array is [(0.7, 7xxxx...), (4.6, 9xxxx...)].\n\n", "30": "<h3 class=\"w3-text-teal\">Transaction validity</h3>\n\nThe bank appends a transaction to <code>L</code> if and only if the\ntransaction is valid. The bank checks for validity by carrying out the\nfollowing steps:\n<ol>\n<li>\n  The bank verifies that the payers into the transaction signed the\n  transaction.\n  </li>\n<li>\n  The bank checks that the total value of coins paid out from the\n  transaction does not\n  exceed the total value paid in to the transaction. (If the value paid\n  in exceeds the value paid out then the bank takes the difference as\n  a transaction fee. More about fees later.)\n  </li>\n<li>\n  The bank verifies that the payers' claims to have received coins in\n  previous transactions is genuine. For example,\n  if the agent with\n  public key 7xxxx... claims to have received coins worth 2.1 in the\n  transaction with id 10, and payee array index 0, then the bank\n  verifies this claim by that transaction.\n  </li>\n<li>\n  The bank ensures that coins paid into the transaction haven't\n  already been spent.\n  </li>\n</ol>\n", "31": "<h3 class=\"w3-text-teal\">Optimizations: Blocks and Block Chain</h3>\nVerifying large numbers of small transactions requires more\ncomputation than verifying small numbers of blocks of many transactions.\nA <i>block chain</i> is a tamper-evident ledger in which\neach element of the ledger is a block consisting of many transactions.\nA block of transactions can be aggregated into a single large\ntransaction by aggregating all the payers and payees of the smaller\ntransactions.\n\n<p>\nThe amount of computation decreases as the number of transactions in a\nblock increases. The time required to fill a block with transactions\nis larger when the number of transactions to fill the block increases.\n\n</p>", "32": "<h3 class=\"w3-text-teal\">Checking the Trusted Agent</h3>\nConsider a system in which the trusted agent broadcasts its current\ncopy of the tamper-evident ledger to all agents.\nEvery agent can inspect its copy of the tamper-evident ledger to determine whether the\nledger has been tampered with. So, every agent can validate its trust\nin the trusted agent; however, this validation\nsuffers from a crucial problem: Agents may only\nhave copies of <i>old, stale</i> versions of the ledger.\nBy the time that an agent receives a copy of the ledger, the trusted\nagent may have added more transactions to the ledger.\n\n\n    <figure>\n<img alt=\"Fig4\" src=\"raw_webcrawl_data/Crypto/CryptoCurrencyIntroduction/Slide4.jpg\" style=\"width:100%\"/>\n<figcaption>Fig.4: Old Copy is a Prefix of the Block Chain</figcaption>\n</figure>\n<p>\nAn old copy of the ledger can differ from the current copy in only one\nway: the current copy may have transactions appended to the end\nof the old copy. So all agents can validate <i>past</i> behavior of the\ntrusted agent. An agent cannot, however, treat its copy of the ledger\nas the master copy because the agent may not have the transactions\nadded most recently to the ledger.\n\n</p><p>\nIn the next module we will see how the Bitcoin algorithm addresses\nthis problem.\n\n\n\n</p>", "33": "<h3 class=\"w3-text-teal\">Advantages of this cryptocurrency</h3>\nAny agent can get a copy of the tamper-evident ledger and verify that all\ntransactions in the ledger are valid. Any agent can verify that\nthe only way in which the ledger is modified is that elements are\nappend to its tail; all that the agent needs to do is to check that\nthe pointer to the tail is modified only by appending\nelements. Because the ledger is tamper-evident, an agent can check\nthat the ledger doesn't change while the hash pointer to the end\nof the ledger doesn't change.\n\n<p>\nThe bank can't forge a transaction because all payers sign the\ntransaction. Agents can remain anonymous because an agent's only\npublic information is the agent's public key, and an agent can create\nmultiple public keys. Every agent can verify the correctness of every\ntransaction.\n\n</p>", "34": "<h3 class=\"w3-text-teal\">Disadvantages of this cryptocurrency</h3>\nUsers may not trust the bank. Transactions are not private because the\nbank has a record of all transactions. And the bank is a single point\nof failure.\n\n<p>\nNext we'll look at Bitcoin's algorithms for\nimplementing a cryptocurrency without trusted agents.\n\n</p>"}, "raw_webcrawl_data/Crypto/BitCoin.html": {"0": "<h1 class=\"w3-text-teal\">Introduction to Bitcoin</h1>\n", "1": "<h4 class=\"w3-text-teal\">\nThis module introduces the algorithm underlying BitCoin.\n</h4>\n\n    Bitcoin is based on cryptography and distributed consensus. We\n    discussed aspects of cryptography required for Bitcoin in an\n    <a href=\"./CryptoCurrencyIntroduction.html\">\n    earlier module.</a> Next, we study the distributed consensus\n    algorithm used by BitCoin.\n    \n    <a href=\"https://www.lopp.net/pdf/princeton_bitcoin_book.pdf\">\n    This Princeton University book\n    has a superb (and longer) description of consensus in Bitcoin.</a>\n<p>\n    We discussed distributed consensus in the modules on\n    <a href=\"../Paxos/Paxos.html\">Paxos</a> and\n    <a href=\"../Byzantine/ByzantineOral.html\">Byzantine Generals with\n    written</a> and\n    <a href=\"../Byzantine/ByzantineWritten.html\">oral messages.</a>\n    The specification of consensus is weaker in\n    Bitcoin and the algorithm used to obtain consensus is different\n    from those used in Paxos and Byzantine Generals written and oral\n    algorithms. The Byzantine Generals algorithms assumes that the\n    algorithm operates in rounds, whereas the Bitcoin algorithm\n    doesn't require synchronous rounds in which all agents\n    participate. The Paxos algorithm does not guarantee progress\n    whereas Bitcoin requires progress with high\n    probability; you wouldn't want to use coins if you had to wait\n    a long time to buy anything.\n\n    </p>", "2": "<h3 class=\"w3-text-teal\">No trusted agent</h3>\n    In an earlier module we described an algorithm that had many of\n    the features that we expect from a\n    cryptocurrency. That algorithm had, however, a\n    characteristic which is problematic to some: It relies on a\ntrusted agent.\nThe trusted agent could be the\n<a href=\"https://www.federalreserve.gov/\">Federal Reserve</a> in the\nUS or a <a href=\"https://www.ecb.europa.eu/home/html/index.en.html\">\ncentral bank </a> that manages a currency.\nTwo of many reasons given for mistrusting banks are that (1) people may\n    want to execute transactions in secret giving only their public\n    keys, and (2) central banks may be able to print money whereas\n    some cryptocurrencies, such as Bitcoin, limit the total amount of\n    coins that can ever exist.\n    The Bitcoin algorithm is a modification that eliminates the\n    trusted agent from the algorithm given in the previous module.\n\n    ", "3": "<h3 class=\"w3-text-teal\">No assumptions about numbers of agents</h3>\n    The Byzantine Generals algorithm uses an upper bound on the number\n    of faulty agents. Paxos assumes that the total number of agents is known. \n    The Bitcoin algorithm makes no assumptions about numbers of faulty and\n    non-faulty agents other than that there are a large number of agents.\n\n\n", "4": "<h3 class=\"w3-text-teal\">Incentives and transaction fees</h3>\nWhen a currency is managed by a single trusted agent, such as a\nbank, we assume that the bank gets some reward for its\nservice or is paid by a government to carry out this service.\nThe Bitcoin algorithm pays agents with Bitcoins for checking the validity of\ntransactions. This payment consists of new coins that are\n\"mined\". More about mining later\n\n\n", "5": "<h2 class=\"w3-text-teal\">A First Proposal for an Algorithm</h2>\nHow can we modify the algorithm we described earlier so that it works\nwithout a trusted agent? \nLet's try this modification:\nSelect random agents to play the role of the trusted manager.\n\n<p>\nA step of the algorithm is as follows:\nA single agent is\nchosen randomly to play the role of the trusted manager. This agent\nreceives and validates transactions, gathers some of the transactions\ninto a block, appends the block to the block chain, and\nbroadcasts the updated block chain. The\nother agents update their copies of the block chain when they receive this\nvalue. The system waits for all agents to update their copies and then\nexecutes the next step.\n\n</p>", "6": "<h4 class=\"w3-text-teal\">Challenges of the Proposed Algorithm</h4>\nThis algorithm has several challenges.\n<ol>\n<li>\n<i>Selecting a single agent.</i>\n  How can the collection of agents select a <i>single</i> agent to add\n  a block to the block chain? The selection of a single agent requires\n  all agents to reach a consensus about which agent to \n  select. So solving the problem this way would require solving\n  another consensus problem. The Bitcoin algorithm does\n  not select a unique agent to add a block to the block chain;\n  however, it uses an ingenious mechanism to ensure that multiple agents\n  don't attempt to add blocks at about the same time.\n  </li>\n<li>\n<i>Synchronization</i>:\n  How can the collection of agents wait long enough to ensure that all\n  agents have updated their copies of the ledger to the most recent version before\n  the ledger is modified again?\n  <p>\n  Consider the following example scenario.  All agents have the same\n  copy x of the ledger at some point t. Agent B, selected randomly to\n  act as the trusted agent, appends transaction y to the ledger at a\n  later point t' at which point B's copy is [x, y].  Then, agent C,\n  selected randomly to act as the trusted agent, appends transaction z\n  to the ledger at a later point t''. If C's copy is still [x],\n  because it has not as yet been updated to [x, y], then after C\n  appends z to the ledger, C's copy becomes [x, z].\n\n  </p><hr class=\"new2\"/>\n</li></ol>", "7": "<h5 style=\"color:blue;\">\n  A key property of the algorithm with the trusted agent is that two\n  copies of the ledger are either identical or the longer copy\n  consists of additional transactions appended to the\n  shorter copy.\n  </h5>\n<hr class=\"new2\"/>\n   With this property, two copies of the ledger are\n  either identical or the shorter copy can eventually \"catch up\" to the longer copy by\n  merely by appending more values.\n\n  <p>\n  In the example, B may receive\n  information about transaction z after receiving information about\n  transaction y, whereas C may receive this information in the reverse\n  order, which leaves B's copy as [x, y, z] and C's copy as [x, z,\n  y]. In this case, the copies remain different forever.\n  </p><p>\n  The Bitcoin algorithm <i>does not guarantee synchronization</i>; however,\n  its mechanism helps to make many agents append blocks in the same\n  order. \n  </p>\n<li>\n<i>Incentives</i>: \n  Why should an agent chosen to play the role of trusted agent agree\n  to play that role? What's the incentive? Why wouldn't the trusted\n  agent do nothing at all execute its step slowly?\n  </li>\n<li>\n<i>Untrustworthy Agents</i>:\n  The randomly-chosen trusted manager may not be trustworthy. You can\n  imagine what may go wrong in the previous algorithm if the bank was\n  dishonest.\n  </li>\n\nNext, let's look at how the Bitcoin algorithm addresses challenges 1\nand 2. We'll look at challenges 3 and 4 later.\n\n\n\n", "8": "<h3 class=\"w3-text-teal\">Selecting a Single Random Agent</h3>\nHow can an arbitrary set of agents, some of whom may be\nmalicious, and where the size of the set is unknown, pick a random\nagent?\n\n\n", "9": "<h4 class=\"w3-text-teal\">Using Puzzles to Select a Single Agent</h4>\nLet's look at a simple situation:\nseveral people solve puzzles in the same room. When a\nperson solves her puzzle, she yells \"I won!\". When a\nperson hears that somebody else has won she stops solving her puzzle.\n\n<p>\nIf everybody starts at the same instant and take the same time\nthen there will be collisions --- many will claim to win at the same time.\nIf, however, the time to solve\na puzzle is a random variable with a flat distribution then collisions\nare unlikely. Bitcoin uses the <i>puzzle-friendly property</i> of cryptographic hash\nfunctions discussed in the\n<a href=\"./Crypto/CryptoCurrencyIntroduction.html\">previous module</a>.\n\n</p><p>\nNow, instead of people being all in the same room, assume that they\nare competing across a network. When a person solves a puzzle,\nshe broadcasts a \"I won\" message. When a person working on a puzzle\ngets a \"I won\" message from somebody else, she stops working on her\npuzzle.\n\n</p><p>\nCollisions are likely if the expected time to solve the puzzle\nis small (say a millisecond) compared with the expected time (say a\nminute) for a message broadcast by one person to reach others. Message\ndelays may cause multiple people to solve their puzzles before\nreceiving \"I won\" messages.\nCollisions are unlikely when times to solve puzzles are much greater\nthan message delays.\n\n</p><p>\nWe could attempt to use timestamps: When a person solves her puzzle\nshe broadcasts a \"I won\" message and the time at which she finished\nsolving the puzzle. If a person gets a \"I won\" message with an earlier\ntimestamp then she concedes. This approach is problematic because a\ndevious agent may not solve the puzzle, or may set her timestamp to an\nearlier value.\n\n\n</p>", "10": "<h4 class=\"w3-text-teal\"> Puzzles in Bitcoin</h4>\nNext let's look at the puzzles used in Bitcoin.\nEach agent has its own copy of the block chain.\nAn agent \\(A\\) collects a set \\(trans\\) of transactions that haven't as yet been added\nto \\(A\\)'s copy of the block chain.\nAgent \\(A\\) proposes\nto append a block to the chain where the block\nconsists\nof the set \\(trans\\) in the\nfollowing way.\n\n<p>\nLet \\(ptr\\) be the pointer to agent \\(A\\)'s copy of the block chain. \nAgent \\(A\\) can add a block containing \\(trans\\) to the chain only if it\nproves that it has solved the following problem --- the \"puzzle.\"\n\n</p><p>\nFind a number, called \\(nonce\\), such that:\n</p><p>\n\\(H(nonce + ptr + trans) &lt; target \\)\n</p><p>\nwhere \\(+\\) is the concatenation operator , and <i>target</i> is a\ngiven value.\n\n</p><p>\nFor the time being\nassume that target is a constant; later, we'll see that it decreases very\nslowly over time.\nThe smaller the value of <i>target</i> the\ngreater the expected time to solve the puzzle.\n\n\n</p><p>\nThe time to solve a <a href=\"./CryptoCurrencyIntroduction.html\">Bitcoin puzzle</a> is a\nrandom variable with a flat distribution. Each proposer \nof a block is probably solving a different puzzle because the block of\ntransactions that it is aggregating is likely to be different from\nthat of other proposers. Agents have different amounts of\ncomputing capacity, and the time to solve a puzzle decreases with\ncapacity.\nAgents are unlikely to start solving their puzzles\nat the same instant. For these reasons, it is possible, but unlikely, that many agents\nwill solve their puzzles at the same time.\n\n\n\n</p><p>\nUsing puzzles to identify a single random agent leaves us with at least three challenges: (1) Collisions\nwill occur; (2) agents may be devious --- they may claim to have\nsolved puzzles when they haven't; and (3) agents with computing power\nthat far exceeds those of others will solve their puzzles faster than\nothers do --- and so though agents are selected randomly, those with\nlarge computing power are likely to be selected more often.\n\n\n</p>", "11": "<h3 class=\"w3-text-teal\">Attempts at Synchronization</h3>\nWhen an agent \\(A\\) appends a new block \\(B\\) to a block chain \\(L\\) it broadcasts\nthe new chain \\(L + B\\). This is analogous to a person shouting \"I won\" in the\nexample given earlier.\n\n<p>\nWhen a (non-devious) agent \\(A'\\), which proposes to extend chain\n\\(L\\), gets a message saying that \\(L\\) has already been extended to\n\\(L + B\\) then \\(A'\\)\nstops attempting to append a block to \\(L\\).\nInstead, \\(A\\) starts again with a\nnew set of transactions that it proposes to append to the\nextended chain \\(L + B\\).\n\n</p><p>\nIf the message delay between agents \\(A\\) and \\(A'\\) is small compared\nto the time to solve puzzles, then a collision\nbetween \\(A\\) and \\(A'\\) is unlikely, but still possible. So, it is\npossible that \\(A\\) broadcasts \\(L + B\\) while \\(A'\\) broadcasts\n\\(L + B'\\) at about the same time. So different agents may have\ndifferent copies of the block chain.\nWhat is the equivalent of the \"true\" system-wide\nblock chain when different agents have different copies?\n\n</p><p>\nThe Bitcoin algorithm does not use synchrony to deal with this\nissue. The problem of different copies of the block chain \nextant at the same time is handled in an ingenious asynchronous way that we\ndescribe later.\n\n\n</p>", "12": "<h3 class=\"w3-text-teal\">Managing Concurrent Updates</h3>\n\nA key step of the Bitcoin algorithm that updates local copies of block\nchains is as follows. After an agent creates a block and appends the\nnewly created block to its local copy it broadcasts its copy of\nthe block chain.\n\n<hr class=\"new2\"/>\n", "13": "<h5 style=\"color:blue;\">\nWhen an agent \\(A\\) gets a message containing a copy\nof another agent's block chain, agent \\(A\\) sets its local copy to the\nblock chain in the message <i>if and only if</i> the length of the\nblock chain in the message exceeds the length of \\(A\\)'s local copy.\n</h5>\n<hr class=\"new2\"/>\n<p>\nLet's look at a scenario.\nFor this scenario, \\(X\\) and \\(Y\\) are single blocks.\nAssume that agent \\(A\\)'s copy of the block chain\nis \\(L\\) when \\(A\\) receives a message containing the block chain \\(L\n+ X\\). Because the length of the block chain \\(L + X\\) is bigger than \\(A\\)'s\ncopy, \\(A\\) sets its copy to \\(L + X\\).\n\n</p><p>\nNow suppose agent \\(A\\) gets a\nmessage containing the block chain \\(L + Y\\); what does \\(A\\) do?\nAgent \\(A\\) ignores the message because the length of \\(L + Y\\) does not exceed\nthat of \\(A\\)'s current copy, \\(L + X\\).\n\n</p>", "14": "<h4 class=\"w3-text-teal\">Continuing Collisions</h4>\nLet's continue the above scenario.\nCan block \\(Y\\) become part of \\(A\\)'s\nchain, or will it remain forever an \"orphan\" block\nas far as \\(A\\) is concerned?\n\n<p>\nHere's a possible scenario.\nAn agent with a block\nchain copy \\(L + \nX\\), solves its puzzle and appends a block \\(X_{1}\\) to get a new copy\n\\(L + X + X_{1}\\) of the block chain which the agent broadcasts.\nAt the same time, another agent with a block\nchain copy \\(L + \nY\\), solves its puzzle and appends a block \\(Y_{1}\\) to get a new copy\n\\(L + Y + Y_{1}\\) which is broadcast.\nIf \\(A\\) receives \\(L + Y + Y_{1}\\) before receiving \\(L + X + X_{1}\\)\nthen \\(A\\) will set its chain to \\(L + Y + Y_{1}\\), and then reject\n\\(L + X + X_{1}\\).\n\n</p><p>\nYou can construct a scenario with a sequence of collisions between\nagents appending blocks \\(X_{i}\\) and other agents appending blocks\n\\(Y_{j}\\) so that \\(A\\)'s chain switches back and forth between \\(X\\)\nand \\(Y\\) values.\n\n</p><p>\nLong sequences of collisions are unlikely, and the longer the sequence\nthe less the probability of continuing collisions.\n\n</p><p>\nSo how can an agent determine whether a block is in the chain? And so\nhow can an agent find out if a transaction has been executed? You\nsell your used bicycle to somebody for coins, but how do you know if\nthat transaction becomes part of <i>the</i> block chain when different\nagents have different copies? And so how do you know that you can\nspend those coins that\nyou should have received for your bicycle?\n\n</p>", "15": "<h4 class=\"w3-text-teal\">Confidence that a Block is in the Chain</h4>\nIn this section we discuss the behavior of non-devious agents; we will\nshow how the algorithm handles devious agents later.\n\nSuppose an agent's copy of the block chain is \\(X + X_{1} + X_{2}\n+ \\ldots + X_{K}\\), and another agent's copy is \\(Y + ? + ? +\n\\ldots\\), where \\(?\\) represents arbitrary values. What is the\nlikelihood that \\(Y \\neq X\\)?\n\n<p>\n\\(Y\\) can be different from \\(X\\) only if there are a sequence of\n\\(K\\) or more collisions. And the likelihood of a sequence of\ncollisions decreases with \\(K\\). Likewise, the likelihood that an\nagent never receives a block chain containing \\(X\\) decreases with\ntime, and so decreases with \\(K\\). So, if \\(K\\) is large then with\nvery high probability, \\(X\\) is part of every agent's block chain.\n\n\n</p><p>\nNow suppose an agent's copy of the block chain is \\(L + X + X_{1} + X_{2}\n+ \\ldots + X_{K}\\). What is the likelihood\nthat another agent's copy is \\(L + Y + ? + \\ldots\\) where \\(L\\) is an\narbitrary sequence? By the same\nargument, when if \\(K\\) is large then with\nvery high probability, \\(X\\) is part of every block chain.\n\n</p><p>\nFor practical purposes,\nmany agents assume that if \\(K &gt; 6\\) then \\(L + X\\) is a prefix of\nmost agents' block chains.\n<a href=\"https://www.lopp.net/pdf/princeton_bitcoin_book.pdf\n\">See the Princeton Bitcoin book.</a>\n</p><p>\nSuppose agent \\(A\\) gets \\(N\\) Bitcoins in transaction \\(X\\). If \\(K =\n0\\) then an agent can't be confident that \\(A\\) ever received these coins\nbecause this transaction may not persist in the block chain. As \\(K\\)\nincreases agents become more confident that the transaction is in the\nblock chain and that \\(A\\) did, indeed, receive these coins.\n\n\n    <figure>\n<img alt=\"Fig1\" src=\"raw_webcrawl_data/Crypto/Bitcoin/Slide1.jpg\" style=\"width:100%\"/>\n<figcaption>Fig.1: More Confidence in Older Blocks in the\n    Chain</figcaption>\n</figure></p>", "16": "<h4 class=\"w3-text-teal\">What happens to Orphan Transactions?</h4>\nAn agent may append a block \\(Y\\) to its chain, and this block may be\ndropped from the chains of all agents and never reappear after some\npoint. We saw a scenario in which this happens.\nSuch a block is an \"orphan,\" because no agent has a record of that\ntransaction after some time.\nIf an orphan block contains the transaction in which you\nsold your bicycle in exchange for coins, then will you ever be able to\nspend your coins? Yes, you will get your coins as we see next.\n\n<p>\nAgents aggregate transactions that have not\nappeared in the agent's block chain into blocks and propose to append these\nblocks to the chain. A transaction that does not appear in block\nchains will be agrregated eventually by some agent and inserted into\na block in the chian. Though the orphan block disappears the\ntransactions in the block do not.\n\n</p>", "17": "<h2 class=\"w3-text-teal\">Incentives</h2>\nNext, looks look at challenge number 3. Why should an agent create\nblocks of validated transactions?\n\n<p>\nBecause the agent gets paid! Payment\nis from either a <i>block reward</i> or transaction fees.\n\n</p><p class=\"w3-text-teal\">Block rewards</p>\nAn agent that creates a block gets a specified number of Bitcoins for\nitself as a reward called a <i>block reward</i>.\nThe Bitcoins in a block reward are created by making the\nblock; these Bitcoins don't exist until the block is created. The process of making\nblocks and acquiring block rewards is called \"mining.\"\n<i>Mining is the only way of creating new Bitcoins.</i>\n<p>\nWhen Bitcoin started the reward for creating a block was 50 Bitcoins.\nThe reward\nhalves after the creation of a certain number (210,000) of blocks. The\nreward was reduced to 25 in 2013 and to 12.5 in 2018.\nBlock rewards will vanish at some point in the future.\nThe\ntotal number of Bitcoins that can ever be created has an upper bound:\nabout 21 million.\n\n</p><p>\n(Bitcoins can be lost. An agent may lose the hash pointer to the\ntransaction that gave the agent ownership of the coin, or an agent may\nlose its private key.)\n\n</p><p>\nEven when block rewards vanish, miners will continue to\nmine provided that they get paid transaction fees. A\ntransaction fee is a payment by payers and payees to\nminers. Transaction fees are voluntary. A high-fee for a\ntransaction is an incentive to miners to put this transaction into a\nblock quickly. So, it's possible that agents that offer no fee or low\nfees may have to wait longer for their transactions to enter the\nblock chain. \n\n</p><p>\nIncentives are critical for Bitcoin. Miners get paid to\nget their blocks into the long-term \"consensus\" block chain.\nMiners have an incentive to police the block chain because \nthey don't get paid for appending erroneous blocks. If a miner appends\nan erroneous block to the chain then other\nminers won't extend chains containing the erroneous block, and so the\nerroneous block will become an orphan.\n</p><p>\nAny agent can check whether its copy of the block chain is\nvalid; however, agents making ordinary transactions don't\nneed to do so because there are many miners each of whom has an\nincentive to ensure that the block chain is legitimate.\n\n</p>", "18": "<h2 class=\"w3-text-teal\">Attacks</h2>\nNext, looks look at challenge number 4.\n\n", "19": "<h3 class=\"w3-text-teal\">Stealing coins</h3>\nCan an agent steal a coin from an agent \\(X\\) by appending a block to\nthe chain where the block contains a transaction in which \\(X\\) gives\ncoins to \\(Y\\)?\n\n<p>\nNo, this can't happen thanks to cryptography. A transaction into which\n\\(X\\) puts coins is valid only if \\(X\\) signs the transaction.  \\(Y\\)\ncannot forge \\(X\\)'s signature, and so \\(Y\\) cannot create blocks that\ncontain such fraudulent transactions.\n\n</p>", "20": "<h3 class=\"w3-text-teal\">Double spend</h3>\nCan an agent spend the same coin twice? Can an agent buy something\nwithout paying for it?\n\n<p>\nConsider the following transaction using conventional checks issued by\nbanks. A buyer gives a seller a check for $\\(100,000\\) for a house.\nThe house is put in escrow.  When the check clears and the seller\nreceives the payment the buyer gets possession of the house. The legal\nprocess that includes notaries, real estate agents, and banks, helps\nensure that the transaction concludes correctly or is aborted correctly.\n\n</p><p>\nNext, let's look at a transaction in which a person buys a video\nonline by paying the seller Bitcoins. The buyer and seller know each\nother by their public keys and by their online addresses. \nThe buyer broadcasts the transaction in which the buyer gives the seller\nthe payment in Bitcoins.\nThe amount is specified as\na pair (transaction id, array index) described in the section <i>pay\ntransactions</i> in \n<a href=\"./CryptoCurrencyIntroduction.html\">\nthe module introducing crypto currencies.</a>\n</p><p>\nA miner puts the transaction into a block \\(X\\);\nappends the block to its copy \\(L\\) of the block chain; and broadcasts the extended\nblock chain \\(L + X\\).\nWhen the seller gets a copy of the block chain \\(L + X\\), the\nseller concludes that it has received the payment from the\nbuyer because the \ntransaction has been recorded in a block chain. So the seller gives\nthe video to the buyer.  \n\n</p><p>\nThe buyer cheats. The buyer creates a transaction in which the buyer\ntransfers the same Bitcoins  to the buyer itself.\nA miner creates a block \\(Y\\) that includes this transaction.\nA miner who has only received block\nchain \\(L\\) (and hasn't yet received chain \\(L + X\\)) appends \\(Y\\) to\n\\(L\\) to get a chain \\(L + Y\\), and broacasts \\(L + Y\\).\n\n</p><p>\nNow we have a situation in which one\nminer broadcasts a legitimate block chain \\(L + X\\) and a different\nminer broadcasts a legitimate block chain \\(L + Y\\). Both chains have\nthe same length.\nA miner with chain \\(L + X\\) does not know at this point that another\nminder has chain \\(L + Y\\). So, miners will extend both block chains.\n(Note that the algorithm will not permit chains \\(L + X + Y\\) or \\(L +\nY + X\\).) \n\n</p><p>\nWe've seen this situation before:  look at <i>block collisions</i> described\nearlier in this module. Both block chains will be extended, but\neventually, with very high probability, one of the blocks \\(X\\) or\n\\(Y\\) will drop out of chain.\n\n</p><p class=\"w3-text-teal\">\nHow should sellers protect themselves?\n</p>\n<p>\nWhat is the equivalent of the buyer's check clearing in the bank?\n</p><p>\nA seller should give the item to the buyer only after the transaction\nappears with high confidence in a block in the chain --- see Figure 1.\nThe seller listens to block chains broadcast by miners. If the seller\ngets a block chain \\(L + X + L'\\) where \\(L'\\) is itself a long block chain\nthen the seller has high confidence that the transaction is in the\npermanent record.\n\n</p><p>\nThe seller waits to get block chains in which its transaction appears\nin a block which is then followed by \\(m\\) blocks, for large\n\\(m\\). The larger the value of \\(m\\), the greater the seller's\nconfidence, but the longer the buyer has to wait to get paid.\nlength of the extension \\(L'\\).\n<a href=\"https://www.lopp.net/pdf/princeton_bitcoin_book.pdf\">\nA value of \\(m = 6\\) gives adequate confidence in most cases.</a>\n</p>", "21": "<h3 class=\"w3-text-teal\">Fraudulent miners</h3>\nA miner gets paid for every block the miner creates; so, why shouldn't\nthe miner create fraudulent blocks and get paid for them?\n\n<p>\nThe answer is the same as that for the double-spending attack. A miner\nmay have \ncreated a block and appended it to the chain; however, a suspicious\nagent (and we hope that all agents are suspicious!) will not accept\nthe block until many blocks have been appended to the chain after\nit. Other miners won't append their blocks to an invalid one. The\ninvalid blocks will become permanent orphans, and so the fraudulent\nminer won't be able to spend the coins in these blocks.\n\n</p>", "22": "<h3 class=\"w3-text-teal\">Fifty One Percent Attacks</h3>\nA 51% attack can be carried out by an agent that has more mining\npower (e.g. 51% or more) than all other agents combined. The higher\nthe proportion of mining power of a single agent, the greater the\nchances that attacks by that agent will succeed. You can see the\ndanger of a single agent having predominant mining power by thinking\nabout an agent with say, 99% of the total mining power. This agent\ncan mine so much faster than others that it can manipulate the block\nchain in many ways. It can create double-spend transactions and deny\nservices to some transactions.\n\n<p>\nA group of miners can collude to gain predominant mining power. Also,\nminers can <i>rent</i> Cloud-based systems for mining --- as opposed to having to <i>own</i>\nhuge data centers. An attacker can rent a large system for the specific purpose and\nduration of an attack. The public may not know if and when such\nattacks are successful because\n<a href=\"https://dci.mit.edu/51-attacks\">cryptocurrencies do not have an\nincentive to publish such attacks.</a>\n</p>", "23": "<h3 class=\"w3-text-teal\">Denial of service</h3>\nCan agents collude so that an agent \\(X\\)'s transactions never get\ninto blocks and so never get processed?\n\n<p>\nAn agent's identity does not appear in a transaction, only a public\nkey does. An agent can create new public keys at will. So let's ask\nanother question: can agents collude so that transactions with a\nspecific public key do not get into blocks?\n\n</p><p>\nOnly an agent that can solve puzzles in reasonable time can make\nblocks. These agents have significant computational power. One can\nconcoct a situation where many agents with significant computation\npower collude to avoid transactions from a specific public key. This\ncould have the effect of slowing processing of certain\ntransactions. However, such a situation isn't likely to arise because\nagents have an incentive to create blocks and so they compete ---\nrather than collude --- with each other.\nColluding\nagents, with massive computing power, may help to deny or slow service\nto a public key but not to another agent because agents can create\npublic keys at will.\n\n</p>", "24": "<h2>Further Reading</h2>\nThere are many issues that we have not covered. This material only\ncovers the basics from the point of view of distributed algorithms.\n\n"}, "raw_webcrawl_data/DISTRIBUTED_SYSTEM_MODELS/Basics.html": {"0": "<h1 class=\"w3-text-teal\">Basics</h1>\n\n<p class=\"w3-text-red\">\n  This page describes a simple model and notation for distributed\n  algorithms.\n\n  The model is adequate for describing a collection of algorithms.\n\n  Other models are introduced later.\n  \n  </p>\n\n", "1": "<h3 class=\"w3-text-teal\">Models of Distributed Systems</h3>\n\n  Distributed systems are complex.\n\n  A banking information system supports actions\n  in ATM machines, branch offices, and fraud analysis centers.\n\n  An earthquake monitoring system has thousands of sensors and agents\n  carrying out complex calculations.\n  \n<p>\n  \n  A model of a distributed system is an abstraction that ignores some\n  features.\n\n  Different models are used to design algorithms in different\n  settings;\n\n  for example, a model of a system in which messages may be\n  corrupted is different from one in which messages are incorruptible.\n\n\n  </p><p>\n  We begin with a simple model and notation.\n  Examples of some distributed algorithms are given in Python using a\n  simulator and using software libraries such as a\n  <a href=\"https://www.rabbitmq.com/tutorials/tutorial-one-python.html\">\n  Python implementation</a>\n  of the\n  <a href=\"https://en.wikipedia.org/wiki/Advanced_Message_Queuing_Protocol\">\n  Advanced Message Queuing Protocol (AMQP).\n  </a>\n\n</p>", "2": "<h3 class=\"w3-text-teal\">A Simple Model: A Network of Agents and Channels</h3>\n\n<p>\n\nA distributed system consists of a set of agents and a set of\nchannels.\n  \n  A channel is directed from one agent to one agent.\n\n  A channel from an agent <code>P</code> to an agent <code>Q</code> is\ncalled an output of <code>P</code> and an input of <code>Q</code>.\n\nThe ordered pair <code>(P, Q)</code> represents the channel from\n<code>P</code> to <code>Q</code>.\n  \n  An agent can send messages on its output channels and receive\n  messages on its input channels.\n\n</p><p>\nA system is represented by a\n<a href=\"https://en.wikipedia.org/wiki/Directed_graph\">\ndirected graph\n</a> in which vertices\nrepresent agents and edges represent channels.\nThe graph is called a <i>network of agents.</i>\n</p><p>\nFigure 1 shows a network of 4 agents: <code>pos</code>,\n<code>neg</code>, <code>sum</code>, and <code>result</code>.\n\nThe network has 3 edges: <code>(pos, sum)</code>, <code>(neg, sum)</code>,\nand <code>(sum, result)</code>.\n\n   <figure>\n<img alt=\"Fig1\" src=\"raw_webcrawl_data/DISTRIBUTED_SYSTEM_MODELS/DistributedSystemModels/DistributedSystemModels.020.jpeg\" style=\"width:60%\"/>\n<figcaption>Fig.1: Agent Network of the Example</figcaption>\n</figure>\n</p><p>\nA distributed system is initiated with sets of agents and channels\n  that remain unchanged. \n\nAgents and channels are not created or deleted during a computation.\n\n\n\n\n</p>", "3": "<h4 class=\"w3-text-teal\">Message Communication: Channels</h4>\n\n\nThe state of a channel is a\n<a href=\"https://en.wikipedia.org/wiki/Queue_(abstract_data_type)\">\nqueue</a> consisting of the sequence of\nmessages in the channel -- these are the messages that have been sent\non the channel and that have not been received.\n\n<p>\nAn agent sends a message by appending the message to the tail (rear)\nof the queue.\n\nA message from a nonempty queue is delivered to an agent by removing\nthe message from the head (front) of the queue, and calling the\n<code>receive</code> function of the agent.\n\n</p><p>\nMessages are not lost or modified in channels.\nEvery message sent is received.\nMessage delays are arbitrary (but finite).\n\n</p><p>\nA system has the following property: For all \\(n\\),\nthe \\(n\\)-th message received on a channel is the \\(n\\)-th message\nsent on the channel, and the \\(n\\)-th message is received on a channel\nonly after the \\(n\\)-th message is sent on the channel.\n\n\n\n</p>", "4": "<h4 class=\"w3-text-teal\">Agents</h4>\n\nAn agent is an object that sends and receives messages.\n\nAn agent is a sequential program that has two parts:\n(1) a part that initializes\nvariables and output channels of the agent and\n(2) a function\n<pre>\nreceive(message, sender)\n</pre>\ncalled a callback function in\n<a href=\"https://www.rabbitmq.com/tutorials/tutorial-one-python.html\">\nmessage queuing libraries.\n</a>\n<p class=\"w3-text-teal\">Delivering a message in a channel to an agent</p>\nIf an agent has a nonempty input channel then the <code>receive</code>\nfunction of the agent is called where <code>message</code> is the\nmessage at the head of the channel and <code>sender</code> is the\nagent that sent the message.\n\nThe message at the channel head is removed and processed by the agent.\n\n\n<p>\nA <code>receive</code> function must not be recursive: an agent cannot\nreceive a new message while it is executing a receive on a previous\nmessage. \n\nEvery execution of <code>receive</code> must terminate.\n\n</p><p>\nAn agent may have many nonempty input channels but the agent processes\nonly one message at a time. An agent is not interrupted while it\nis executing a <code>receive</code>. Messages that arrive while an\nagent is executing a <code>receive</code> remain in channels.\n\n\n</p><p class=\"w3-text-teal\">Sending a message on a channel by an agent</p>\nAn agent sends a message by executing\n<pre>\nsend(message, receiver)\n</pre>\nThe first parameter of <code>send</code> is the message that is sent,\nand the second paramenter is the agent to which the message is sent.\n\nExecution of this statement places the message in the output channel\ndirected from the sender to the receiver.\n\nStatements that send messages appear in the <code>receive</code>\nfunction of the agent.\n\nAn agent sends messages in response to messages that it receives.\n\n\nSee <a href=\"../CODE_EXAMPLES\">examples</a>.\n\n\n\n\n", "5": "<h4 class=\"w3-text-teal\">The State of a System</h4>\nThe state of a system is given by the states of its agents and\nchannels.\n\nWe represent a system state as a tuple with an element of the tuple\nfor each agent and each channel.\n\n<p>\nThe state of an agent is given by the values of its variables\n(including its <a href=\"https://en.wikipedia.org/wiki/Program_counter\"> program\ncounter</a>).\n\nAn agent's state changes while it executes its <code>receive</code>\nfunction.\n\nAn agent's state remains unchanged while it is idle, waiting to\nexecute a <code>receive</code>.\n\n</p><p>\nThe state of a channel is the sequence of messages sent on the channel\nthat have not as yet been delivered, and the channel state changes when a message is\nsent on the channel or a message in the channel is delivered.\n\n\n\n\n</p>", "6": "<h4 style=\"color:red;\">Example of an Agent</h4>\n\nThis is an example of an agent, <code>sum_pos_neg</code>,\nthat receives messages from agents\n<code>pos</code> and <code>neg</code>,\nand sends messages to agent <code>results</code>.\n\nSee an <a href=\"../CODE_EXAMPLES/SumPosAndNeg.py\">example</a>\nof an implementation in Python.\n\n<p>\n\nThe statements before the <code>receive</code> function specify the\ninitial values of variables and output channels of the agent.\n\nThe initial state of an output channel is an empty queue unless it is assigned\na different value.\n\nIn this example, the initial state is <code>sum = 0</code> and the\nagent's output channels are empty initially.\n\n\n\n\n</p><pre>\n# Initialization\nsum = 0\n\n# Callback function\ndef receive(message, sender):\n   if sender == pos:\n       sum = sum + message\n   else:\n       sum = sum - message\n   send(sum, results)\n</pre>\n\nIf the agent receives a message from agent <code>pos</code> then the\nagent increments <code>sum</code> by the contents of the message and sends the\nresulting value of <code>sum</code> to the agent <code>results</code>.\n\nThe agent takes similar actions when it receives a message from\n<code>neg</code> except that it decrements <code>sum</code>.\n\n\n\n"}, "raw_webcrawl_data/DISTRIBUTED_SYSTEM_MODELS/ComputationsReview.html": {"0": "<h1 class=\"w3-text-teal\">Computations: Review</h1>\n\n  Answer these questions to review the webpage on computations.\n\n  <p>\n  A computation is a central concept in describing and analyzing\n  distributed algorithms.\n\n  The relationship between computations and dataflow helps us\n  understand many algorithms.\n\n  </p><ol>\n<li>\n    What is the relationship between a single event and an iteration\n    of the sequential program abstraction of a distributed system.\n    </li>\n<li>\n    What is the change in state caused by execution of a single\n    iteration of the while loop representation of a distributed\n    system? What is the relationship between this change in state and\n    the change in state specified by an event?\n    </li>\n<li>\n    What is a computation?\n    </li>\n<li>\n    What is the relationship between a computation and the sequence of\n    states of the while loop abstraction of a distributed system?\n    </li>\n<li>\n    Review sequential programming: What is an invariant of a while\n    loop? How do you prove invariants?\n    </li>\n<li>\n    Review sequential programming: What is a loop variant of a while\n    loop?\n    </li>\n<li>\n    Review sequential programming: Review your earlier courses on\n    proving properties of loops. Give and prove an algorithm for\n    computing the all-points shortest paths in graphs.\n    </li>\n<li>\n    What is the difference between <code>event</code> and\n    <code>step</code>?\n    </li>\n<li>\n    What is the relationship between computations and dataflow\n    graphs?\n    </li>\n<li>\n    What is a topological sort of a directed acyclic graph?\n    </li>\n<li>\n    What is the key theorem relating computations and dataflow graphs?\n    The relationship between computations and dataflow graphs is\n    central to understanding many distributed algorithms.\n    </li>\n</ol>\n"}, "raw_webcrawl_data/DISTRIBUTED_SYSTEM_MODELS/BasicsReview.html": {"0": "<h1 class=\"w3-text-teal\">Basics: Review</h1>\n\n  Answer these questions to review the webpage on Basics.\n\n  <p>\n  A distributed system consists of a set of agents and a set of\n  channels.\n\n  </p><ol>\n<li>\n    What is an agent? Give an example.\n    </li>\n<li>\n    What is the state of an agent?\n    </li>\n<li>\n    When does the state of an agent change?\n    </li>\n<li>\n    What is a channel?\n    </li>\n<li>\n    What is the state of a channel?\n    </li>\n<li>\n    When does the state of a channel change?\n    </li>\n<li>\n    A network of agents and channels is represented by a graph. Give\n    an example of a graph for a system in which a group of people\n    exchange messages to determine where to go for lunch.\n    </li>\n<li>\n    What is the state of a system?\n    </li>\n</ol>\n"}, "raw_webcrawl_data/DISTRIBUTED_SYSTEM_MODELS/TimelinesProofs.html": {"0": "<h2 class=\"w3-text-teal\">Past, Future and Cuts in Dataflow: Proofs</h2>\n\n\n", "1": "<h3 class=\"w3-text-teal\">Properties of Cuts of Dataflow</h3>\n\n\n", "2": "<h4 class=\"w3-text-teal\">Dataflows of past events and\nfuture events.</h4>\n<p class=\"w3-text-teal\">Theorem</p>\nGiven a cut <code>(past, future)</code>, there exists exists a\ndataflow consisting only of <code>past</code> events, and there\nexists exists a dataflow consisting only of <code>future</code>\nevents. \n\n<p class=\"w3-text-teal\">Proof</p>\nFor a dataflow graph \\(G\\) with initial state \\(S_{init}\\) and final state\n\\(S_{fini}\\) and a state \\(S^{*}\\) at a cut <code>(past,\nfuture)</code>, we will prove that:\n<ol>\n<li>\n  There exists a dataflow graph \\(H\\) of\n  <code>past</code> events with initial state  \\(S_{init}\\) and final\n  state \\(S^{*}\\).\n  </li>\n<li>\n  There exists a dataflow graph \\(H'\\) of\n  <code>future</code> events with initial state \\(S*\\) and final\n  state \\(S_{fini}\\).\n  </li>\n</ol>\n\n\nEvery vertex of \\(H\\), apart from final vertices, is a vertex in\n\\(G\\).\nEvery edge of \\(H\\), apart from edges to final vertices, is an edge in\n\\(G\\).\nSince every vertex in \\(G\\) represents an event so does every vertex\nin \\(H\\).\n<p>\nThe proof for the second part is similar.\n\nFigure 1 illustrates the idea underlying the proof.\n\n\n</p>", "3": "<h4 class=\"w3-text-teal\">Computation in which Past precedes\nFuture.</h4>\n<p class=\"w3-text-teal\">Theorem</p>\nGiven a dataflow graph and a cut <code>(past, future)</code> of\n  the graph, there exists a computation that starts at the initial\n  state of the dataflow, executes the events in\n<code>past</code> and then executes events in <code>future</code>.\n\n<p class=\"w3-text-teal\">Proof</p>\nEvery topological sort of a dataflow graph is a computation.\n\nThere exists at least one topological sort of a directed acyclic\ngraph.\n\nFrom the previous theorem there is at least one computation consisting\nof past events, and there is at least one computation consisting of\nfuture events.\n\n<p>\nThe final state of computations of past events is the initial state of\ncomputations of future events.\n\n\n\n\n</p>", "4": "<h4 class=\"w3-text-teal\">Initial Subsequences of Steps by Agents</h4>\nFor the remainder of this page we assume that agents are indexed \\(i\\)\nfor \\(0 \\leq i &lt; N\\).\n\nWe restrict attention to a cut \n<code>(past, future)</code> specified by a vector \\(n\\)\nwhere <code>past</code> consists of the\nfirst \\(n[k]\\) steps of agent \\(k\\).\n\nCuts specified by a vector, in this way, are used in taking global\nsnapshots. \n\n\n\n\n<p class=\"w3-text-teal\">Theorem</p>\nThere exists a cut <code>(past, future)</code>\nspecified by a vector \\(n\\),\nwhere <code>past</code>\nconsists of the first \\(n[k]\\) steps of agent \\(k\\),\nexactly when:\n<center>\n<p style=\"color:blue;\">\nEvery message received in <code>past</code> is sent in\n<code>past</code>.\n</p></center>\n\n<p class=\"w3-text-teal\">Proof</p>\n<p>\nIf the \\(i\\)-th step of agent \\(k\\) is in <code>past</code> then\nall steps of agent \\(k\\) before its \\(i\\)-th step are\nalso in <code>past</code>.\n\nSo, if all messages received in <code>past</code> are sent in\n<code>past</code> then all paths in the dataflow to <code>past</code>\nare from <code>past</code>.\n\n\n\n</p>", "5": "<h4 class=\"w3-text-teal\">Numbers of Messages Sent and Received</h4>\n\nFor a channel \\(c\\), let \\(c_{sent}\\) and \\(c_{rcvd}\\) be the numbers\nof messages sent and received along the channel at each point in a\ncomputation.\n\n<p class=\"w3-text-teal\">Theorem</p>\nThere exists a cut <code>(past, future)</code>\nspecified by a vector \\(n\\)\nwhere <code>past</code>\nconsists of the first \\(n[k]\\) steps of agent \\(k\\)\nexactly when:\n<center>\n<p style=\"color:blue;\">\nFor all channels \\(c\\): \\(c_{sent} \\geq c_{rcvd}\\).\n</p>\n</center>\n\n"}, "raw_webcrawl_data/DISTRIBUTED_SYSTEM_MODELS/ModelReview.html": {"0": "<h1 class=\"w3-text-teal\">Timelines and Dataflow: Review</h1>\n\n  Answer these questions to review the webpage on dataflow.\n\n  <p>\n  We use dataflow in analyzing several algorithms.\n  \n  We use timelines to discuss performance of a few algorithms.\n\n  This review emphasizes dataflow.\n\n  </p><ol>\n<li>\n    What is an event?\n    </li>\n<li>\n    How is an event related to an execution of a <code>receive</code>\n    statement by an agent?\n    </li>\n<li>\n    What are the inputs and outputs of an event?\n    </li>\n<li>\n    What are the vertices of a dataflow graph?\n    </li>\n<li>\n    What are the edges of a dataflow graph?\n    </li>\n<li>\n    How do the edges represent\n    the flow of data?\n    </li>\n<li>\n    What is the concept of time in dataflow?\n    </li>\n<li>\n    What is the difference between <code>event</code> and\n    <code>step</code>?\n    </li>\n<li>\n    What is the relationship between a timeline and a dataflow\n    abstraction of the timeline?\n    </li>\n</ol>\n"}, "raw_webcrawl_data/DISTRIBUTED_SYSTEM_MODELS/Timelines.html": {"0": "<h2 class=\"w3-text-teal\">Past, Future and Cuts in Dataflow</h2>\n\n<p class=\"w3-text-red\">\n  This page introduces the concept of a <i>cut</i> of a dataflow\n  graph and relates a cut to the only notions of time -- past and\n  future -- in dataflow.\n\n  A cut separates past from future.\n\n  Cuts of dataflow are central to understanding detection algorithms\n  such as deadlock detection and for global snapshot algorithms that\n  determine states of distributed systems.\n  </p>\n\n", "1": "<h3 class=\"w3-text-teal\">Cuts of a Dataflow Graph</h3>\n\n  The only concept of time in dataflow is that of\n  <a href=\"DISTRIBUTED_SYSTEM_MODELS/Model.html\">\n  before and after.</a><a>\n  \n  Recall that a vertex \\(v\\) is before a vertex \\(w\\) in the\n  dataflow graph exactly when there is a path from \\(v\\) to \\(w\\).\n  And \\(w\\) is after \\(v\\) exactly when \\(v\\) is before \\(w\\).\n\n  <hr class=\"new2\"/>\n<p style=\"color:blue;\">\n  A <i>cut</i> of a dataflow graph is a partition of the set of vertices of\n  the graph into subsets <code>past</code> and <code>future</code>\n  such that all steps before <code>past</code> steps are\n  <code>past</code> steps.\n  </p>\n<hr class=\"new2\"/> \n  \n  The cut requirement can also be specified as all steps after\n  <code>future</code> steps are <code>future</code> steps, or\n  equivalently as: There is no <code>past</code> step after a\n  <code>future</code> step. \n\n  <p>\n  A cut of a dataflow graph is an instance of the general\n  <a href=\"https://en.wikipedia.org/wiki/Cut_(graph_theory)\">cut of a\n  graph</a>, and more specifically a cut of a flow network where the\n  source is an initial vertex and the sink is a final vertex.\n\n    \n  \n</p></a>", "2": "<h3 class=\"w3-text-teal\">The State at a Cut</h3>\n\n  The state of the system at a cut <code>(past, future)</code>\n  is specified by the labels of edges from\n  <code>past</code> to <code>future</code>.\n\n\n   \n  \n<p style=\"color:red;\">Example</p>\n  \n  The top diagram of figure 1 shows a cut in which vertices in\n  <code>past</code> are \n  colored red and vertices in <code>future</code> are green.\n\n  The curved black line is the boundary separating <code>past</code>\n  from <code>future</code>.\n\n  <figure>\n<img alt=\"Fig1\" src=\"raw_webcrawl_data/DISTRIBUTED_SYSTEM_MODELS/Model/Model.005.jpeg\" style=\"width:100%\"/>\n<figcaption>Fig.1: Example - A Cut of a Dataflow Graph</figcaption>\n</figure>\n\n  In figure 1 all edges to <code>past</code> steps are from <code>past</code>\n  steps, and \n\n  all edges from <code>future</code> steps are to <code>future</code>\n  steps.\n\n  <p>\n  The state <code>S*</code> after completion of <code>past</code> steps and before\n  initiation of \n  <code>future</code> steps is given by the labels of\n  edges from <code>past</code> to <code>future</code>.\n\n  These are the\n  edges that cross the boundary line separating <code>past</code>\n  from <code>future</code>.\n\n  </p><p>\n<code>S*</code> is\n  the final state of the sub-dataflow of <code>past</code> steps.\n\n  <code>S*</code> is also the initial state of the sub-dataflow of\n  <code>future</code> steps.\n\n</p><p>\nSee the lower diagram in figure 1.\n<code>S*</code> is shown as final vertices (which are labeled\n<code>N</code>) of <code>past</code> dataflow, \n and <code>S*</code> is shown as as initial vertices (which\n  are labeled <code>0</code>) of <code>future</code> dataflow.\n  \n\n  \n \n</p>", "3": "<h4 class=\"w3-text-teal\">Cuts of Computations</h4>\n\n\nAssociated with a computation is\n<a href=\"Computations.html\">\nexactly one dataflow graph</a>\nconsisting of the steps of the computation.\n\nWe define a cut of a computation as a cut of the dataflow of the\ncomputation.\n\n<p>\n\n</p>", "4": "<h3 class=\"w3-text-teal\">Properties of Cuts</h3>\n\nThe proofs of these properties are straigtforward and are given\n<a href=\"TimelinesProofs.html\">here.</a>\n\n", "5": "<h5 class=\"w3-text-teal\">Dataflows of Past steps before Future\nsteps.</h5> \n\nGiven a cut <code>(past, future)</code> of a dataflow graph:\n  There exists exists a\ndataflow consisting only of <code>past</code> steps, and there\nexists exists a dataflow consisting only of <code>future</code>\nsteps.\n\n\n\n", "6": "<h5 class=\"w3-text-teal\">Computations of Past before Future.</h5>\n<p>\nLet computation \\(X\\) start in state \\(S_{init}\\) and end in state\n\\(S_{fini}\\).\n\nLet \\(S^{*}\\) be the state at a cut <code>(past, future)</code> of \\(X\\).\n\nThere exists a computation \\(Y\\) that starts in \\(S_{init}\\), visits\n\\(S^{*}\\), and ends in \\(S_{fini}\\).\n\n</p><p>\nAll steps in <code>past</code> are\nexecuted before all steps in <code>future</code> in \\(Y\\).\n\n\n</p>", "7": "<h5 class=\"w3-text-teal\">Cut based on Message is Received only after\nit is Sent</h5>\n\nThere exists a cut <code>(past, future)</code> exactly when the\nfollowing two conditions hold:\n<ol>\n<li>\n  Every message received in <code>past</code> is sent in\n  <code>past</code>.\n  </li>\n<li>\n  If a step \\(x\\) of an agent is in <code>past</code> then steps at\n  that agent before \\(x\\) are also in <code>past</code>.\n  </li>\n</ol>\n", "8": "<h5 class=\"w3-text-teal\">Cut based on Counts of Messages Sent and Received</h5>\nA version of this property that uses only counts of messages is used\nin termination detection and is as follows.\n\n<p>\nThere exists a cut <code>(past, future)</code> exactly when the\nfollowing two conditions hold:\n\n</p><ol>\n<li>\n  For all \\(C\\):  \\(\\; C_{s} \\geq C_{r}\\),\n  <p>\n  where\n  \\(C_{s}\\) and \\(C_{r}\\) are the numbers of messages sent and\n  received, respectively, on channel \\(C\\), in <code>past</code>.\n  </p></li>\n<li>\n  If a step \\(x\\) of an agent is in <code>past</code> then steps at\n  that agent before \\(x\\) are also in <code>past</code>.\n  </li>\n</ol>\n\n", "9": "<h5 class=\"w3-text-teal\">Computations through Increasing Cuts</h5>\n\nLet \\(S_{i}\\) be the state at a cut <code>(past</code>\\(_{i}\\) <code>,\nfuture</code>\\(_{i}\\)), for \\(i = 0, 1, \\ldots\\) where:\n<p>\nFor all \\(i\\): <code> past</code>\\(_{i}\\) \\(\\subseteq\\) <code>past</code>\\(_{i+1}\\),\n\n</p><p>\nThere exists a computation that visits states\n\\(S_{i}\\) in increasing order of \\(i\\).\n\n\n\n\n\n</p>"}, "raw_webcrawl_data/DISTRIBUTED_SYSTEM_MODELS/TimelinesReview.html": {"0": "<h1 class=\"w3-text-teal\">Past, Future and Cuts in Dataflow: Review</h1>\n\n  Answer these questions to review the webpage on past, future and\n    cuts in dataflow.\n\n\n\n  <ol>\n<li>\n    What is the concept of time in dataflow? What do before and after\n    mean in a dataflow graph? \n    </li>\n<li>\n    What is a cut in a dataflow graph?\n    </li>\n<li>\n    What is the relationship between a cut in a dataflow graph and a\n    cut in a flow network?\n    </li>\n<li>\n    What is the state of a cut?\n    </li>\n<li>\n    Draw a dataflow graph. Identify two or more cuts in the graph.\n    </li>\n<li>\n    For the cuts in the previous example, give examples of dataflows\n    consisting only of <code>past</code> events, and only of\n    <code>future</code> events. \n    </li>\n<li>\n    For the cuts in the previous example, give examples of\n    computations in which <code>past</code> events precede\n    <code>future</code> events. \n    </li>\n<li>\n    Give an example of a partition of the set of vertices of a\n    dataflow graph into subsets <code>past</code> and\n    <code>future</code> where a message from <code>future</code> is\n    received in <code>past</code>.\n    Explain why this partition is not a cut.\n    </li>\n<li>\n    A property of a cut is that for each channel \\(c\\), the number of\n    messages sent on the channel in <code>past</code> is at least the\n    number of messages received on the channel.\n    Consider a partition <code>(past, future)</code> of the set of vertices,\n    specified by a vector \\(n\\) where <code>past</code> consists of\n    the first \\(n[k]\\) events of agent \\(k\\).\n    Is this partition necessarily a cut if the <i>total</i> number of\n    messages sent in <code>past</code> is at least the total number of\n    messages received in <code>past</code>?\n    </li>\n</ol>\n\n"}, "raw_webcrawl_data/DISTRIBUTED_SYSTEM_MODELS/Computations.html": {"0": "<h2 class=\"w3-text-teal\">Computations</h2>\n\n<p class=\"w3-text-red\">\n  A computation is a sequential representation of the execution of a\n  distributed system.\n\n  We use techniques for proving sequential programs to prove\n  properties of computations and thus prove properties of distributed\n  systems.\n\n\n\n\n \n  \n</p>", "1": "<h3 class=\"w3-text-teal\">A Sequential Programming Representation</h3>\n\nA timeline is representation of an execution of a distributed system;\na dataflow is an abstraction of a timeline.\nNext, we look at a sequential programming abstraction of a distributed\n  system.\n\n  <p>\n  Consider a sequential program consisting of an initialization (not\n  shown) and the following <code>while</code> loop.\n\n\n</p><pre>\nwhile there exists a nonempty channel in the system:\n   select a nonempty channel (u, v)\n   let the head of channel (u, v) be msg\n   v executes receive(msg, u)\n</pre>\n<p>\nExecution of the while loop terminates in a state in which all\nchannels are empty.\n\nIf there are multiple nonempty channels in an iteration then any\nnonempty channel is selected.\n\n  We discuss fairness in selection later in the course.\n\n  Each iteration of the while loop executes a single event.\n\n  Another representation of the loop is:\n  \n</p><pre>\nwhile there exists an event that can be executed:\n   execute any executable event\n  </pre>\n  \n  Each iteration of the while loop causes a state transition by\n  changing the state of one \n  agent and the states of channels incident on that agent.\n\n  Next we define computations and relate the sequential program to\n  computations. \n\n\n   \n  \n", "2": "<h3 class=\"w3-text-teal\">Computations</h3>\n\n\n  A computation is a sequence of one or more states,\n\\([S_{0}, S_{1}, S_{2}, \\ldots]\\) where the initial state, \\(S_{0}\\) of the\nsequence is an initial state of the system and where there exists an\nevent at an agent of the system that causes a transition from each\nstate in the sequence to the next.\n\nA computation may be finite or infinite.\n\n<p>\nA computation can also be specified by an initial state, \\(S_{0}\\), and a sequence\nof events \\([e_{0}, e_{1}, \\ldots, ]\\) where execution of the \\(i\\)-th\nevent \\(e_{i}\\) causes a transition from the \\(i\\)-th to the \\(i+1\\)-th state of the\ncomputation.\n\n\n</p><p>\nA <i>step</i> of the computation is an execution of a single event in this\nsequence.\n\nThe same event -- specified by the 4-tuple that defines a state\ntransition -- can occur multiple times in a computation.\n\nSo, different steps may be executions of the same event.\n\n\n\n\n</p>", "3": "<h4 class=\"w3-text-teal\">Relationship between Computations and a While Loop</h4>\nLet \\(S_{0}\\) be the state before execution of the while loop and let\n\\(S_{i}\\) be the state upon completion of the \\(i\\)-th iteration of\nthe loop for \\(i &gt; 0\\).\n\n<pre>\nwhile there exists an event that can be executed:\n   execute any executable event\n  </pre>\n\nThe sequence of states \\([S_{0}, S_{1}, \\ldots, ]\\)\nis a computation.\n\nThe while loop may not terminate and a computation may be infinite.\n\n<p>\nThe advantage of using a while loop to generate the states \nof a computation is that we can use familiar techniques for reasoning about\nwhile loops to reason about distributed algorithms.\n\n\n</p><p class=\"w3-text-teal\">Reasoning about While Loops: Loop Invariants and Loop Variants</p>\n\nA <a href=\"https://en.wikipedia.org/wiki/Loop_invariant\">\n  loop invariant</a>\n  is an assertion about the state of the program that holds before and\nafter each iteration of the loop.\n\nWe use invariants to prove that all states in an execution of a\ndistributed system have some property, such as all states are safe.\n\n<p>\nWe prove that some desired property holds at some point in the\nexecution of a while loop by using\n<a href=\"https://en.wikipedia.org/wiki/Loop_variant\">\nloop variants</a>, also called variant functions or metrics.\nMethods for proving progress properties are given later.\n\n\n\n\n  \n  \n</p>", "4": "<h3 class=\"w3-text-teal\">Relationship between Computations and Dataflow</h3>\n\n\nAssociated with each computation is a dataflow that has the same steps\nas the computation.\n\nA computation is a <i>sequence</i> of steps whereas a dataflow is a <i>partial\nordering</i> on steps.\n\nA computation has exactly one dataflow associated with it.\n\nA dataflow may have multiple computations associated with it.\n\n<p>\nWe adopt the following convention in diagrams of of computations.\n\nWe depict a computation by a dataflow graph with earlier steps in the\ncomputation appearing to the left of later steps in the computation.\n\nFigure 4 shows computations \\([1, 2, 3, 4]\\) and \\([2, 1, 3, 4]\\).\nBoth computations have the same dataflow.\n\n    <figure>\n<img alt=\"Fig4\" src=\"raw_webcrawl_data/DISTRIBUTED_SYSTEM_MODELS/Model/Model.004.jpeg\" style=\"width:100%\"/>\n<figcaption>Fig.4: Example: Different Computations with the same Dataflow</figcaption>\n</figure>\n\n</p><p class=\"w3-text-teal\">Topological Sorts of Directed Acyclic Graphs</p>\n\nA <a href=\"https://en.wikipedia.org/wiki/Topological_sorting\">\n  topological sort</a> of a directed acyclic graph is a sequence of\n  vertices of the graph where every edge \\((e, e')\\) in the graph,\n\\(e\\) appears before \\(e'\\) in the sequence.\n\n<p>\nFor example, \\([0, 1, 2, 3, 4, N]\\) and \\([0, 2, 1, 3, 4, N]\\) are\ntopological sorts of the graph shown in figure 4.\n\\([0, 1, 3, 2, 4, N]\\) is not a topological sort because the graph has\nan edge \\((2, 3)\\) and \\(3\\) appears before \\(2\\) in the sequence.\nFor similar reasons, \\([0, 1, 2, 4, 3, N]\\) is not a topological sort.\n\n\n</p><hr class=\"new2\"/>\n\n", "5": "<h4 style=\"color:blue;\">Theorem</h4>\n\n<p style=\"color:blue;\">\n\nAll topological sorts of a dataflow graph are computations.\n</p>\n<hr class=\"new2\"/>\n\nThe initial state is the state after the initial step (step 0) at\neach agent.\n\n<p class=\"w3-text-teal\">Proof</p>\n\nLet \\(z\\) be a topological sort of  a dataflow graph \\(G\\).\n\nWe will prove by induction on \\(n\\) that \\(z_{n}\\), the sequence\nconsisting of the first \\(n\\) elements of \\(z\\) is a computation.\n\nThe base case is \\(n = 0\\).\n\nLet the \\(n+1\\)-th element of \\(z\\) be vertex \\(v\\).\n\nLet this vertex represent a step at an agent \\(a\\) in which \\(a\\)\nreceives a message \\(m\\) on a channel \\(c\\), and the state of \\(a\\)\nbefore the step is \\(s\\).\n\n<p>\nBecause \\(z\\) is a topological sort:\n</p><ol>\n<li>\n  \\(z_{n}\\) consists of vertices with paths to \\(v\\). So the state of\n  agent \\(a\\) before \\(v\\) in \\(G\\) is \\(s\\).\n  </li>\n<li>\n  The step in which message \\(m\\) is sent is in \\(z_{n}\\), and all\n  messages received by \\(a\\) on channel \\(c\\) prior to \\(m\\) are also\n  in \\(z_{n}\\). So after \\(z_{n}\\) the state of \\(c\\) has \\(m\\) at its\n  head.\n  </li>\n</ol>\n<p>\nTherefore there is a step in  which \\(a\\)\nreceives a message \\(m\\) on a channel \\(c\\), and the state of \\(a\\)\nbefore the step is \\(s\\).\n\n\n\n  \n</p>", "6": "<h5 class=\"w3-text-teal\">Alternate Version of the Theorem</h5>\n\nA computation is a sequence of steps of a dataflow graph in which for all edges\n  \\((e, e')\\) in the graph:\n  <center>\\(e\\) occurs before \\(e'\\) in the\n  sequence.\n  </center>\n\n"}, "raw_webcrawl_data/DISTRIBUTED_SYSTEM_MODELS/Model.html": {"0": "<h2 class=\"w3-text-teal\">Timelines and Dataflow</h2>\n\n<p class=\"w3-text-red\">\n  A <i>timeline</i> shows how the state of a system changes over time.\n\n  A <i>dataflow graph</i> (dataflow for short) is an abstraction of a timeline; this\n  abstraction has no concept of time other than an ordering of\n  <i>before</i> and <i>after</i>.\n\n  We discuss the advantages and disadvantages of using a time-free\n  model. \n  </p>\n\n", "1": "<h3 class=\"w3-text-teal\">Timelines</h3>\n\n<a href=\"https://en.wikipedia.org/wiki/Timeline\">\n  Timelines</a> are used to describe the interactions of nations, animal\n  species, and ideas, over time.\n\n  Here timelines describe the interactions of agents over time.\n\n  \n  <p>\n  While an agent executes a  <code>receive()</code> the agent's state\n  changes over time and the agent may send messages at different\n  points in time.\n\n  The evolution of the state of a system over time is depicted by a\n  timeline diagram in which the horizontal axis is the time axis or t-axis.\n\n  </p><p>\n  Figure 1 shows states of a system with two agents \\(u\\) and\n  \\(v\\).\n  \n  Each horizontal line shows the state of an agent at each point in\n  time. \n\n  The lower horizontal line shows the states of agent v\n  and the upper horizontal line shows the states of agent u.\n\n\n    <figure>\n<img alt=\"Fig1\" src=\"raw_webcrawl_data/DISTRIBUTED_SYSTEM_MODELS/Model/Model.001.jpeg\" style=\"width:100%\"/>\n<figcaption>Fig.1: Example: Timeline</figcaption>\n</figure>\n</p><p class=\"w3-text-teal\">Representation of execution of a receive</p>\n  \nAn execution of a receive is represented by a rectangle, and\nthe length of the rectangle represents the time to execute the\n  receive.\n\nFor example, rectangles 1 and 4 represent receives executed by agent\n<code>u</code>, while rectangles 2 and 3 represent receives executed by \n<code>v</code>.\n\n  The state of the agent changes as the receive is executed and is\nspecified at each point in the rectangle.\n\n<p>\nHorizontal edges between successive executions of receives on the\ntimeline of an agent are labeled with the \nstate of the agent at those points.\n\nFor example, the state of agent <code>v</code> between its second and\nthird <code>receives</code> is <code>v2</code>.\n\n\n  </p><p class=\"w3-text-teal\">Representation of a message</p>\n  Each message is represented by a line which is labeled with the\nmessage.\n\nThe line starts from the point on the sender's timeline at the time that the\nmessage is sent.\n\nThe line ends at the point on the receiver's timeline at the time that the\nreceiver starts executing the <code>receive()</code> which receives\n  the message.\n\nFor example, messages m2 and m3 are sent by \\(u\\) while it is executing its\n  first <code>receive</code>.\n\n\n<p class=\"w3-text-teal\">Representation of an instant in time</p>\nAn instant at time T is represented by a vertical line T units to the\nright of the origin.\n\nThe state of an agent at time T is the state where the vertical line\nat T intersects the horizontal line representing the agent's state.\n\nThe state of a channel at T is shown by the labels of the channels' message lines\nthat intersect the vertical line at T.\n\n<p>\nIn figure 1, time T is represented by the dotted red line.\nAt T, agent\n<code>u</code> is executing its first receive and <code>v</code> is\nexecuting its second; channel\n<code>(v, u)</code> contains a single message; <code>m4</code>, and \nchannel <code>(u, v)</code> is empty. \n\n</p><p>\nThe initial state of the timeline is given by the initial states of\nagents and channels and is shown by the labels of the edges\nrepresenting agent states and messages from the point at time 0 -- the\npoint at which execution starts.\n\n\n\n</p><p class=\"w3-text-teal\">A system may have many timelines</p>\nThe hardware on which a distributed algorithm executes may be unspecified.\n\nThe operating system executing a distributed algorithm may interrupt\nan agent executing a <code>receive</code> and restart it later.\n\nSome agents may execute on slower processors than others.\n\nThe communication mechanism may get congested causing message\ndelays to change.\n\nWe can determine properties of the timelines of a distributed\nalgorithm only if we have specifications of the operating system and\nhardware on which the algorithm executes, and we often don't have the\ninformation. \n\n\n\n<p class=\"w3-text-teal\">The passage of time</p>\nThe passage of time, as determined by an agent's local clock, may play\na role in the behavior of a system.\n\nFor example, a receive function of an agent may have a sleep statement\n(e.g., <code>time.sleep(t)</code> in Python).\n\nThe system behavior may depend on the fact that one agent sleeps for 1\nsecond while another agent sleeps for 10 seconds.\n\nLocal clocks are not perfectly synchronized; however, a system\ndesigner may know that clocks are usually accurate and may use clocks\nin designing the system.\n\n<p>\nNext, we describe a model that ignores time and makes no assumptions\nabout how far local clocks may drift from each other and from true\ntime.\n\nLater, we discuss the advantages and disadvantages of using our weak\nmodel.\n\n\n\n  \n\n</p>", "2": "<h3 class=\"w3-text-teal\">Events</h3>\n\n\nThe top diagram of figure 2 depicts an execution of\n<code>receive</code> as time progresses, and the lower diagram ignores\ntime and restricts attention to the states of the agent before and\nafter execution of the receive.\n\n    <figure>\n<img alt=\"Fig1\" src=\"raw_webcrawl_data/DISTRIBUTED_SYSTEM_MODELS/Model/Model.002.jpeg\" style=\"width:100%\"/>\n<figcaption>Fig.2: Representation of receive with and without time</figcaption>\n</figure>\n\nAn execution of a <code>receive</code> by an agent\n<code>u</code> is specified by an <i>event at <code>u</code></i>.\n\nAn event in which an agent <code>u</code> executes\n  <code>receive(msg, v)</code> is a 4-tuple:\n<ol>\n<li>\n<code>u</code>'s state before it executes the <code>receive</code>,\n  </li>\n<li>\n  the message, <code>msg</code>, received and the sender, <code>v</code>, of the message,\n  </li>\n<li>\n<code>u</code>'s state after it completes execution of the <code>receive</code>, and  \n  </li>\n<li>\n  for each output channel of <code>u</code>, the sequence of messages\n  sent and the receivers of the messages.\n  </li>\n</ol>\nThe first two elements of the tuple are called the <i>inputs to the event</i>, and\nthe last two elements are the <i>outputs of the event</i>.\n\nAn event does not specify the duration of a receive, or the state of\nan agent while it executes a receive, or when messages are sent during\nexecution of a receive.\n\n<p>\nAn agent's <code>receive</code> function can be specified by the set\nof events at that agent.\n\nWe say that an agent <i>executes event e</i> to mean that the agent executes a\n<code>receive</code> that is specified by <i>e</i>.\n\nAn agent changes state by executing a sequence of events where the\nagent's state after the \\(i\\)-th event is its state before the\n\\(i+1\\)-th, for all \\(i\\).\n\n\n\n\n  \n</p>", "3": "<h3 class=\"w3-text-teal\">Dataflow</h3>\n\nA dataflow graph is an abstraction of a timeline diagram.\n\nAn execution of a receive is represented by a vertex in the dataflow\ngraph whereas it is represented by an evolution of the agent's state\nin a timeline.\n\nThe lower diagram of figure 3 is the dataflow abstraction of the\ntimeline shown in the upper diagram.\n\n\n  \n    <figure>\n<img alt=\"Fig3\" src=\"raw_webcrawl_data/DISTRIBUTED_SYSTEM_MODELS/Model/Model.003.jpeg\" style=\"width:100%\"/>\n<figcaption>Fig.3: Example: Dataflow Graph</figcaption>\n</figure>\n", "4": "<h5 class=\"w3-text-teal\">Vertices in Dataflow</h5>\n\nA dataflow graph of a system is a labeled, directed, acyclic graph.\n\nEach vertex represents the execution of an event at an agent.\n\nThe inputs and outputs of each vertex in a dataflow graph are\nspecified by the event that the vertex represents.\n\n<p>\nFor example, vertex 1 in the graph represents execution of an event at agent\n<code>u</code> specified by the 4-tuple: (1) the state before the event is\n<code>u1</code>; (2) a single message is received in the event and the\nmessage is <code>m1</code> from <code>v</code>; (3) the state after\nthe event is <code>u2</code>; (4) a single message <code>m3</code> is\nsent in the event and this message is  sent to <code>v</code>.\n\n</p>", "5": "<h5 class=\"w3-text-teal\">Edges in Dataflow</h5>\n\nThe edges are in the same as in a timeline and are as follows:\n<ol>\n<li>\nThere is an edge from an event at an agent to the next event at that\nagent; this edge is labeled with the state of the agent between the\nevents and is called an <i>agent edge</i>.\n  </li>\n<li>\nThere is an edge from an event in which a message is sent\nto the edge at which the message is received; this edge is labeled\n  with the message and is called a <i>message edge</i>.\n  </li>\n</ol>\n\n", "6": "<h5 class=\"w3-text-teal\">Initial and Final States</h5>\n\n<p>\nA dataflow graph starts in an initial state of the system.\n\nA dataflow graph may be finite or infinite.\n\nA dataflow graph has an initial vertex, labeled <i>O</i>, for each\nagent.\n\nThe output edges from an initial vertex at an agent <code>u</code>\nspecify <code>u</code>'s initial state and the states of\n<code>u</code>'s output channels.\n\n\n</p><p>\nThe graph representing a finite execution of a system has a vertex labeled\n<i>N</i> for each agent, representing the final state.\n\nThis vertex is called the final vertex of the agent.\n\nThe input edges to a final vertex at an agent <code>u</code>\nspecify <code>u</code>'s final state and the final states of\n<code>u</code>'s input channels.\n\nThe initial and final vertices do not represent events.\n\n</p><p>\nA finite dataflow graph represents an execution of the system from the\ninitial state to the final state of the graph.\n\n\n \n  \n</p>", "7": "<h4 class=\"w3-text-teal\">Events and Steps</h4>\n\n\nAn agent can execute multiple receives specified by the same event.\n\nAn event is a 4-tuple that specifies a state transition of an agent.\n\nEach execution of an event by an agent is called a <i>step</i> taken by the\nagent to distinguish an execution of an event from the event itself.\n\nEach vertex in a dataflow is a step taken by an agent.\n\n\n \n  \n", "8": "<h4 class=\"w3-text-teal\">Time in Dataflow: Before and After</h4>\n\nThe only concept of time in a dataflow is that some steps occur before\nother steps.\n\nA message is sent <i>before</i> it is received.\n\nEquivalently, a message is received <i>after</i> it is sent.\n\nLater steps at an agent occur after earlier steps at the agent.\n\n<i>before</i> and <i>after</i> are partial orders on steps in a dataflow.\n\n<p>\nA step \\(v\\) occurs before a step \\(v'\\) in a dataflow exactly when\nthere is a path from \\(v\\) to \\(v'\\).\n\nFor example, steps 0, 1 and 2 occur before step 3.\n\nLikewise steps 4 and N occur after step 3.\n\nStep 1 is neither before nor after step 2.\n\n\n\n \n  \n</p>", "9": "<h3 class=\"w3-text-teal\">Advantages and Disadvantages of the\n  Dataflow Model</h3>\n\n<p class=\"w3-text-teal\">Disadvantages</p>\nThe obvious disadvantage of dataflow is that ignores time which can\nplay a role in ensuring that a system behaves correctly.\n\nMoreover a model that doesn't deal with time cannot deal with\nspecifications that include time.\n\nAlso time plays a critical role in system performance.\n\n<p class=\"w3-text-teal\">Advantages</p>\nAnalyzing simpler models is easier than analyzing complex ones.\n\nA single dataflow represents all timelines with the same flow\nstructure and arbitrary timing; so, a property of the dataflow is also\na property of all timelines that the dataflow represents.\n\nWe will prove properties of all dataflows of a system, and these\nproperties also hold for all timelines.\n\n<p>\nWe use the simplest model adequate for the task at hand.\n\n\n\n\n</p>"}, "raw_webcrawl_data/DistributedCollaboration/DrinkingPhilosophers/DrinkingPhilosophers.html": {"0": "<h1 class=\"w3-text-teal\">Resource Management: Drinking\nPhilosophers</h1>\n", "1": "<h4 class=\"w3-text-teal\">\nThis module describes algorithms for \"The Distributed Drinking\nPhilosophers Problem,\" by which distributed agents share indivisible\nresources, such as exclusive access to files. \n</h4>\n", "2": "<h2 class=\"w3-text-teal\">Key Ideas</h2>\n<ol>\n<li>\n      This module describes algorithms by which distributed agents\n      share indivisible resources, such as exclusive access to files,\n      in a fair way. \n      </li>\n<li>\n      The module shows how to use a <i>total</i> ordering of\n    priorities to share resources fairly. The dining philosophers\n      algorithm, given in an earlier module, uses a partial ordering.\n      </li>\n<li>\n      We show how time --- readings from local clocks of agents --- is\n      used to obtain priorities that are totally ordered.\n      </li>\n</ol>\n<p>\n   The description of the algorithm in this module is not as detailed\n    as that given for dining philosophers. You can define a formal\n    specification and carry out a detailed proof for this algorithm in\n    almost exactly the same was as for dining philosophers.\n\n    \n   \n    </p>", "3": "<h2 class=\"w3-text-teal\">The Problem</h2>\n    A set of agents shares a set of indivisible resources. Exclusive\n    access to a file is an example of an indivisible resource.\n\n    <p>\n    The\n    lifecycle of an agent is the same as in Dining Philosophers: (1)\n    executing outside the critical section, (2) waiting to enter a\n    critical section, and (3) executing in the critical section. The\n    problem is identical to Dining Philosophers except for the\n    transition to the critical section.\n\n    </p><p>\n    An  agent executes outside the critical section without\n    holding any resources. An agent may execute outside critical\n    sections for ever, or it may start waiting to enter a critical section.\n    When an agent starts waiting\n    to enter a critical section it waits to get exclusive access to a\n    nonempty subset of the resources. It continues waiting until it\n    gets all the resources for which it waits. While it waits it does\n    not change the subset of resources for which it waits.\n    The agent continues to hold these resources when it executes in\n    the critical section. An agent remains in a critical section for\n    only a finite number of steps and then starts executing outside\n    the critical section at which point it no longer needs resources.\n\n    </p><p>\n    For example, an agent needs exclusive access to a set of files for\n    it to execute in its next critical section, and it waits until it\n    is given this access. While it is in the critical section it\n    continues to have exclusive access to these files. When it is\n    executing outside the critical section it does not need access to\n    these files. Each time an agent starts to wait it may wait for a\ndifferent set of files.\n\n\n\n    <figure>\n<img alt=\"Fig1\" src=\"raw_webcrawl_data/DistributedCollaboration/DrinkingPhilosophers/Slide1.jpg\" style=\"width:100%\"/>\n<figcaption>Fig.1: Agent Lifecycle</figcaption>\n</figure>\n</p><p>\n    The ideas in this module can be used to manage agents that require\n    read or write access to files. Multiple agents can have read\n    access to a file concurrently. While an agent has write access to\n    a file no other agent can have access to the file. A homework\n    problem deals with read/write access.\n\n</p><p class=\"w3-text-teal\">Drinking Philosophers</p>\nThe drinking philosophers name is another example of an attempt at\nCS humor. A philosopher is in one of three states: <i>tranquil</i>,\n<i>thirsty</i> or <i>drinking</i>. A philosopher may remain tranquil\nfor ever, or it may become thirsty for one or more beverages. It\nremains thirsty until it gets all the beverages for which it\nwaits. Only when a thirsty philosopher gets all the beverages for\nwhich it waits does it start drinking. It continues to drinking all these\nbeverages until it becomes tranquil again. Philosophers drink for only\nfinite time.\n\n<p>\nA philosopher that enters the tranquil state remains in that state for\nat least a constant \\(\\gamma\\) amount of time. So, a philosopher can't\ngo from drinking to thirsty instantaneously or in an arbitrarily small\namount of time.\n\n\n    <figure>\n<img alt=\"Fig2\" src=\"raw_webcrawl_data/DistributedCollaboration/DrinkingPhilosophers/Slide2.jpg\" style=\"width:100%\"/>\n<figcaption>Fig.1: Drinking Philosophers</figcaption>\n</figure>\n</p><p>\nA beverage can be held by at most one philosopher. Imagine there's\nonly one bottle of each beverage in the system, and philosophers send\nbottles to each other. One philosopher can drink vodka and cola\nwhile another philosopher drinks gin and tonic. However, one agent cannot drink\nvodka and cola while another drinks vodka and orange juice.\n\n</p>", "4": "<h3 class=\"w3-text-teal\">An Algorithm</h3>\nThere are many algorithms for this problem; here we discuss one.\nEach beverage is an indivisible unique token. A token is\nexchanged between agents and a <i>manager</i> of the token. We assume\nthat each token has its own manager --- this assumption is merely for\nconvenience of exposition.\n\n<p class=\"w3-text-teal\">Messages</p>\n<ol>\n<li>\n<i>request</i>: An agent sends a request for a beverage to a\n  manager. A request is a pair \\(agent\\_id, request\\_priority\\), the\n  id of the requestor and the priority of the request.\n  </li>\n<li>\n<i>beverage</i>: A manager sends a beverage to a requesting agent, and agents\n  send beverages back to managers. A beverage is uniquely identified\n  by its name.\n  </li>\n<li>\n<i>demand</i>: A manager sends a demand to an agent for the\n  beverage that the manager manages and that the agent holds. \n  </li>\n</ol>\n<p class=\"w3-text-teal\">Agent Actions</p>\n<ol>\n<li>\n    When an agent becomes thirsty it sends requests to managers for\n    all the beverages that it needs to drink.\n    </li>\n<li>\n    If a thirsty agent gets a demand to return a beverage to a manager\n    then the agent returns the beverage and sends another request for\n  the beverage. The priority of this new request is the same as the\n  priority of this agent's last request.\n    </li>\n<li>\n    If a thirsty agent gets all the beverages that it needs to drink\n    then it starts drinking.\n    </li>\n<li>\n    When an agent finishes drinking it returns all the beverages that\n    it holds to the managers of the beverages.\n    </li>\n</ol>\n<p class=\"w3-text-teal\">Manager Actions</p>\n  A manager has local variable \\((hr, hp)\\),  which is the id of the agent to which the\n  manager has most recently sent \n  the beverage, and the priority of the request made by that\n  agent. \\(hr\\) is an acronym for <i>h</i>andling <i>r</i>equestor,\n  and \\(hp\\) is an acronym for <i>h</i>andling <i>p</i>riority.\n  If the manager holds the beverage then this variable is empty (\\(Null\\)). \n  <p>\n  A manager also maintains a priority queue of pending requests ordered by\n  priority.\n\n  </p><p>The actions of a manager are as follows.\n  \n  </p><ol>\n<li>\n    If a manager gets a request \\((r, p)\\) while it holds a beverage then it\n    sends the beverage to the requestor \\(r\\), and sets\n    \\(hr, hp  = r, p\\).\n    </li>\n<li>\n    If a manager gets a request \\((r, p)\\) while it does not hold the\n    beverage and \\(hp &gt; p\\) then the manager inserts the request \\((r,\n    p)\\) into the priority queue of pending requests.\n    </li>\n<li>\n    If a manager gets a request \\((r, p)\\) while it does not hold the\n    beverage and \\(hp &lt; p\\) then:\n    <ol>\n<li>the manager sends a demand to \\(hr\\) to return the\n      beverage, if the manager has not already sent that demand, and\n      </li>\n<li> the manager inserts the request \\((r, p)\\) into its priority queue of\n      pending requests.\n      </li>\n</ol>\n</li>\n<li>\n    If a manager gets a beverage and it has no pending requests then\n    the manager holds the beverage and sets the handling requestor and\n    priority to empty.\n    </li>\n<li>\n    If a manager gets a beverage and it has pending requests then\n    let \\((r, p)\\) be the request at the head of the priority queue\n    (i.e. the request with the highest priority). It sends the\n    beverage to requestor \\(r\\),\n    removes \\((r, p)\\) from the queue of pending requests,\n    and sets\n     \\(hr, hp  = r, p\\).\n    </li>\n</ol>\n", "5": "<h2 style=\"color:red;\">Example</h2>\n  The example shows a scenario. The system has two agents, Maya and\n  Liu both of whom are tranquil in the initial state. There are three beverages: Tea,\n  coffee and milk. Initially, these beverages are with their managers.\n  <p>\n  The next diagram, stage 1, shows the state after Maya gets thirsty for tea and milk.\n  So she sends requests to the managers of tea and milk. The priority\n  of this request is 2. (We will discuss how priorities are obtained\n  later.)\n  \n    <figure>\n<img alt=\"Fig3\" src=\"raw_webcrawl_data/DistributedCollaboration/DrinkingPhilosophers/Slide3.jpg\" style=\"width:100%\"/>\n<figcaption>Fig.3: Stage 1</figcaption>\n</figure>\n\n  The next diagram, stage 2, shows the situation after the managers of tea and\n  milk get requests from Maya, and respond by sending the beverages to\n  Maya because there are no pending requests for these beverages. The\n  beverages, tea and milk, are in the channel to Maya.\n  <figure>\n<img alt=\"Fig4\" src=\"raw_webcrawl_data/DistributedCollaboration/DrinkingPhilosophers/Slide4.jpg\" style=\"width:100%\"/>\n<figcaption>Fig.4: Stage 2</figcaption>\n</figure>\n\n  The stage 3 diagram shows a state after Liu becomes thirsty for\n  coffee and milk. So she sends requests for coffee and milk to the\n  managers of these beverages. The priority of the request is 5.\n  </p><p>\n  Maya has received the milk beverage but tea is still in the\n  channel. So, Maya remains thirsty.\n  <figure>\n<img alt=\"Fig5\" src=\"raw_webcrawl_data/DistributedCollaboration/DrinkingPhilosophers/Slide5.jpg\" style=\"width:100%\"/>\n<figcaption>Fig.5: Stage 3</figcaption>\n</figure>\n\n  The stage 4 diagram shows a state in which the coffee manager\n  receives Liu's request and sends\n  coffee to Liu because there are no pending requests for coffee. When\n  the milk manager gets Liu's request, the manager puts the request on\n  the priority queue of pending requests. At this point the queue has\n  only Liu's request.\n\n  The\n  milk manager demands milk back from Maya because her request has\n  priority 2 whereas Liu's request has priority 5. Maya is still\n  thirsty because the tea hasn't arrived yet.\n  <figure>\n<img alt=\"Fig6\" src=\"raw_webcrawl_data/DistributedCollaboration/DrinkingPhilosophers/Slide6.jpg\" style=\"width:100%\"/>\n<figcaption>Fig.6: Stage 4</figcaption>\n</figure>\n\n  Next, Maya receives the demand for milk and responds by sending the\n  milk back to the milk manager. Maya also resends its original\nrequest for milk, with priority 2, to the milk manager.\nThen Maya receives tea. Maya\n  continues to be thirsty because she has only one of the two\n  beverages that she needs to drink. Liu has coffee, but she remains\n  thirsty because milk is in the channel from Maya to the milk manager.\n  <figure>\n<img alt=\"Fig7\" src=\"raw_webcrawl_data/DistributedCollaboration/DrinkingPhilosophers/Slide7.jpg\" style=\"width:100%\"/>\n<figcaption>Fig.7: Stage 5</figcaption>\n</figure>\n\nIn stage 6, the milk manager has received Maya's request for milk with\n  priority 2 and received milk. So, the milk manager sends the milk to\n  respond to the highest priority request in the priority queue; this\n  request is from Liu. The milk manager puts Maya's request into the\n  queue of pending requests.\n  <figure>\n<img alt=\"Fig8\" src=\"raw_webcrawl_data/DistributedCollaboration/DrinkingPhilosophers/Slide8.jpg\" style=\"width:100%\"/>\n<figcaption>Fig.8: Stage 6</figcaption>\n</figure>\n\nIn stage 7, Liu has received both milk and coffee, and so she is\ndrinking. Maya is still thirsty, holding tea, while Maya's request for\nmilk is in the queue of pending requests for milk.\n<figure>\n<img alt=\"Fig9\" src=\"raw_webcrawl_data/DistributedCollaboration/DrinkingPhilosophers/Slide9.jpg\" style=\"width:100%\"/>\n<figcaption>Fig.9: Stage 7</figcaption>\n</figure>\n</p>", "6": "<h2 class=\"w3-text-teal\">How Priorities Change</h2>\nIf priorities don't change then the agents with high priorities\nmay continue to go rapidly through the tranquil, thirsty, drinking\ncycle while other agents remain thirsty for ever.\n\n<p>\nOne way to assign priorities is as follows.\nAssociated with each request is a timestamp which is \nthe time read from the requestor's local clock at the instant at which\nthe request is made. A request's timestamp does not change after the\nrequest is created.\n</p><p>\nA request's priority is a pair (timestamp,\nrequestor's id), with priorities compared lexicographically, and lower\nvalues having higher priority. So, requests made earlier have higher\npriority than requests made later. The requestor's id is used to break\nties.\n</p><p>\nWhat are the requirement's of agents' local\nclocks that ensure that all thirsty philosophers drink eventually?\n\n</p><p class=\"w3-text-teal\">A Minimum Requirement on Clocks</p>\n<i>An agent's clock must tick forward</i> between successive requests\nby the agent. If the agent's clock remained stuck at the same\nvalue for ever then the agent using that clock may get the highest\npriority for ever, and go through tranquil, thirsty, drinking cycles\ninfinitely which makes other agents remain tranquil for ever.\n\n<p>\nAssume that each clock reading is an integer: for example, the number of\npicoseconds since January 1, 1900. (NTP units are \\(2^{-32}\\) of a\nsecond.) The clock ticks forward\nbetween successive requests by the same philosopher because the\nphilosopher remains in tranquil state for at least \\(\\gamma\\) units of\ntime where \\(\\gamma\\) is a positive constant. Treat each agent's clock\ntick as an event.\n\n</p><p>\nLocal clocks of agents may drift apart, but let's assume that the\nmagnitude of the difference between clock readings of different agents\nis bounded.\n\n</p><p>\nEven if the\nphilosopher makes a new request when its clock ticks forward by one\nunit, eventually the timestamp of a request from that philosopher will\nexceed \\(T\\) for any value of \\(T\\). This ensures that while a\nphilosopher is thirsty, another philosopher cannot overtake it for\never.\n\n</p>", "7": "<h4 class=\"w3-text-teal\">Proof of Correctness</h4>\nThe proof of safety --- multiple philosophers don't hold the same\nbeverage at the same time --- is straightforward. It follows\nbecause a token is at exactly one agent or in exactly one channel.\n\n<p>\nLet's prove that a thirsty philosopher \\(v\\) with a request with timestamp\n\\(T\\) drinks eventually. Let \\(t[i]\\) be the reading of philosopher\n\\(i\\)'s clock.\n\n</p><p class=\"w3-text-teal\">Each agent's clock reading eventually exceeds \\(T\\)</p>\n\nWe observe that each agent's clock ticks forward:\n<p>\n\\(\n\\forall i, \\tau: (t[i] = \\tau) \\leadsto (t[i] \\geq \\tau + 1)\n\\)\n</p><p>\nTherefore, each \\(i\\)'s clock eventually reads a\nvalue greater than \\(T\\), using transitivity of \\(\\leadsto\\).\n</p><p>\n\\(\n\\forall i, \\tau: (t[i] = \\tau) \\leadsto (t[i] &gt; T)\n\\)\n</p><p>\nFrom the previous formula, and taking disjunction over all values of\n\\(\\tau\\): \n</p><p>\n\\(\n\\forall i:  \\quad true \\leadsto (t[i] &gt; T)\n\\)\n</p><p>\nBecause clock readings never go back in time, \\(t[i] &gt; T\\) is stable,\nand therefore:\n</p><p>\n\\(\n\\forall i:  \\quad true \\leadsto always(t[i] &gt; T)\n\\)\n</p><p>\nLet \\(Q\\) be the predicate that all clocks read values that\nexceed \\(T\\):\n</p><p>\n\\(Q = \\forall i: t[i] &gt; T\\).\n</p><p>\nFrom the above formula and because \\(X \\leadsto always(Y)\\) and \\(X\n\\leadsto always(Y')\\) allows us to deduce \\(X \\leadsto always(Y \\vee Y')\\)\n</p><p>\n\\(\n true \\leadsto always(Q)\n\\)\n\n</p><p class=\"w3-text-teal\">The number of pending requests with timestamps\nless than \\(T\\) decreases</p>\n<p>\nFrom the above formula:\n</p><p>\n\\(\nv.thirsty \\; \\leadsto \\; (v.drinking \\wedge Q) \\vee (v.thirsty \\wedge Q)\n\\)\n</p><p>\nNext we will prove that if \\(v\\) is thirsty and all clocks read values\nthat exceed \\(T\\), then \\(v\\) will drink eventually:\n</p><p>\n\\(\n (v.thirsty \\wedge Q) \\leadsto v.drinking\n\\)\n</p><p>\nLet \\(req\\) be the number of pending requests with timestamps less than\n\\(T\\). We will prove \n</p><p>\n\\(\n(v.thirsty \\wedge Q \\wedge (req = k) \\; \\leadsto \\;\nv.drinking \\vee (v.thirsty \\wedge Q \\wedge (req &lt; k))\n\\)\n</p><p>\nThis proof is straightforward: The request with the lowest timestamp\ngets all the resources it needs.\n</p><p>\n\\(\n(v.thirsty \\wedge Q \\wedge (req = 0) \\; \\leadsto \\;\nv.drinking \n\\)\n</p><p>\nThe result follows using the rules\nfor variant functions.\n\n</p>"}, "raw_webcrawl_data/DistributedCollaboration/DiningPhilosophers/DiningPhilosophers.html": {"0": "<h1 class=\"w3-text-teal\">Dining Philosophers</h1>\n\n", "1": "<h4 class=\"w3-text-teal\">\nThis module describes algorithms for \"The Distributed Dining\nPhilosophers Problem,\" a problem that exemplifies mutual exclusion of\ncritical operations among agents in a distributed setting.\n</h4>\n\n\nThis and following modules describe algorithms by which agents manage\nconflicts among themselves. This module describes an\n    algorithm for a distributed mutual exclusion problem called\n    <a href=\"https://en.wikipedia.org/wiki/Dining_philosophers_problem\">\n<i>The Distributed Dining Philosophers Problem</i>\n</a>.\n\n\n", "2": "<h3 class=\"w3-text-teal\">Key Concepts</h3>\n\n", "3": "<h4 class=\"w3-text-teal\">1. Mutual Exclusion</h4>\n    The distributed dining philosophers is a generalization of the\n    mutual exclusion problem to distributed systems.  A <i>mutual\n    exclusion</i> or <i>mutex</i> problem is one in which only one\n    agent can carry out some activity such as executing a <i>critical\n    section</i> of a program. Mutex algorithms ensure that all agents\n    that want to carry out such activities do so eventually. \n\n    ", "4": "<h4 class=\"w3-text-teal\">2. Priorities among Agents</h4>\n    Distributed conflict-resolution algorithms ensure that\n    when multiple agents are in a conflict that only one of them can\n    win, every agent wins eventually. A\n    standard way of managing conflicts is to have agents agree on\n    relative priorities among themselves; the agent with higher\n    priority wins.\n\n    <p>\n    Distributed algorithms often use a good-neighbor policy to ensure\n    that all agents win conflicts eventually: <i>An agent that wins a\n    conflict makes its priority lower than the priorities of all the\n    agents with which it competes.</i>\n</p>", "5": "<h4 class=\"w3-text-teal\">3. Tokens and What Agents Know</h4>\n    An agent can resolve a conflict with other agents only if it knows\n    something about the states of other agents. For example, what\n    agents want to enter a critical section and what are their\npriorities?\n\n    <a href=\"../Knowledge/Knowledge.html\">\n    What agents know is defined in this module.</a> We used the\n    concept of tokens to illustrate what agents know.  A system has a\n    fixed number of indivisible tokens which are neither created nor\n    destroyed. An agent that holds a token knows that no other agent\n    holds that token.  This knowledge is at the core of many conflict\n    resolution algorithms.\n\n\n\n", "6": "<h2 class=\"w3-text-teal\">The Problem</h2>\n\n", "7": "<h4 class=\"w3-text-teal\">Agent States in a Mutual Exclusion Problem</h4>\n<figure>\n<img alt=\"Fig2\" src=\"raw_webcrawl_data/DistributedCollaboration/DiningPhilosophers/Slide02.jpg\" style=\"width:100%\"/>\n<figcaption>Fig.2: States in Mutual Exclusion</figcaption>\n</figure>\n\nAn agent in a mutual exclusion problem is in one of three states:\n<ol>\n<li>\n<i>Outside critical section</i>:\n  The agent is executing outside its critical section. An agent can remain in this\n  state for ever, or it may transit after finite time to the next state: waiting to\n  enter its critical section.\n  </li>\n<li>\n<i>Waiting to enter critical section</i>:\n  The agent waits to enter critical section until it is given\n  permission to do so by the operating system.\n  </li>\n<li>\n<i>In critical section</i>:\n  The agent executes in its critical section.  An agent does not\n  remain in its critical section forever. It does so for only\n  <i>finite</i> time after which it transits to the state, outside\n  critical section.\n  </li>\n</ol>\n<p>\nClients determine transitions from <i>outside critical section</i> to <i>waiting to\nenter critical section</i>, and from <i>in critical section</i> to\n<i>outside critical section</i>.\n\nThe operating system determines transitions from <i>waiting to\nenter critical section</i> to <i>outside critical section</i>.\n\n\n\n</p>", "8": "<h4 class=\"w3-text-teal\">Agent States in the Dining PhilosophersProblem</h4>\n<figure>\n<img alt=\"Fig3\" src=\"raw_webcrawl_data/DistributedCollaboration/DiningPhilosophers/Slide03.jpg\" style=\"width:100%\"/>\n<figcaption>Fig.3: States in Dining Philosophers</figcaption>\n</figure>\n\nThe name \"Dining Philosophers\" is an example of CS humor\n(an oxymoron?).\nPhilosophers may think for ever, but eat for only finite time. The\nalgorithm must ensure that hungry philosophers don't starve --- they\nget to eat <i>eventually</i>.\nThe problem and its name were proposed by Edsger W. Dijkstra, a CS pioneer.\n\n<p>\nThe states \"Thinking\", \"Hungry\", and \"Eating\" correspond exactly to\n<i>Outside critical section</i>, <i>Waiting to enter critical\nsection</i>, and <i>In critical section</i>, respectively.\n\n\n</p>", "9": "<h3 class=\"w3-text-teal\">Agent Communication Structure</h3>\n<figure>\n<img alt=\"Fig4\" src=\"raw_webcrawl_data/DistributedCollaboration/DiningPhilosophers/Slide04.jpg\" style=\"width:100%\"/>\n<figcaption>Fig.4: Communication among Agents</figcaption>\n</figure>\n\nThe commununication structure among agents is represented by an\nundirected graph in which the nodes are agents and each edge\nrepresents two channels, one in each direction. The agents are either\nOS or client agents. There is one client agent associated with each OS\nagent. Clients are shown as squares and OS agents as circles.\nFor example client agent \\(w\\) is associated with OS agent\n\\(w\\). The diagram does not show all clients so as not to make the\ndiagram too crowded.\n\n<p>\nA pair of OS agents are neighbors when there is an edge between\nthem. A pair of client agents are neighbors when the OS agents with\nwhich they are associated are neighbors. For example, in the figure, \n\\(w\\) and \\(y\\) are neighbors.\n\n\n</p>", "10": "<h2 class=\"w3-text-teal\">Specification</h2>\n", "11": "<h3 class=\"w3-text-teal\">Safety: Neighbors do not eat at the same time</h3>\nLet \\(safe\\) be the predicate <i>Neighboring clients are\nnot eating</i>, and let \\(init\\) be a predicate that holds initially.\n<p>\nThe safety part of the specification is that \\(safe\\) holds in every state in every path from\nevery initial state:\n</p><p>\n\\([init\n\\Rightarrow A(safe)]\\)\n\n\n\n</p>", "12": "<h3 class=\"w3-text-teal\">Progress: Hungry agents eat eventually</h3>\nThe progress part of the specification is that every hungry agent\ntransits to eating state eventually.\n<p>\nFor every agent \\(v\\):\n<br/>\n\\(\nv.state = hungry \\quad \\leadsto \\quad v.state = eating\n\\)\n\n</p>", "13": "<h3 style=\"color:red;\">Example of Safety</h3>\n<figure>\n<img alt=\"Fig5\" src=\"raw_webcrawl_data/DistributedCollaboration/DiningPhilosophers/Slide05.jpg\" style=\"width:100%\"/>\n<figcaption>Fig.5: Diagrams illustrating Safety</figcaption>\n</figure>\n\nFigure 5 shows a client eating as a red node depicting the client and\nits OS agent. An uncolored node represents a client that is thinking\nor hungry. The diagram on the left shows the system in a safe state:\nthere are no edges between red vertices. The diagram on the right\nshows an unsafe state because there are edges between red vertices.\n\n\n", "14": "<h3 class=\"w3-text-teal\">The Client's Program</h3>\n<p>\nWe use two tokens that move\nbetween a client and its OS agent. The tokens are called the\n<i>resource token</i> and the <i>request token</i>. The client's\nstates are represented by which tokens the client holds.\n\n</p><ol>\n<li><i>Thinking State</i>: A thinking client holds the request token but\n  not the resource token.\n  </li>\n<li><i>Transition: Thinking to Hungry</i>: Send the request token to\n  the OS.\n  </li>\n<li><i>Hungry State</i>: The client holds neither the request nor the\n  resource token.\n  </li>\n<li><i>Transition Hungry to Eating</i>: The client transits to\n  eating when it receives both the request and resource token.\n  </li>\n<li><i>Eating</i>: The client holds both the request and resource tokens.\n  </li>\n<li><i>Transition from Eating to Thinking</i>:\n  The client holds sends the resource token to the OS and continues to\n  hold the request token.\n  </li>\n</ol>\n\nThe figure below illustrates the states of the client. The request\n    token is shown as a square and the resource token as a circle.\n    <figure>\n<img alt=\"Fig6\" src=\"raw_webcrawl_data/DistributedCollaboration/DiningPhilosophers/Slide06.jpg\" style=\"width:100%\"/>\n<figcaption>Fig.6: Client's Program</figcaption>\n</figure>\n\nInitially all clients are thinking; all resource tokens are with OS\nagents; and all request tokens are with clients.\n\n", "15": "<h3 class=\"w3-text-teal\">What the OS Knows</h3>\nWhile the OS agent holds the request token it knows that its client is\nhungry. While the OS agent holds the resource token it knows that its client is\nnot eating.\n    <figure>\n<img alt=\"Fig6\" src=\"raw_webcrawl_data/DistributedCollaboration/DiningPhilosophers/DiningPhilosophers/Slide23.jpg\" style=\"width:100%\"/>\n<figcaption>Fig.7: OS Agent's Program</figcaption>\n</figure>\n<p>\nWhen a client transits from thinking to hungry it sends its request\ntoken to the agent. When the OS receives the request token, the OS\nalso has the resource token; so the OS knows that its client is\nhungry. There is an interval after the client sends the request token\nand before the OS receives it during which the client is hungry but\nthe OS doesn't know that. An OS agent does not need to know \nwhat state its client is in at every point. Likewise, a client does\nnot need to know its OS agent's state at every point.\n</p><p>\nA client is hungry <i>leads-to</i> its OS agent knowing that its client is\nhungry. The <i>leads-to</i> captures the fact that the OS doesn't know\nthe client's state at the instant that the client transitions from\nthinking to hungry.\n\n</p>", "16": "<h2 class=\"w3-text-teal\">Introduction of Tokens</h2>\nWe introduce a token on each edge of the agent communication graph,\nsee figure 4. The token on an edge \\(v, w\\) is in one of four states:\nheld by \\(v\\), in the channel from \\(v\\) to \\(w\\), held by \\(w\\), or\nin the channel from \\(w\\) to \\(v\\). Therefore while \\(v\\) holds this\ntoken it knows that \\(w\\) doesn't hold it. Likewise, while \\(w\\) holds\nthis token it knows that \\(v\\) doesn't.\n\n    <figure>\n<img alt=\"Fig7\" src=\"raw_webcrawl_data/DistributedCollaboration/DiningPhilosophers/Slide07.jpg\" style=\"width:100%\"/>\n<figcaption>Fig.7: Fork on each edge of the agent communication\n    graph</figcaption>\n</figure>\n<p>\nThese tokens are called <i>forks</i>. (They are called\n<i>chopsticks</i> in some papers.) An agent eats only if it holds\nforks for all the edges incident on it. Therefore, while an agent eats\nnone of its neighbors do, and so the safety specification is satisfied.\n\n\n</p>", "17": "<h3 class=\"w3-text-teal\">Key Question: When does a hungry agent yield\nforks?</h3>\nAn eating philosopher holds on to all its forks until it finishes\neating. A thinking philosopher can give a fork to a neighbor that\nrequests it. So the key question is: <i>Under what conditions should a\nhungry neighbor give a fork that it holds to a neighbor that requests\nit?</i>\n<p>\nSuppose every hungry agent gives a fork to a neighbor that requests\nit. Then we can get the scenario shown in the figure below.\n\n    <figure>\n<img alt=\"Fig8\" src=\"raw_webcrawl_data/DistributedCollaboration/DiningPhilosophers/Slide08.jpg\" style=\"width:100%\"/>\n<figcaption>Fig.8: Scenario when hungry agents yield forks</figcaption>\n</figure>\nThe figure shows a system with 3 agents indexed 0, 1, 2. The forks are\n    shown as small colored circles. The state on\n    the left shows agent \\(j\\) holding the fork that it shares with\n    agent \\((j+1)\\:\\textrm{mod}\\:3\\). If each agent yields the fork to\n    its neighbor we get the state on the right in which agent \\(j\\)\n    holds the fork that it shares with \n    agent \\((j-1)\\:\\textrm{mod}\\:3\\). So, if all hungry agents yield\n    forks, the states can alternate for ever between the one on the\nleft and the one on the right. In this case hungry agents starve: they\n    remain hungry forever.\n\n</p><p>\nIf hungry agents don't yield forks then the state on the left persists\nfor ever. In this case too, hungry agents starve.\n\n\n\n</p>", "18": "<h3 class=\"w3-text-teal\">Creating a Partial Order of Priorities</h3>\nLet's assign priorities to agents so that a hungry agent \\(v\\)\nreleases a fork to a neighbor \\(w\\) only if \\(v\\) has lower priority\nthan \\(w\\). If there is a cycle of agents, all with the same priority,\nthen we get the situation shown in figure 8. So, we will ensure that\npriorities form a partial order in all states in all\ntransitions. Priorities form a partial order exactly when the priority\ngraph is acyclic. The graph has an edge from \\(v\\) to \\(w\\) exactly\nwhen \\(v\\) has higher priority over \\(w\\).\n\n<p>\nIn the figure below, the diagram on the left shows an acyclic priority\ngraph while the one on the right shows a graph with a cycle.\n\n    <figure>\n<img alt=\"Fig9\" src=\"raw_webcrawl_data/DistributedCollaboration/DiningPhilosophers/Slide09.jpg\" style=\"width:100%\"/>\n<figcaption>Fig.9: Priority Graph must be Acyclic</figcaption>\n</figure>\n\n</p>", "19": "<h3 class=\"w3-text-teal\">How should Priorities Change?</h3>\n<figure>\n<img alt=\"Fig10\" src=\"raw_webcrawl_data/DistributedCollaboration/DiningPhilosophers/Slide10.jpg\" style=\"width:100%\"/>\n<figcaption>Fig.10: How should priorities change when \\(v\\)\n    eats?</figcaption>\n</figure>\n\nHow should priorities change when an agent eats so that the priority\ngraph remains acyclic? For example, consider\nthe priority graph shown in figure 10. Assume agent \\(v\\) has all its\nforks and is about to eat. Should the directions of edges incident on\n\\(v\\) be flipped? Or should \\(v\\) have lower priority than all its\nneighbors, i.e. all edges incident on \\(v\\) point towards \\(v\\)?\n\n<p>\nWhat happens if we flip the directions of the edges incident on \\(v\\)?\nAfter the flip, the edges are directed from \\(w\\), \\(x\\) and \\(y\\) towards \\(v\\), and\nfrom \\(v\\) to \\(u\\). But now we have a cycle: \\(y\\) to \\(v\\) to \\(u\\)\nto \\(w\\) to \\(y\\). So, flipping edge directions doesn't work.  \n\n</p><p>\nWhat happens if agents adopt the good neighbor policy?\nThe winner of a conflict gives itself lower priority than <i>all</i> the agents\nwith which it competes. So, an agent that starts eating gives itself lower\npriority than all its neighbors. All edges point towards an eating\nagent. \n<figure>\n<img alt=\"Fig11\" src=\"raw_webcrawl_data/DistributedCollaboration/DiningPhilosophers/Slide11.jpg\" style=\"width:100%\"/>\n<figcaption>Fig.11: Winner gets lower priority than its neighbors</figcaption>\n</figure>\nWhen all edges incident on a vertex point towards the vertex then\nthere is no cycle through that vertex. So, directing all edges\nincident on an eating vertex towards the eating vertex maintains\nacyclicity of the graph.\n</p><p>\nFor example, in the figure directing all edges incident on vertex \\(v\\) towards\n \\(v\\) ensures that no new cycle is created.\n\n</p>", "20": "<h4 class=\"w3-text-teal\">\nAgent's priority does not decrease until the agent\neats.</h4>\n", "21": "<h3 class=\"w3-text-teal\">How an Agent knows its Priority</h3>\nWe assign an attribute <i>clean / dirty</i> to forks.\nA fork is either <i>dirty</i> or <i>clean</i>.\nThe forks held by an eating agent are <i>dirty</i>. When\nan agent receives a fork from another agent, the receiving agent\ncleans the fork. So the receiver holds a <i>clean</i> fork, and the\nfork remains clean until an agent next eats with it.\n(This is the \"hygenic solution:\" Another - sad? - attempt at CS\nhumor.)\n\n<p>\nAn agent holding a dirty fork knows that it has lower priority than\nthe agent with which it shares the fork. Likewise,\nan agent holding a clean fork knows that it has higher priority than\nthe agent with which it shares the fork.\n</p><p>\nIf an agent does not hold the fork that it shares with a neighbor then\nthe agent does not know its priority with respect to that neighbor.\n\n</p>", "22": "<h2 style=\"color:red;\">Example of a Fork's Lifecycle</h2>\nThe diagram below shows states of a fork shared by agents \\(u\\) and\n\\(v\\). The red arrow shows priority, and the black arrows show\nchannels. The blue dot represents the fork.\n\n<p>\nIn the figure on the top left, agent \\(u\\) is hungry and holds a clean\nfork. So, \\(u\\) knows that it has priority over \\(v\\). At this point\n\\(v\\) does not know whether \\(v\\) has priority over \\(u\\) or not.\n</p><p>\nThe next figure, at the top right, shows that when \\(u\\) transits from\nhungry to eating, the fork becomes dirty, and \\(u\\) has lower priority\nthan \\(v\\). Agent \\(u\\) continues to hold the fork while it eats.\n</p><p>\nThe next figure, bottom right, shows the situation after \\(u\\) gets a\nrequest for the fork from \\(v\\). Because \\(u\\) got the request from\n\\(v\\) and \\(u\\) hasn't sent the fork to \\(v\\), agent \\(u\\) knows that\n\\(v\\) is hungry. Since the fork is dirty, \\(u\\) sends\nthe fork to \\(v\\). The figure shows the fork in the channel from \\(u\\)\nto \\(v\\). While the fork is in the channel it doesn't matter\nwhether the fork is clean or dirty; however, merely for convenience,\nlet's assume that \\(u\\), being hygenic, cleans the fork before sending\nit to its partner. While the fork is in a\nchannel the priority doesn't change but neither \\(u\\) nor \\(v\\) knows\nwhat the priority is. \n</p><p>\nThe next figure, bottom left, shows the situation when \\(v\\) receives\nthe fork.  Receiving the fork doesn't change the priority.\nAt this point \\(v\\) is hungry and the fork is\nclean and so \\(v\\) knows that it has higher\npriority. \\(v\\) holds on to the fork until it next eats.\n\n<figure>\n<img alt=\"Fig12\" src=\"raw_webcrawl_data/DistributedCollaboration/DiningPhilosophers/Slide12.jpg\" style=\"width:100%\"/>\n<figcaption>Fig.12: How an Agent knows its Priority</figcaption>\n</figure>\n</p>", "23": "<h2 class=\"w3-text-teal\">Algorithm</h2>\n", "24": "<h3 class=\"w3-text-teal\">Properties of Reachable States</h3>\nHere is a list of some of the properties of states in all\ntrajectories. \n<ol>\n<li>\n  The priority graph is acyclic. \\(u\\) has priority over a neighbor\n  \\(v\\) exactly when \\(u\\) holds the fork that it shares with \\(v\\)\n  and the fork is clean, or the fork is in the channel from \\(v\\) to\n  \\(u\\), or \\(v\\) holds the fork and the fork is dirty.\n  </li>\n<li>\n  Eating philosophers hold the forks for all edges incident on them,\n  and these forks are dirty.\n  </li>\n<li>\n  All forks held by thinking philosphers are dirty.\n  </li>\n<li>\n  Thinking philosophers never send requests and never receive\n  forks. Thinking philosophers respond to request for forks by sending\n  the requested forks.\n  </li>\n</ol>\n", "25": "<h3 class=\"w3-text-teal\">Initial States</h3>\nInitially all philosophers are thinking; all forks are dirty; and all channels are\nempty. The forks are placed so that the priority graph is acyclic.\nThe initial assignment of forks is as follows.\nGiven an arbitrary acyclic graph, for any edge directed from \\(v\\) to\n\\(w\\), the fork shared by \\(v\\) and \\(w\\) is initially at \\(w\\) and\nthe fork is dirty. \n\n", "26": "<h3 class=\"w3-text-teal\">Algorithm Commands</h3>\n<p>\nThe algorithm is specified by the following commands.\n</p><ol>\n<li>\n  When a thinking philosophers gets a request for a fork that it holds\n  it sends the fork. (A fork held by a thinking philosopher is dirty.)\n  </li>\n<li>\n  When a thinking philosopher transits to hungry it sends requests for\n  all forks that it does not hold.\n  </li>\n<li>\n  When a hungry philosopher receives a fork, it records the fork as\n  being clean. If the hungry philosopher holds all its forks, and if\n  it has no request for any dirty fork that it holds, then it transits\n  to eating, and records all the forks that it holds in the eating\n  state as dirty. \n  </li>\n<li>\n  When a hungry philosopher receives a request for a fork that it\n  holds, it sends the fork if the fork is dirty, and holds on to the\n  fork if it is clean.\n  </li>\n<li>\n  When an eating philosopher receives a request for a fork it\n  registers the request in memory, and continues eating while holding\n  the fork. When an eating philosopher transits to thinking it sends\n  forks for all requests that it received.\n  </li>\n</ol>\n", "27": "<h4 class=\"w3-text-teal\">What could go wrong?</h4>\nThe proof of safety is straightforward: Neighbors aren't eating\nbecause neighbors can't hold the same fork at the same time.\n\n<p>\nBefore we look at the proof of progress, let's see what may go wrong.\n\n</p><p>\nCould a group of philosophers exchange forks with each other so that\nmembers of the group eat repeatedly, and starve a philosopher who is\nnot in the group? For example, in the figure below, could philosophers\n\\(u, v, w\\) exchange forks so that they each eat in turn, and starve\n\\(y\\)? \n<figure>\n<img alt=\"Fig13\" src=\"raw_webcrawl_data/DistributedCollaboration/DiningPhilosophers/Slide13.jpg\" style=\"width:100%\"/>\n<figcaption>Fig.13: Potential Problems: What could go wrong?</figcaption>\n</figure>\n</p><p>\nCould the system enter a deadlock state in which each hungry philosopher in a\ngroup holds only some --- but not all --- of the forks that it needs\nto eat, while other members of the group hold the remaining forks?\n\n</p>", "28": "<h2 class=\"w3-text-teal\">Proof of Correctness</h2>\nThe algorithm is correct. We are required to prove that every hungery\nphilosopher eats eventually:\n<p>\n\\(\n\\forall v: \\quad v.h \\leadsto v.e\n\\)\n</p><p>\nwhere for a philosopher \\(v\\), \\(v.h\\) holds exactly when \\(v\\) is\nhungry and \\(v.e\\) holds exactly when \\(v\\) is\neating.\n\n</p>", "29": "<h3 class=\"w3-text-teal\">Variant Function</h3>\nTo prove this progress property we find a variant function that\nsatisfies the following two conditions:\n<ol>\n<li>\n<i>Safety</i>: The variant function does not increase while \\(v\\)\n  remains hungry. \n  </li>\n<li>\n<i>progress</i>: The following predicate does not hold forever: The\n  variant function remains unchanged and \\(v\\) remains hungry.\n  </li>\n</ol>\n<p>\nWe propose a variant function which is a pair of integers\n \\(nT, nH\\),  which are the number of thinking and hungry philosophers,\nrespectively, of \nhigher priority than \\(v\\).\nIn terms of the priority graph, \\(nT, nH\\) are the numbers of thinking\nand hungry philosophers (i.e. vertices) with paths to \\(v\\). \n\n</p>", "30": "<h3 style=\"color:red;\">Example of Variant Function</h3>\nThe figure below shows a portion of the priority graph in a state of the\nsystem. The figure only shows philosophers with higher priority than\nphilosopher \\(v\\), i.e., it only shows vertices in the graph with\npaths to \\(v\\). Since eating philosophers have lower priority than\ntheir neighbors, eating philosophers don't appear in this graph.\n<p>\nA hungry philosopher is marked with an \"H\" and a thinking philosopher\nwith a \"T.\" In the diagram, philosophers \\(v, w, x, y\\) are hungry and\n\\(u\\) is thinking.\nForks are located at philosophers and are shown as small\ncolored circles. A dirty fork is colored red and clean one is\nblue. For example the fork shared by \\(v\\) and \\(y\\) is at \\(y\\) and\nis clean.\n\n\n<figure>\n<img alt=\"Fig15\" src=\"raw_webcrawl_data/DistributedCollaboration/DiningPhilosophers/Slide15.jpg\" style=\"width:100%\"/>\n<figcaption>Fig.15: A Variant Function: Numbers of higher priority\n    thinking, hungry agents</figcaption>\n</figure>\n</p>", "31": "<h3 style=\"color:red;\">Example of Changes to Variant Function</h3>\nThe next figure is an example of changes to the variant function. The\ndiagram on the top left shows the higher priority vertices in the\nstate of the previous figure. If agent \\(x\\) eats next the priority\ngraph transits to the diagram on the top right, and the variant\nfunction \\((nT, nH)\\) changes from \\((1, 3)\\) to \\((1, 2)\\). \n\n\n\n<figure>\n<img alt=\"Fig16\" src=\"raw_webcrawl_data/DistributedCollaboration/DiningPhilosophers/Slide16.jpg\" style=\"width:100%\"/>\n<figcaption>Fig.16: Example of Values of the Variant Function</figcaption>\n</figure>\n\n\nIf agent \\(y\\) eats next the priority\ngraph transits to the diagram on the bottom right, and the variant\nfunction \\((nT, nH)\\) changes from \\((1, 2)\\) to \\((1, 1)\\).\n\nIf agent \\(w\\) eats next the priority\ngraph transits to the diagram on the bottom left, and the variant\nfunction \\((nT, nH)\\) changes from \\((1, 1)\\) to \\((0, 0)\\).\n\n", "32": "<h4 class=\"w3-text-teal\">Proof that the variant function does not increase\nwhile \\(v\\) remains hungry</h4>\n<p>\nIf a philosopher of higher priority than \\(v\\) transits from thinking\nto hungry then \\(nT\\) decreases. Though \\(nH\\) increases, the variant function \\((nT,\nnH)\\) decreases because ordering of function values is done\nlexicographically.\n</p><p>\nIf a philosopher of higher priority than \\(v\\) transits from hungry\nto eating then \\(nH\\) decreases,  and so the variant function \\((nT,\nnH)\\) decreases.\n\n</p>", "33": "<h4 class=\"w3-text-teal\">Proof that the following predicate does not\nhold forever: The variant function remains unchanged and\n\\(v\\) remains hungry.\n</h4>\n<p>\nLet \\(w\\) be a highest-priority hungry philosopher, i.e. a philosopher with no\nhungry philosopher with priority higher than \\(w\\). (Note: \\(w\\) may be\nthe same as \\(v\\).)\nAll philosophers with priority higher than \\(w\\) are thinking. In the\nnext paragraph we show that either \\(w\\) gets all its forks and then transits\nfrom hungry to eating, or the variant function decreases.\n\n</p><p>\nFrom the algorithm, a hungry philosopher \\(w\\) requests forks from its\nneighbors. From the algorithm, \\(w\\) eventually gets forks from \nall its lower priority neighbors. A higher priority neighbor \\(x\\) of\n\\(w\\) is thinking. So when \\(x\\) gets a request for a fork from \\(w\\)\neither (1) \\(x\\) sends the requested fork to \\(w\\) or (2) \\(x\\) transits from\nthinking to hungry in which case the variant function \\((nT, nH)\\)\ndecreases. \n\n</p>", "34": "<h2 class=\"w3-text-teal\">Summary: Key Ideas of this Module</h2>\nThis module introduced the problem of distributed mutual exclusion;\nshowed how the good neighbor policy --- a winning agent reduces its\npriority to be lower than all the agents that it competes with --\nsolves this conflict resolution problem; introduced tokens and what\nagents know about other agents holding tokens; and showed a proof\npattern that is one of the most common patterns for proving progress.\n\n"}, "raw_webcrawl_data/ChannelSnapshots/DatabaseDeadlockDetection.html": {"0": "<h2 class=\"w3-text-teal\">Detecting Database Deadlocks</h2>\n\n<p class=\"w3-text-red\">\n  A database deadlock occurs when each agent in a cycle remains\n  waiting forever for a resource held and required by the next agent\n  in the cycle. \n\n  A database deadlock detection algorithm is executed by the operating\n  system to determine whether there exists a cycle of deadlocked agents.\n\n  </p>\n", "1": "<h3 class=\"w3-text-teal\">The Problem</h3>\n<p>\n  The problem described here is a simplification of the database\n  deadlock problem.\n\n  The simplification focuses on the essentials of the problem.\n\n  </p><p>\n  Agents in a system share a set of indivisible resources.\n\n  An example of such a resource is exclusive access to a file.\n\n  </p><p>\n  A deadlock arises when there is a cycle of agents,\n  \\([x_{0}, x_{1}, \\ldots, x_{n-1}, x_{0}]\\) where for all \\(i\\):\n  </p><ol>\n<li>\n    agent \\(x_{i}\\) holds a resource \\(r_{i}\\), and\n    </li>\n<li>\n    agent \\(x_{i}\\) requires resources \\(r_{i}\\) and \\(r_{(i+1)}\\) to\n    continue executing.\n    </li>\n</ol>\n\n  \n    (Operations on indices of agents are taken mod \\(n\\) and \\(n &gt;\n    1\\).) \n\n  \n", "2": "<h3 style=\"color:red;\">Example</h3>\n\n  \n  In the example, a resource is identified by its color.\n  \n  A system has one red, one blue, and one green resource.\n  \n  Agents \\(x\\), \\(y\\) and \\(z\\) are deadlocked in the following\n  state.\n  <ol>\n<li>\n  Agent \\(x\\) requires the red and blue resources to continue\n    executing; \\(x\\) is holding the red resource and is waiting to\n    acquire the blue resource.\n    </li>\n<li>\n  Agent \\(y\\) requires the blue and green resources to continue\n    executing; \\(y\\) ; is holding the blue resource and is waiting to\n    acquire the green resource.\n    </li>\n<li>\n  Agent \\(z\\) requires the green and red resources to continue\n  executing; \\(z\\) holds the green resource and is waiting to acquire\n    the red resource.\n    </li>\n</ol>\n<figure>\n<img alt=\"Fig2\" src=\"raw_webcrawl_data/ChannelSnapshots/DatabaseDeadlockDetection/DatabaseDeadlockDetection.002.jpeg\" style=\"width:100%\"/>\n<figcaption>Fig.2 - An Example of a Deadlock</figcaption>\n</figure>\n\n", "3": "<h3 class=\"w3-text-teal\">How Should You Solve the Problem?</h3>\n\n<p class=\"w3-text-teal\">Strategy</p>\n\n  A strategy to solve detection problems is to start with the\n  <a href=\"ChannelSnapshots.html\">general detection algorithm</a> and\n  then explore optimizations by using\n  <a href=\"../DISTRIBUTED_SYSTEM_MODELS/Timelines.html\">\n  properties of cuts.</a>\n<p>\n  In the general detection algorithm, an observer gets global snapshots and\n  determines if there is a cycle of waiting agents in the snapshot.\n\n  Algorithms for determining cycles are found\n  <a href=\"https://en.wikipedia.org/wiki/Cycle_(graph_theory)\">here</a>.\n\n  </p><p>\n  Next let's explore\n  <a href=\"ChannelSnapshots.html\">\n  optimizations\n  </a>;\n  see \"Detection without Observers: Distributed Algorithms on Local\n  Snapshots.\"\n\n  The optimized algorithm has two phases: (1) First a global snapshot\n  algorithm is executed. (2) After the global snapshot algorithm\n  terminates a distributed detection algorithm is executed on\n  the local snapshots recorded by the global snapshots.\n\n  </p><p>\n  The two phases can be merged for some problems, including this one.\n\n\n\n\n  \n</p>", "4": "<h4 class=\"w3-text-teal\">\n  A Distributed Algorithm to Detect a Cycle of\n  Waiting Agents\n  </h4>\n\n<p>\n  Multiple detection algorithms and global snapshot algorithms may\n  execute concurrently.\n  \n  These algorithms are disambiguated by tagging each algorithm with\n  the initiator and a sequence id.\n\n  Next we describe a single detection algorithm which is executed after \n  a single global snapshot algorithm terminates.\n\n  </p><p class=\"w3-text-teal\">Local Constants</p>\n  In the detection algorithm, \neach agent <code>v</code> has the following local constants:\n<ol>\n<li> <code>v.waits</code> is the set\n  of resources that <code>v</code> has to acquire from other agents to start\n  execution.\n  </li>\n<li>\n<code>v.holds</code> is the set of resources that <code>v</code>\n  holds and must continue to hold\n  to start execution.\n  </li>\n</ol>\n<p style=\"color:red;\">Example</p>\nIn the example of the figure:\n<br/>\n<code>x.waits = {blue}, x.holds = {red}</code>\n<br/>\n<code>y.waits = {green}, y.holds = {blue}</code>\n<br/>\n<code>z.waits = {red}, y.holds = {green}</code>\n<p>\n<code>v.waits</code> and <code>v.holds</code> are constant for all <code>v</code> in the\ndetection algorithm  because these values are\nspecified in the global snapshot.\n\n\n  </p><p class=\"w3-text-teal\">Messages</p>\n  The algorithm to detect waiting cycles is similar to the global\n  snapshot algorithm.\n\nInstead of the <code>marker</code> message used in global snapshots, a\nmessage in the detection identifies the set\nof resources for which the sender of the message is waiting.\n\n<p>\nEach message <code>m</code> sent by an agent <code>v</code> has a\nfield <code>m.waits</code> where:\n</p><center><code>m.waits</code> = <code>v.waits</code>.</center>\n<p class=\"w3-text-teal\">Initiating the Algorithm</p>\n  A waiting agent <code>u</code> initiates the algorithm by sending a message\n  <code>m</code> on each of its output channels where\n  <code>m.waits</code> = <code>u.waits</code>.\n  \n <p class=\"w3-text-teal\">Action by an Agent other than the\nInitiator</p>\n<p>\nWhen an agent <code>v</code> receives a message <code>m</code>:\n</p><ol>\n<li>\n  If <code>v</code> has already sent messages then <code>v</code> takes no\n  action.\n  </li>\n<li>\n  If <code>v</code> has not sent messages, and if there is a resource\n  common to <code>m.waits</code> and <code>v.holds</code> then\n  <code>v</code> sends a  message <code>m'</code> on each of its\n  output channels where <code>m'.waits</code> = <code>v.waits</code>.\n  </li>\n</ol>\n<p class=\"w3-text-teal\">Cycle Detection</p>\nIf the initiator <code>u</code> receives a message <code>m</code>\nwhere there is a resource common to <code>m.waits</code> and\n<code>u.holds</code> then <code>u</code> detects a cycle of waiting\nagents.\n\n", "5": "<h4 class=\"w3-text-teal\">Proof of Correctness</h4>\nThe proof outline is as follows.\n\nAn agent <code>v</code> sends messages only if there is a path of waiting\nprocesses from <code>u</code> to <code>v</code>.\n\nSo, <code>u</code> detects a cycle of waiting processes only if there\nexists a cycle of waiting processes.\n\n<p>\nIf there exists a cycle of waiting processes from <code>u</code>  to\n<code>u</code>  then \nmessages are sent along one such cycle, and so\n<code>u</code> detects a cycle.\n\n</p><p>\nThe algorithm terminates because it sends at most one message on each\nchannel. \n\n\n\n\n\n</p>", "6": "<h4 style=\"color:red;\">Example</h4>\n\n\nThis example shows steps in the case of the cycle of waiting processes shown in\nfigure 2.\n\nThe algorithm is initiated by agent <code>x</code> by broadcasting a\nmessage <code>m</code> where <code>m.waits = x.waits = {blue}</code>.\n\n<p>\n\nWhen <code>y</code> receives a message <code>m</code> where\n<code>m.waits = {blue}</code>, there is a resource in both <code>m.waits</code> and\n<code>y.holds</code>, and so <code>y</code> broadcasts message\n<code>m'</code> where <code>m'.waits = y.waits = {green}</code>.\n\n</p><p>\nWhen <code>z</code> receives a message <code>m</code> where\n<code>m.waits = {blue}</code>, <code>z</code> takes no action because\nthere is no resource in both <code>m.waits</code> and\n<code>z.holds</code>.\n\n\n</p><p>\nWhen <code>z</code> receives a message <code>m</code> where\n<code>m.waits = {green}</code>, <code>z</code> broadcasts message\n<code>m'</code> where <code>m'.waits = z.waits = {red}</code>.\n\n\n</p><p>\nWhen the initiator <code>x</code> receives a message <code>m</code> where\n<code>m.waits = {red}</code>, <code>x</code> detects a deadlock\nbecause there is a resource common to <code>m.waits</code> and\n<code>x.holds</code>. \n\n</p>"}, "raw_webcrawl_data/ChannelSnapshots/LogicalClocksSnapshots.html": {"0": "<h2 class=\"w3-text-teal\">Logical Clocks and Global Snapshots</h2>\n\n<p class=\"w3-text-red\">\n  The state at which all agents are at the same logical time \\(t\\) is\n  a global snapshot. \n\n  The state when local physical clocks\n  of all agents are at the same time \\(t\\) may not be a global snapshot.\n\n  Combining physical and logical clocks results in clocks that tick\n  forward and where the state when all local clocks are at the same\n  time is a global snapshot.\n\n  Examples of algorithms that use such clocks are given later.\n\n  </p>\n\n", "1": "<h3 class=\"w3-text-teal\">Problem: Global Snapshots at Logical Times</h3>\n\n<p>\n  Design an algorithm that computes global snapshots using logical\n  clocks.\n\n  \n   \n  \n</p>", "2": "<h4 class=\"w3-text-teal\">How Should You Solve the Problem?</h4>\n\n<p class=\"w3-text-teal\">Strategy</p>\n  Use properties of logical times and computations.\n  \n  <p>\n  Sequences of steps in increasing logical time are computations.\n\n  Therefore, the sequence of steps with logical time at most  \\(t\\) is\n  a computation, for all  \\(t\\).\n\n  This suggests the following definition of the state at logical time\n  \\(t\\).\n  \n\n  </p>", "3": "<h5 class=\"w3-text-teal\">State at Logical Time \\(t\\)</h5>\n  The state of an agent \\(A\\) at logical time \\(t\\) is its state after\n  steps with logical time \\(t\\) or less and before steps with\n  logical time greater than \\(t\\).\n\n  <p>\n  The state of a channel at logical time \\(t\\) is the sequence of\n  messages sent along the channel in steps with logical time at most\n  \\(t\\), but not received in these steps.\n\n\n\n</p>", "4": "<h4 style=\"color:red;\">Example: State at Logical Time \\(t\\)</h4>\n\nFigure 1 illustrates the state at logical time 6.5 of the\ncomputation shown in figure 1.\n\n  The curved purple line represents the cut.\n\n  The cut separates the past of the cut from its future.\n\n  Past steps are colored black while future steps are colored\ngreen.\n\nThe states of agents and channels at logical time \\(t = 6.5\\) are\ngiven by the labels of the edges that cut the purple line.\n\n<figure>\n<img alt=\"Fig2\" src=\"raw_webcrawl_data/ChannelSnapshots/Timelines/Timelines.002.jpeg\" style=\"width:100%\"/>\n<figcaption>Fig. 1: Cut at Logical Time 6.5. Past is Set of Steps\n    with Logical Time at most 6.5</figcaption>\n</figure>\n<p>\nThe point at which the purple line cuts the timeline for agent \\(A\\)\ncan be thought of as the point in \\(A\\)'s computation at which the\nlogical time is exactly 6.5.\n\nThis cut is on the edge from the step at \\(A\\) with\n  logical time at most 6.5 to the step with logical time greater\n  than 6.5.\n\n  In this example the cut is on the edge from step 3 to step 5.\n\n  </p><p>\n  The message edge from step 3 to step 7 represents a message sent\n  along the channel from \\(A\\) to \\(B\\) in the past that is received\n  in the future.\n\nIn this example, the state of the channel \\((A, B)\\) is the sequence\nconsisting of a \nsingle message which is the label of this edge.\n\n\n\n\n</p>", "5": "<h4 class=\"w3-text-teal\">\nGlobal Snapshot Algorithm to Record the State at a Logical Time\n</h4>\n\nAn algorithm to record the state at logical time \\(t\\) follows\ndirectly from the definition of the state at logical time \\(t\\).\n\n<ol>\n<li>\nEach agent takes its local snapshot -- i.e. records its state -- after\na step with logical time at most \\(t\\) and before a\n  step with logical time greater than \\(t\\).\n  </li>\n<li>\nAn agent records the state of an input channel as the sequence of\nmessages with timestamps atmost \\(t\\) that the agent\n  receives when its logical clock exceeds \\(t\\).\n  </li>\n</ol>\n\nThe purple line in figure 2 represents the global snapshot at logical\ntime 6.5.\n\n\n  \n", "6": "<h3 class=\"w3-text-teal\">Using Imperfect Clocks in Distributed Algorithms</h3>\n\n<p class=\"w3-text-teal\">Intuition</p>\n  We will design some algorithms using \nlogical time to play the role of real time.\nFigure 3 shows the computation in figure 2 with the horizontal axis\nrepresenting real time.\nThe position of a step with logical time \\(t\\) is at a distance of\n\\(t\\) units from the origin.\n\n<figure>\n<img alt=\"Fig3\" src=\"raw_webcrawl_data/ChannelSnapshots/Timelines/Timelines.003.jpeg\" style=\"width:80%\"/>\n<figcaption>Fig. 3: Computation with Logical Time as Real Time</figcaption>\n</figure>\n<p>\nThink of logical time as continuous, just as real time is continuous.\nIn this example, points at logical times \\(6.5\\) and \\(6.6\\), at agent\n\\(A\\), refer to the same edge.\n\nIt helps intuition, however, to think\nof the point at logical time \\(6.6\\) as a location to the right of the\npoint at logical time \\(6.5\\) on the same edge.\n\nImagine that logical time \\(6.6\\) is 0.1 time units to the right of\nlogical time \\(6.5\\). \n\n</p><p>\nThe cut at logical time \\(6.5\\) is represented by the vertical line\nat time \\(6.5\\).\nThe left of the line is the past at logical time \\(6.5\\), and the\nright side of the line is the future at that time. \n\n\n</p><p class=\"w3-text-teal\">Physical and Logical Clocks</p>\n<p>\nOperating systems maintain clocks. Some have atomic clocks or other\n  high-fidelity clocks that use Precision or Network Time Protocols\n(PTP, NTP).\n\nWith high-fidelity clocks, a message sent when the sender's clock is\nat \\(t\\) will almost always be received when the receiver's\nclock is later than \\(t\\).\n\nSo physical clocks almost always obey the logical clock requirement.\n\n</p><p>\nWe cannot rule out the possibility that a message sent when the\nsender's clock is at \\(t\\) is received when the receiver's clock is\nearlier than \\(t\\) or equal to \\(t\\).\n\nWe can use physical clocks, but correct them so that messages are\nreceived only after they are sent, where the times are determined by\ncorrected physical clocks.\n\n</p><p>\nSuch clocks have the following properties that we use in\ndesigning algorithms:\n\n</p><ol>\n<li>\n\n  Clocks tick forward forever: For all \\(t\\), there is a point in\n  an infinite computation at which clocks of all agents exceed \\(t\\).\n  </li>\n<li>\n\n  Sequences of steps in ascending order of time are computations.\n  </li>\n</ol>\n<p>\n</p>"}, "raw_webcrawl_data/ChannelSnapshots/LogicalClocks.html": {"0": "<h2 class=\"w3-text-teal\">Logical Clocks</h2>\n\n<p class=\"w3-text-red\">\n  A logical clock algorithm assigns a value, called the logical time,\n  to each step in a computation so that all sequences of steps in\n  ascending logical time are computations.\n\n  </p>\n\n", "1": "<h4 class=\"w3-text-teal\">The Problem</h4>\n\n<p>\n  Design an algorithm that assigns a value,  called the <i>logical\n  time</i>, to each step in a computation so that all sequences of\n  steps in ascending logical time are computations.\n\n  </p><p>\n  This\n  <a href=\"https://en.wikipedia.org/wiki/Logical_clock\">\n  definition is different</a> from that given in the paper that\n  introduces the concept.\n\n   \n\n</p>", "2": "<h5 class=\"w3-text-teal\">How Should You Solve the Problem?</h5>\n\n<p class=\"w3-text-teal\">Strategy</p>\n<p>\n  A strategy for assigning attributes to steps in computations is to\n  find a \n  <a href=\"DISTRIBUTED_SYSTEM_MODELS/Computations.html\">\n  property of computations</a>\n  that can help.\n\n  </p><p>\n  We use the following property:\n\n  A computation is a sequence of steps in which for all edges\n  \\((e, e')\\) in the dataflow graph corresponding to the computation:\n  </p><center>\\(e\\) occurs before \\(e'\\) in the sequence.\n  </center>\n<p>\n  This property suggests the following algorithm to assign logical\n  time \\(t(e)\\) to step \\(e\\).\n\n  \n\n  \n  </p><hr class=\"new2\"/>\n<p style=\"color:blue;\">\n</p>", "3": "<h4 style=\"color:blue;\">The Logical Time Property</h4>\n<p style=\"color:blue;\">\n  For all edges \\((e, e')\\) of the dataflow graph:\n\\(t(e) &lt; t(e')\\).\n</p>\n<hr class=\"new2\"/>\n\n", "4": "<h5 style=\"color:red;\">Example: Logical Times of Steps</h5>\n\n  \n  Figure 1 shows the dataflow graph of a computation with agents\n  \\(A, B, C\\) and an step sequence \\([0, 1, 2, \\ldots, ]\\).\n\n  The numbers inside the vertices are the step ids which show the\n  position of the step in the computation.\n\n  The red numbers outside the steps are logical times assigned to\n  steps.\n\n  <p>\n  Logical times are arbitrary provided\n  every edge is directed from a lower to a higer logical time.\n  <figure>\n<img alt=\"Fig1\" src=\"raw_webcrawl_data/ChannelSnapshots/Timelines/Timelines.001.jpeg\" style=\"width:100%\"/>\n<figcaption>Fig. 1: Logical Times of Steps: Edges Directed from\n  Lower to Higher Logical Times</figcaption>\n</figure>\n\nVerify that every edge in figure 1 is from an step with a lower\nlogical time to an step with a higher logical time.\n\n \n\n</p>", "5": "<h4 class=\"w3-text-teal\">Steps in Increasing Order of Logical Time</h4>\n\n<i>All sequences of steps in increasing order of logical time are\ncomputations.</i>\n<p>\nThis result follows from the fact that\n<a href=\"DISTRIBUTED_SYSTEM_MODELS/Computations.html\">\nall topological sorts of dataflow graphs are computations</a>, and\nsequences of of steps in increasing \nlogical time are topological sorts.\n\n</p><p>For example, a sequence of steps in increasing logical time in\nfigure 1 is:\n[1, 2, 4, 3, 5, 6, 7, 8, 9], and this sequence is a computation.\n  \n\n  \n\n</p>", "6": "<h4 class=\"w3-text-teal\">A Logical Clock Algorithm</h4>\n\nThe following algorithm is suggested by the logical time property.\n\nLet \\(t(e)\\) be the logical time assigned to step \\(e\\).\n\nA message sent in an step \\(e\\) is assigned a timestamp \\(t(e)\\).\n\n<hr class=\"new2\"/>\n<p style=\"color:blue;\">\nLet \\(e'\\) be the step immediately preceding an step \\(e\\) at an\nagent, and let the timestamp of the message received in \\(e\\) be\n\\(T\\).\n</p><p style=\"color:blue;\">\nSet \\(t(e)\\) to be any value greater than max(t(e'), T).\n</p><hr class=\"new2\"/>\n<p>\nThe correctness of the algorithm is self evident.\n\n\n</p>"}, "raw_webcrawl_data/ChannelSnapshots/TerminationDetection.html": {"0": "<h1 class=\"w3-text-teal\">Termination Detection</h1>\n\n\n<p class=\"w3-text-red\">\n  A distributed computation has terminated when all agents are idle\n  and all channels are empty.\n\n  A termination detection algorithm is executed by the operating\n  system to determine whether a client computation has terminated.\n\n  </p>\n", "1": "<h3 class=\"w3-text-teal\">Problem Definition</h3>\n<p>\n  A computation terminates in states in which all\n  channels are empty and all agents are waiting to receive messages.\n\n  An agent is said to be <i>active</i> while it is processing a\n  message and <i>idle</i> while it is waiting to process a message.\n\n  A terminated state is one in which all agents are idle and all\n  channels are empty.\n\n  </p><p>\n  Let \\(C_{s}\\) and \\(C_{r}\\) be the numbers of messages \n  sent and received (respectively) on channel \\(C\\).\n\n  A computation is in a terminated state exactly\n  when:\n  </p><ol>\n<li>\n    All agents are idle and\n    </li>\n<li>\n    for all channels \\(C\\):  \\(\\; C_{s} = C_{r}\\).\n    </li>\n</ol>\n\n  The problem is to design an algorithm that detects whether the\n  computation is in a terminated state.\n  \n  \n", "2": "<h4 class=\"w3-text-teal\">How Should You Solve the Problem?</h4>\n\n<p class=\"w3-text-teal\">Strategy</p>\n\n  A strategy to solve detection problems is to start with the\n  <a href=\"ChannelSnapshots.html\">general detection algorithm</a> and\n  then explore optimizations by using\n  <a href=\"DISTRIBUTED_SYSTEM_MODELS/Timelines.html\">\n  properties of cuts.</a>\n  \n  Let's explore optimizations.\n\n  <p>\n  The algorithm detects whether a channel is empty and it can do so\n  given the numbers of messages sent and received on the channel\n  without information about message contents.\n\n  What <a href=\"DISTRIBUTED_SYSTEM_MODELS/Timelines.html\">\n  properties of cuts</a>\n  come to mind to help us design an algorithm based on message counts?\n\n  </p><p>\n  We use\n  <a href=\"DISTRIBUTED_SYSTEM_MODELS/Timelines.html\">\n  \"Cut based on Counts of Messages Sent and Received,\"</a>\n  which is given again below.\n\n</p><p>\nThere exists a cut <code>(past, future)</code> exactly when the\nfollowing two conditions hold:\n\n</p><ol>\n<li>\n  For all \\(C\\):  \\(\\; C_{s} \\geq C_{r}\\),\n  <p>\n  where\n  \\(C_{s}\\) and \\(C_{r}\\) are the numbers of messages sent and\n  received, respectively, on channel \\(C\\), in <code>past</code>.\n  </p></li>\n<li>\n  If a step \\(x\\) of an agent is in <code>past</code> then steps at\n  that agent before \\(x\\) are also in <code>past</code>.\n  </li>\n</ol>\n\nThe property suggests the following algorithm.\n\n\n\n  ", "3": "<h4 class=\"w3-text-teal\">A Termination Detection Algorithm</h4>\n<p class=\"w3-text-teal\">Agent Actions</p>\n  When an agent changes state from active to idle the agent sends a\n  message to the observer. \n\n  This message contains \\(C_{s}\\) for each output channel and\n  \\(C_{r}\\) for each input channel of the agent.\n\n\n  <p class=\"w3-text-teal\">Observer Actions</p>\n  The observer keeps only the latest message that it receives from\n  each agent.\n  \n  For each channel \\(C\\), let \\(C_{s}^{*}\\) and \\(C_{r}^{*}\\) be the\n  latest value of \\(C_{s}\\) and \\(C_{r}\\), respectively, that the\nobserver has received.\n\n\n  \n<p class=\"w3-text-teal\">Initial Condition</p>\nAll agents are idle.\n\n\\(C_{s}^{*}\\) and \\(C_{r}^{*}\\) are the numbers of messages sent and\nreceived (respectively) on channel \\(C\\) for all \\(C\\).\n\n<p class=\"w3-text-teal\">Termination Detection</p>\n<hr class=\"new2\"/>\n<p style=\"color:blue;\">\n  The observer detects computation has terminated if\n  for all channels \\(C\\): \\(C_{s}^{*} = C_{r}^{*}\\).\n</p>\n<hr class=\"new2\"/>\n\n", "4": "<h5 class=\"w3-text-teal\">Proof of Correctness</h5>\n\n<p>\n  We first prove that if the observer detects that the computation has\nterminated then the computation has indeed terminated.\n\n</p><p>\nSince the observer has detected termination, for each channel \\(C\\),\neither \\(C_{s}^{*} = C_{r}^{*}\\) initially, or the observer received\nmessages containing \\(C_{s}^{*}\\) and \\(C_{r}^{*}\\) such that\n\\(C_{s}^{*} = C_{r}^{*}\\).\n\n\n</p><p>\nLet<code>(past, future)</code> be a partition of the steps of the\n  computation where <code>past</code> consists of steps at an agent\n  before the agent sent the messages to the observer containing\n\\(C_{s}^{*}\\) and \n\\(C_{r}^{*}\\) for channels \\(C\\) incident on the agent. \n(If the agent sends no messages to the observer \nthen the agent has no steps in <code>past</code>.)\n\n</p><p>\nFrom the property,\n<a href=\"../DISTRIBUTED_SYSTEM_MODELS/Timelines.html\">\n\"Cut based on Counts of Messages Sent and\nReceived,\"</a> <code>(past, future)</code> is a cut.\n\n</p><p>\nFrom the definition of terminated state it follows that the state at\nthis cut is a terminated state.\n\nTermination is a stable property -- once computation has terminated it\nremains terminated.\n\nSo, if the state at a cut of the computation\nis a terminated state then all succeeding states are terminated states.\n\n</p><p>\nNext we prove that if the computation terminates then the observer\ndetects termination.\n\nThe last message sent by each agent has counts \\(C_{s}^{*}\\) and\n\\(C_{r}^{*}\\) of the numbers of messages sent and received (respectively)\nfor each of its output channels \\(C\\).\n\nBecause these are the last messages sent when the algorithm terminates\nit follows that \\(C_{s}^{*} = C_{r}^{*}\\) for all \\(C\\).\n\n\n\n\n</p>"}, "raw_webcrawl_data/ChannelSnapshots/ChannelSnapshots.html": {"0": "<h1 class=\"w3-text-teal\">A Global Snapshot Algorithm</h1>\n\n\n<p class=\"w3-text-red\">\n  A global snapshot algorithm records a state of the system that can\n  occur during a computation.\n\n  The state obtained by the algorithm is called a global snapshot.\n\n  Systems are monitored by taking repeated global snapshots.\n\n  When a transient error is detected, a rollback and recovery\n  algorithm restarts the computation from the most recent snapshot\n  instead of starting it from the initial state.\n\n  \n\n  \n\n  </p>\n\n", "1": "<h4 class=\"w3-text-teal\">Global Snapshot</h4>\n\n<p>\n  A global snapshot algorithm records a state of the system that\n  occurs during a computation of the system.\n\n  A state obtained by the algorithm is called a global snapshot.\n\n  </p><p>\n  A state of the system is a tuple with an element of the tuple for each agent and\n  each channel.\n\n  A system state is also called a global state to distinguish it from\n  states of agents and states of channels.\n  \n\n  </p><p>\n  An algorithm to record the state of a system is not instantaneous\n  because the algorithm records the states of multiple agents and\n  channels.\n\n  The algorithm starts at some point and terminates at a later point.\n\n  A global snapshot is a state that occurs in a computation from the\n  state in which the algorithm starts to the state in which the algorithm ends.\n\n  </p><p>\n  The global snapshot algorithm is an example of an algorithm that is\n  executed by a distributed operating system (OS) on behalf of a\n  client.\n\n  Next, we describe features of the OS that are relevant to the\n  snapshot algorithm.\n\n  \n</p>", "2": "<h4 class=\"w3-text-teal\">A Distributed Operating System</h4>\n \n  \n  \n  Each client agent has an OS agent that supervises it.\n\n  OS agents use the same processors and channels as clients do.\n  \n  OS agents can record, but not modify, states of their clients.\n  \n  OS agents can send and receive OS messages that are not seen\nby clients.\n\n<p>\nFigure 1 is a representation of two OS agents that manage their\nclient agents.\n\nMessages sent by a client are recorded by the OS and passed through\nto destination clients.\n\nThe OS sends messages on the same channels as clients, but the OS\ntraps these messages so that the client does not see them.\n\n\n<figure>\n<img alt=\"Fig1\" src=\"raw_webcrawl_data/ChannelSnapshots/ChannelSnapshots/ChannelSnapshots.001.jpeg\" style=\"width:60%\"/>\n<figcaption>Fig.1: OS and Clients use the same Channels</figcaption>\n</figure>\n</p><p>\n  Execution of an OS agent on a processor may delay a client's\n  steps on the same processor, and thus change the order in which the\n  client's steps are executed. \n\n<i>The OS may change a client's computation -- the order of steps -- but\nthe OS must not change the client's dataflow.</i>\n</p><p>\n  One way to record a global snapshot is for the OS to stop a client\n  computation, then take a global snapshot, and then  restart the\nclient computation.\n\nOur goal is to design an algorithm that does not stop the client.\n\n</p><p>\nHereafter, when we refer to an agent we mean an OS agent.\n\nLikewise, by messages we mean those that are sent and received by the\nOS.\n\n\n</p><p>\nNext, let's develop OS agents and OS messages to record a global\nsnapshot.\n\n\n\n</p>", "3": "<h4 class=\"w3-text-teal\">The Problem</h4>\n\n<p>\nLet \\(S_{init}\\) and \\(S_{fini}\\) be the states in which the\nalgorithm starts and finishes, respectively.\n\nDesign an algorithm that records a state \\(S^{*}\\) such that\nthere exists a computation that starts at \\(S_{init}\\), then visits\n\\(S^{*}\\) and then visits \\(S_{fini}\\).\n\n\n\n</p>", "4": "<h4 class=\"w3-text-teal\">How Should You Solve the Problem?</h4>\n\n<p class=\"w3-text-teal\">Strategy</p>\nA general strategy for designing algorithms dealing with intermediate\nstates is to find a helpful <a href=\"../DISTRIBUTED_SYSTEM_MODELS/Timelines.html\">\nproperty of cuts</a>.\n\nWhat property helps to determine \\(S^{*}\\)?\n\n<p>\nThe property \"Computations of Past before Future,\" given below, tells\nus that \\(S^{*}\\) can be the state at any cut.\n\n\n</p><p class=\"w3-text-teal\">Computations of Past before Future.</p>\n<p>\nLet computation \\(X\\) start in state \\(S_{init}\\) and end in state\n\\(S_{fini}\\).\n\nLet \\(S^{*}\\) be the state at a cut <code>(past, future)</code> of \\(X\\).\n\nThere exists a computation \\(Y\\) that starts in \\(S_{init}\\), visits\n\\(S^{*}\\), and ends in \\(S_{fini}\\).\n\n</p><p>\nUsing this strategy, our tasks reduce to (1) identifying a cut, and\n(2) recording the state, \\(S^{*}\\), at the cut.\n\n</p>", "5": "<h5 class=\"w3-text-teal\">Identifying a Cut</h5>\nEach agent has to record its own state because an agent's state is not\naccessible to other agents.\n\nThe state of an agent that it records is called a <i>local snapshot</i>.\n\n<p>\nDefine <code>past</code> as the set of steps at each agent before the\nagent takes its local snapshot, and\ndefine <code>future</code> as the set of steps at each agent after the\nagent takes its local snapshot.\nSo, if a step \\(x\\) at an agent is in <code>past</code> then all steps\nat that agent before \\(x\\) are also in <code>past</code>.\n\n</p><p>\nLet's use\n<a href=\"../DISTRIBUTED_SYSTEM_MODELS/Timelines.html\"> the following property of\n<code>past</code>, <code>future</code> and cuts:</a>\n</p><p>\nThe partition <code>(past, future)</code> is a cut exactly when every\nmessage received in <code>past</code> is sent in <code>past</code>.\n\n</p><p>\nTherefore, <code>(past, future)</code> is a cut exactly when:\n</p><hr class=\"new2\"/>\n<p style=\"color:blue;\">Global Snapshot Rule</p>\n<p style=\"color:blue;\">\nEach message received before the receiver takes its local snapshot\nis sent before the sender takes its local snapshot.\n</p><p>\n</p><hr class=\"new2\"/>\n\nDesign an algorithm yourself before reading further, and compare your\nalgorithm with the one given below.\n\n\n\n\n", "6": "<h4 class=\"w3-text-teal\">The Global Snapshot Algorithm</h4>\n\n\nA special OS message called a <code>marker</code> is used to distinguish\npre-snapshot from post-snapshot messages.\n\nMessages sent on a channel before a <code>marker</code> is sent on the\nchannel are messages sent in the <code>past</code> -- i.e. before the\nsender takes its local snapshot -- and messages sent\nafter the marker are sent in the <code>future</code>.\n\n<p class=\"w3-text-teal\">The algorithm</p>\n<ol>\n<li>\n  The algorithm begins by one or more agents taking their local\n  snapshots.\n  </li>\n<li>\n  When an agent takes its local snapshot it sends a marker on each\n  of its outgoing channels. \n  </li>\n<li>\n  When an agent receives a marker, the agent takes its local snapshot\n  if it has not already done so. \n  </li>\n<li>\n  The snapshot of a channel is the sequence of messages received on\n  the channel after the receiver takes its snapshot and before the\n  receiver receives a marker on the channel.\n  </li>\n</ol>\n", "7": "<h5 class=\"w3-text-teal\">Proof of correctness</h5>\n<p>\n\nFrom rule 3, each message received by an agent \\(r\\) on a channel\n\\(c\\) before \\(r\\) takes its \nlocal snapshot is a message received by \\(r\\) before \\(r\\) receives a\nmarker on channel \\(c\\).\n\n</p><p>\nBecause channels are first in first out, each message received by\n \\(r\\) on \\(c\\) before \\(r\\) receives a\nmarker on \\(c\\) is sent on \\(c\\) before a marker is\nsent on \\(c\\).\n\n</p><p>\nFrom rule 2 each message sent on \\(c\\) before a marker is sent on\n\\(c\\) is sent before the sender takes its local snapshot.\n\n</p><p>\nFrom the three paragraphs above it follows that the global snapshot\nrule holds for the algorithm.\n\n\n</p><p class=\"w3-text-teal\">Proof about States of Channels</p>\nThe messages in a channel at the cut are the\nmessages sent in <code>past</code> and received in <code>future</code>.\nThese are messages sent before the sender takes its snapshot and\nreceived after the receiver takes its snapshot.\n\nSo, the state of a channel is the sequence of\nmessages received along the channel after the receiver takes its\nsnapshot and before the receiver receives a marker along the channel.\n\n<p>\nNote: If an agent takes its local snapshot when it receives a marker\nalong a channel, then the snapshot of the channel is the empty\nsequence of messages.\n\n\n</p>", "8": "<h5 class=\"w3-text-teal\">Termination of the Algorithm</h5>\n\nAfter any agent \\(v\\) initiates the algorithm, all agents that are\nreachable from \\(v\\) will receive a marker and take their local\nsnapshots.\n\nIf every agent is reachable from an initiator then all agents take\nlocal snapshots. \n\n<p>\nEach agent takes its local snapshot at most once.\n\nSo, a marker is sent on a channel at most once.\n\nThe computation terminates when all markers are received.\n\n\n</p>", "9": "<h5 class=\"w3-text-teal\">Collecting Local Snapshots to form Global\nSnapshots</h5>\nOne way to collect local snapshots is to have an OS agent act as an\nobserver.\n\nEach agent sends its local snapshots to the observer which puts the\nlocal snapshots together to form the global snapshots.\n\nSuccessive snapshots are disambiguated by using sequence numbers or\ntimestamps.\n\n<p>\nSome algorithms carry out distributed computations on local snapshots\nwithout using an observer to collect local snapshots.\n\nLater, we give examples of such algorithms.\n\n\n\n\n</p>", "10": "<h3 class=\"w3-text-teal\">Applications of Global Snapshots</h3>\n", "11": "<h4 class=\"w3-text-teal\">System Monitoring</h4>\nSystems can be monitored by taking global snapshots repeatedly.\n\nLet \\(S_{0}, S_{1}, S_{2}, \\ldots, \\) be the sequence of states\nrecorded by the system.\n\nFrom the property\n<a href=\"DISTRIBUTED_SYSTEM_MODELS/Timelines.html\">\nComputations through Increasing Cuts\n</a>\nthere exists a computation that visits each state \\(S_{i}\\) in order\nof increasing \\(i\\).\n\nThe system monitor checks the sequence of snapshots to determine if\nsome action is required.\n\n\n", "12": "<h4 class=\"w3-text-teal\">Rollback and Recover</h4>\n\n\nLet \\(S^{*}\\) be the most recent snapshot recorded by a system monitor.\n\nFrom the property,\n<a href=\"../DISTRIBUTED_SYSTEM_MODELS/Timelines.html\">\n\"Computations of Past before Future\"</a>\nthere exists a computation that starts at the initial state and later\nvisits \\(S^{*}\\).\n\nSo, if an error is detected in a computation then the computation can be\nrestarted from \\(S^{*}\\) rather than rolling all the way back to the\ninitial state. \n\n\n\n", "13": "<h4 class=\"w3-text-teal\">Detecting Stable Predicates</h4>\n\nA <i>stable predicate</i> is a predicate with the following property: If the\npredicate holds at any point in any computation then it continues to\nhold forever thereafter in that computation. Equivalently, if a stable\npredicate holds in a state \\(s\\) then it holds in all states reachable\nfrom \\(s\\).\n\n<p>\nExamples of stable predicates are: \"The computation has\nterminated,\" and \"The computation is deadlocked.\"\n\nIf a computation has terminated at some point then it remains \nterminated.\n\nLikewise if a computation has deadlocked then it remains deadlocked.\n\n</p>", "14": "<h5 class=\"w3-text-teal\">Specification of Detection Algorithms</h5>\nAn algorithm to detect a stable property \\(P\\) has the following\nspecification.\n<ol>\n<li>\n  If \\(P\\) holds when the algorithm is initiated then the algorithm\n  detects that \\(P\\) holds.\n  </li>\n<li>\n  If the algorithm detects that \\(P\\) holds then \\(P\\) holds when the\n  algorithm terminates.\n  </li>\n</ol>\n", "15": "<h5 class=\"w3-text-teal\">General Detection Algorithms</h5>\nA general solution is for the operating system to monitor a client\ncomputation by taking repeated snapshots of the computation.\n\nThe OS checks whether a specified stable property holds in each\nsnapshot.\n\nFrom the property,\n<a href=\"../DISTRIBUTED_SYSTEM_MODELS/Timelines.html\">\n\"Computations of Past before Future\"</a>\nthis general solution satisfies the specification of detection\nalgorithms. \n\n<p class=\"w3-text-teal\">Detection with Observers</p>\nThe OS uses an agent, the observer, to collect local snapshots and\nform a global snapshot.\n\nThe observer inspects the global snapshot to determine if the property\nholds in the snapshot.\n\n\nThe OS can also use multiple observers each of which collects\nlocal information from subnetworks; the OS then carries out a\ndistributed algorithm on its collection of observers.\n\n<p>\nThe OS can also execute a distributed algorithm on local\nsnapshots without having observers collect local information, as\ndescribed next.\n\n\n\n</p><p class=\"w3-text-teal\">Detection without Observers: Distributed\nAlgorithms on Local Snapshots</p>\n\nDistributed algorithms on local snapshots operate in two phases.\n\nIn the first phase a global snapshot algorithm is executed.\n\nThe local snapshot of each agent and its incoming channels are stored\nlocally, at the agent, without sending the information to observers.\n\n<p>\nIn the second phase a distributed algorithm is executed to determine\nif the local information stored at agents satisfies a specified global\nproperty, such as \"computation has deadlocked.\"\n\nThe algorithm in the second phase operates on unchanging data.\n\nThese algorithms are often distributed graph algorithms.\n\n\n</p><p>\nThe two phases can be executed concurrently in many applications.\n\n\n\n\n</p>"}, "raw_webcrawl_data/ChannelSnapshots/ChannelSnapshotsDetails.html": {"0": "<h1 class=\"w3-text-teal\">A Global Snapshot Algorithm</h1>\n\n\n<p class=\"w3-text-red\">\n  A global snapshot algorithm records a state of the system that can\n  occur during a computation.\n\n  The state obtained by the algorithm is called a global snapshot.\n\n  Systems are monitored by taking repeated global snapshots.\n\n  When a transient error is detected, a rollback and recovery\n  algorithm restarts the computation from the most recent snapshot\n  instead of starting it from the initial state.\n\n  \n\n  \n\n  </p>\n\n", "1": "<h4 class=\"w3-text-teal\">Global Snapshot: Details</h4>\n\n\n  This webpage gives a code outline for the global snapshot algorithm\n  and gives examples of steps in the algorithm.\n\n", "2": "<h5 class=\"w3-text-teal\">Code Structure</h5>\nThe code outline is given below in Python.\n\n<i>channels_recorded</i> is a dict (dictionary), where\n<i>channels_recorded[sender]</i> becomes True when the snapshot for\nthe channel from this sender has finished being recorded.\n<i>channel_snapshots</i> is a dict where\n<i>channel_snapshots[sender]</i> is the ongoing recording of the\nsnapshot of the channel from the sender.\n\n<pre>\ntaken_local_snapshot = False\nchannel_snapshots = {key: [] for key in predecessors}\nchannels_recorded = {key: False for key in predecessors}\n\nstart()\ndef receive(message, sender):\n   if isinstance(message, Marker) and not taken_local_snapshot:\n        local_snapshot = record_state()\n        taken_local_snapshot = True\n        channels_recorded[sender] = True\n        output_message = Marker()\n        for receiver in successors:\n            send(output_message, receiver)\n   elif isinstance(message, Marker) and taken_local_snapshot:\n        channels_recorded[sender] = True\n   else: \n        if taken_local_snapshot and not channels_recorded[sender]:\n           channel_snapshots[sender] = \\\\ \n                       channel_snapshots[sender].append(message)\n</pre>\n\n\nThe remainder of this webpage consists of examples.\n\n\n<center>\n</center>", "3": "<h2 style=\"color:red;\">Examples</h2>\n\n\n\n", "4": "<h4 style=\"color:red;\">Example: Snapshots may change a Client's Computation\n</h4>\n\nThis example shows that the OS algorithm may change a client's\ncomputation though it does not change the client's dataflow.\n\n<p>\nFigure 2 is a representation of a computation with event sequence\n\\([0, 1, 2, \\ldots, ]\\) and agents \\(X, Y, Z\\) without a concurrent OS\nalgorithm, and figure 3 shows how the OS changes this computation.\n\nEvents later in the computation are placed to the\nright of earlier events.\n\n\n<figure>\n<img alt=\"Fig2\" src=\"raw_webcrawl_data/ChannelSnapshots/ChannelSnapshots/ChannelSnapshots.002.jpeg\" style=\"width:80%\"/>\n<figcaption>\nFig.2: Representation of a Computation without Snapshots\n</figcaption>\n</figure>\n</p><p>\nFigure 3 shows how a client's computation is changed when the OS takes\nsnapshots. \n\nThe local snapshots taken by agents are shown as a yellow\ncircle on the agents' timelines.\n\nThe OS delays event 3 so that it occurs after events 4, 5, 6, and 7,\nas shown in the figure.\n\nThe OS changes the computation, but it does not change the dataflow.\n\n<figure>\n<img alt=\"Fig2\" src=\"raw_webcrawl_data/ChannelSnapshots/ChannelSnapshots/ChannelSnapshots.003.jpeg\" style=\"width:80%\"/>\n<figcaption>Fig.3: The OS changes a Client's Computation</figcaption>\n</figure> \n\nIn figure 3, the pre-snapshot events are 0, 1, 2, 4, 6.\n\nThere is only one message received in a pre-snapshot\nevent, namely the message represented by the edge (0, 2).\n\nSo, every message received in a pre-snapshot event is sent in a\npre-snapshot event.\n\nThe figure shows that the set of pre-snapshot events is closed.\n\n\n\n</p>", "5": "<h4 style=\"color:red;\">Example: Steps in a Global Snapshot\nAlgorithm: Initiation</h4>\n\n\nFigure 4 illustrates the first step of the algorithm.\n\n<figure>\n<img alt=\"Fig4\" src=\"raw_webcrawl_data/ChannelSnapshots/ChannelSnapshots/ChannelSnapshots.005.jpeg\" style=\"width:100%\"/>\n<figcaption>Fig.4: Agent Sends Markers when it Takes its Local\n    Snapshot</figcaption>\n</figure>\n\nAgent Y takes its local snapshot shown as a yellow vertex on Y's\ntimeline.\n\nWhen Y takes its snapshot it sends markers on its output channels.\n\nThe markers are shown as green edges in the figure.\n\n\n<p>\nWhen agents X and Z each receive the markers, they take their local\nsnapshots because they haven't taken snapshots earlier.\n\n<figure>\n<img alt=\"Fig5\" src=\"raw_webcrawl_data/ChannelSnapshots/ChannelSnapshots/ChannelSnapshots.006.jpeg\" style=\"width:100%\"/>\n<figcaption>\nFig.5: Agents Take Local Snapshots when they Receive Markers\n</figcaption>\n</figure>\n\nThe actions by X and Z of taking their snapshots are shown as yellow\nvertices on their timelines in figure 5.\n\n\n\n</p>", "6": "<h4 style=\"color:red;\">Example: Agents take Snapshots upon Receiving Markers</h4>\n\n<p>\nWhen X and Z take their snapshots they send markers out on their\noutput channels.\n\nThe markers sent by X are shown in figure 7.\n\nThe markers sent by Z are not shown in the figure.\n\n<figure>\n<img alt=\"Fig6\" src=\"raw_webcrawl_data/ChannelSnapshots/ChannelSnapshots/ChannelSnapshots.007.jpeg\" style=\"width:100%\"/>\n<figcaption>Fig.6: When an Agent takes its Snapshot it sends Markers.\n    </figcaption>\n</figure>\n\n</p>", "7": "<h4 style=\"color:red;\">Example: Snapshot of a Channel</h4>\n\n<p>\nFigure 8 shows how agent Y determines the state of the channel from X\nto Y in the global snapshot.\n\nY starts recording the messages it receives along this channel after Y\ntakes its snapshot and stops the recording when it receives a marker\non this channel\n\nThe only message in this interval is the message corresponding to edge\n(6, 7).\n\n\n<figure>\n<img alt=\"Fig7\" src=\"raw_webcrawl_data/ChannelSnapshots/ChannelSnapshots/ChannelSnapshots.008.jpeg\" style=\"width:100%\"/>\n<figcaption>\nFig.7: Example: Recording a Channel State\n</figcaption>\n</figure> \n\nThe message corresponding to edge \\((0, 2)\\) is from X to Y but is not\nin the snapshot of \nthe channel because both \\(0\\) and \\(2\\) are pre-snapshot events.\n\nLikewise, the message corresponding to edge \\((12, 13)\\) is from X to\nY but is not in the snapshot of\nthe channel because both \\(12\\) and \\(13\\) are post-snapshot events.\n\nThe message corresponding to edge \\((6, 7)\\) was sent in a\npre-snapshot event and received in a post-snapshot event, and so it is\nin the snapshot of the channel.\n\n\n</p>"}, "raw_webcrawl_data/Paxos/Paxos.html": {"0": "<h1 class=\"w3-text-teal\">The Paxos Algorithm</h1>\n\n  Next, we study an algorithm, called\n  \n  <a href=\"https://en.wikipedia.org/wiki/Paxos_(computer_science)\">\n<i>Paxos</i>,\n  by which agents determine a single consensus value from a set of\n  proposed values even though messages may be lost, messages may\novertake each other, agents  may be arbitrarily slow, and agents  may\n  stop and restart.\n\n  From\n  </a><a href=\"../ConsensusImpossible/ConsensusImpossible.html\">the\n  FLP theorem</a>,\n  there is no algorithm that guarantees that\n  consensus among agents will be reached if messages can\n  be lost or \n  delayed for arbitrary time, and if agents may fail or be\n  arbitrarily slow.\n\n  Next we describe an algorithm,\n  <a href=\"https://en.wikipedia.org/wiki/Paxos_(computer_science)\">\n<i>Paxos</i>,\n  that enables consensus\nto be reached in most situations, though there is a possibility that\n  consensus may never be reached.\n\n  Before describing Paxos, we describe a simpler problem which gives\n  insight into Paxos.\n\n  </a>", "1": "<h1>OLD</h1>\n\n  In this situation, \n<a href=\"../ConsensusImpossible/ConsensusImpossible.html\">the\n  FLP theorem</a>,\n  says there is no algorithm that guarantees that consensus will be reached \n\n\n  Next we describe an algorithm that enables consensus\nto be reached in most situations, though there is a possibility that\nconsensus may never be reached. We can use probabalistic algorithms to\nreduce the probability of never reaching consensus.\n\n   \n<p>\nThis module describes, <a href=\"https://en.wikipedia.org/wiki/Paxos_(computer_science)\">\n<i>Paxos</i>, an important algorithm</a> by which agents in a\n    distributed system come to a consensus.\n\n\nThere are other algorithms, such as <a href=\"https://en.wikipedia.org/wiki/Raft_(algorithm)\"> <i>Raft</i>\n</a> that deal with this problem; we will describe them later.\n\n    \n</p>", "2": "<h3 class=\"w3-text-teal\">Assumptions</h3>\n<p>\nThe Paxos algorithm deals with messages that may be lost and that may\novertake each other. Agents  may stop and restart later,\nand agents  may be arbitrarily slow. By contrast, in the\nalgorithms that we have discussed so far, we assume that messages are\nnot lost; messages sent along a channel are delivered in the order\nsent; and agents do not stop.\n\n\n</p><p>\nThe Paxos algorithm, and most distributed algorithms studied here,\nassume that messages are not modified while in transit and that agents\ndo not have <a href=\"../Byzantine/Byzantine.html\">Byzantine\nfaults.</a>\n</p>", "3": "<h3 class=\"w3-text-teal\">Specification</h3>\n<p>\nThe algorithm has agents called\n<i>proposers</i> and others called <i>learners</i>. (In an\nimplementation an agent may play the role of both proposer and\nlearner.) Proposers propose values. Learners come to a consensus among\n    the proposed values.\n\n    </p><p>\nAssociated with each learner \\(L\\) is a local variable \\(L.value\\)\nwhich is initially \\(null\\). Associated with each proposer \\(P\\) is a\nlocal constant \\(P.VALUE\\) which is the value that the\nproposer is proposing.\n</p><p>\nThe specification has three parts.\n</p><ol>\n<li>\n<i>Learners learn only proposed values.</i>\n<p> For every learner \\(L\\):\n  </p><p>\n  \\(\n  (L.value = null) \\quad \\vee \\quad\n  (\\exists \\; \\textrm{proposer} \\: P : L.value = P.VALUE)\n  \\)\n  </p></li>\n<li>\n<i>A learner doesn't change the value that it learns.</i>\n<p>\n  For all learners \\(L\\), and for all non-null values \\(V\\):\n  </p><p>\n  \\(\n  stable (L.value = V)\n  \\)\n  </p></li>\n<li>\n<i>All learners learn the same value.</i>\n<p>\n  For all learners \\(L\\) and \\(L'\\):\n  </p><p>\n  \\(\n  (L.value = null) \\vee (L'.value = null) \\vee \n  ( L.value = L'.value)\n  \\)\n  </p></li>\n</ol>\n", "4": "<h2 class=\"w3-text-teal\">The Paxos Algorithm</h2>\nThe Paxos Algorithm, proposed by Leslie Lamport, has an additional\nlayer of agents called <i>acceptors</i>. The communication structure\nbetween agents is shown in the figure below. Proposers send and\nreceive messages from acceptors.\n\n<p>\nLearners receive messages from\nacceptors. Learners do not send messages to proposers or\nacceptors. Learners may send messages to each other, but we ignore such\nmessages in our description of the algorith because while these\nmessages can improve the algorithm's efficiency they are not required.\n\n</p><p>\n<figure>\n<img alt=\"Fig1\" src=\"raw_webcrawl_data/Paxos/./PaxosLecture/PaxosLecture.002.jpeg\" style=\"width:80%\"/>\n<figcaption>Fig.1: Proposers, Acceptors and Learners</figcaption>\n</figure>\n</p>", "5": "<h4 class=\"w3-text-teal\">Local Variables and Messages</h4>\nIn the description of the algorithm;\n<br/>\n<i>A timestamp</i> is a pair\n  <code>(n, pid)</code>, where <code>n</code> is a number\n  and <code>pid</code> is the id\n  of a proposer. Proposer ids are totally ordered and are used to break\nties between timestamps used by different producers.\nWe assume that <code>n</code> is a nonnegative integer that is\ninitially 0.\n<p>\n<i>A value</i> is either <code>null</code> or <code>P.VALUE</code>\nfor some proposer <code>P</code>.\n\n</p><p class=\"w3-text-teal\">Local variable and constant for proposer\n<code>P</code>.</p>\n<ul>\n<li>\n  Variable: <code>P.t</code>, timestamp which is initially <code>(0,\n  P_id)</code> where <code>P_id</code> is <code>P</code>'s id. \n  </li>\n<li>\n  Constant: <code>P.VALUE</code>, value\n  </li>\n</ul>\n<p class=\"w3-text-teal\">Local variables for acceptor\n<code>A</code>.</p>\n<ul>\n<li>\n<code>A.t</code>, timestamp, which is initially <code>(0,\n  null)</code>. \n  </li>\n<li>\n<code>A.accept_v</code>, value which is initially <code>null</code>.\n  </li>\n<li>\n<code>A.accept_t</code>, timestamp, which is initially <code>(0,\n  null)</code>. \n  </li>\n</ul>\n<p class=\"w3-text-teal\">Messages</p>\n<ol>\n<li>\n  proposer to acceptors: <i>new_time(timestamp)</i>\n</li>\n<li>\n  acceptor replies: <i>updated_time(timestamp, value, timestamp)</i>\n</li>\n<li>\n  proposer to acceptors: <i>proposal(timestamp, value)</i>\n</li>\n<li>\n  acceptors to learners: <i>accept(timestamp, value)</i>\n</li>\n</ol>\n", "6": "<h4 class=\"w3-text-teal\">Learners</h4>\nThe algorithm for learners is simple:\nA learner determines that the consensus value is <code>V</code> if the\nlearner receives <code>accept(T, V)</code> messages from a majority of\nacceptors with identical values of <code>(T, V)</code>.\nWe ignore learners in the algorithm description, and restrict\nattention to whether and when a majority of\nacceptors sends <code>accept(T, V)</code> messages with identical\nvalues of <code>(T, V)</code>. \n\n", "7": "<h3 class=\"w3-text-teal\">Algorithm for Proposers and Acceptors</h3>\n<p>\nThe algorithm for proposers and acceptors is given below.\n\n</p><pre>\n// Proposer P. Step P.1. Send new_time to acceptors\nP.t +=  any positive value\nsend new_time(P.t) to all acceptors\n\n// Acceptor A. Step A.1. Reply with updated_time to new_time\n//   message from P\nupon receiving new_time(t) from any proposer P:\n   if t &gt; A.t:\n      A.t = t\n      send updated_time(A.t, A.accept_v, A.accept_t) to P\n\n// Proposer P. Step P.2. Send proposal to acceptors\nwait to receive updated_time(t, accept_v, accept_t) messages from a\n     majority of acceptors where t == P.t or timeout.\n\nIf wait condition is satisfied:\n     if accept_v is null for all of these messages:\n        P.v = P.VALUE\n     else:\n        let updated_time(A*.t*, A*.accept_v*, A*.accept_t*)  be\n          the message with the largest A.accept_t among these messages\n        P.v = A*.accept_v*\n     send proposal(P.t, P.v) to all acceptors\nIf timeout:\n     return to step P.1\n      \n// Acceptor A. Step A.2. Send accept to learners\nupon receiving proposal(t, v) from P:\n    if t &gt;= A.t:\n       A.t, A.accept_t, A.accept_v = t, t, v\n       send accept(A.accept_t, A.accept_v) to all learners\n</pre>\n", "8": "<h4 class=\"w3-text-teal\">Algorithm Steps</h4>\nThe algorithm has only one point where an agent waits: Step P.2.\nIn step P.2, a proposer waits for a bounded time\nto receive promise messages from a majority of acceptors. If these\nmessages don't arrive within this time then the proposer aborts this\niteration and starts the next iteration, i.e. back to step P.1,\nwithout completing step P.2.\n\n<p>\nA proposer repeatedly executes a loop consisting of steps P.1 and P.2.\nFor the time being assume that the loop never terminates; we will\ndiscuss termination later.\n\n</p><p>\nAcceptors respond immediately to messages that they receive.\n\n</p><p>\nIn the discussion, we refer to <code>P.t</code> and <code>A.t</code>\nas <code>P</code>'s timestamp and <code>A</code>'s timestamp,\nrespectively. We refer to <code>A.accept_v</code> as the value\naccepted by <code>A</code>, and <code>A.accept_t</code> as the\ntimestamp of the value accepted by <code>A</code>. Notice that\n<code>A.accept_t</code> and <code>A.accept_v</code> are modified\ntogether, and only in step A.2 when an acceptor receives a\n<code>proposal</code>.\n\n\n</p>", "9": "<h3 style=\"color:red;\">Example</h3>\nThe figure below shows a system timeline with 3 acceptors and 2\nproposers. Timelines for learners are not shown. The events on the\ntimeline are: \n<ol>\n<li>\n<i>N(t)</i>: A proposer sends <code>new_time(t)</code> message, and\n  the proposer's timestamp when it sends the message is\n  <code>t</code>.\n  </li>\n<li>\n<i>U(t)</i>: An acceptor replies to the proposer with a\n  <code>updated_time(t, accept_v, accept_t)</code> message, and \n  the acceptor's timestamp when it sends the message is\n  <code>t</code>.\n  </li>\n<li>\n<i>P(t)</i>: A proposer responds with a\n  <code>proposal(t, v)</code> message, and the proposer's timestamp at\n  this event is <code>t</code>.\n  </li>\n<li>\n<i>A(t)</i>: An acceptor accepts a proposer's\n  <code>proposal(t, v)</code> by \n  sending a\n  <code>accept(accept_v, accept_t)</code> message to learners, and \n  the acceptor's timestamp when it sends the message is\n  <code>t</code>. The accept messages to learners are not shown.\n  </li>\n</ol>\n<p>\nIn the diagram, <code>new_time</code> messages are grey,\n<code>updated_time</code> messages are purple, and\n<code>proposals</code> are black. The <code>new_time(t)</code> message\nsent by proposer P0 to acceptor A2 is lost, and the the <code>proposal</code> sent\nby P0 to acceptor A1 is lost.\n\n<figure>\n<img alt=\"Fig2\" src=\"raw_webcrawl_data/Paxos/PaxosFigures/PaxosFigures.012.jpeg\" style=\"width:100%\"/>\n<figcaption>Fig.2: Timelines</figcaption>\n</figure>\n</p>", "10": "<h3 class=\"w3-text-teal\">Observations about the Algorithm</h3>\nThe following observations are helpful in understanding the algorithm.\n\n", "11": "<h5 class=\"w3-text-teal\">Acceptor's Timeline</h5>\nThe figure below illustrates the timeline of an acceptor\n<code>A</code> which receives messages from proposers <code>Y</code>\nand <code>Z</code>.\n<p>\n<figure>\n<img alt=\"Fig2\" src=\"raw_webcrawl_data/Paxos/./PaxosLecture/PaxosLecture.003.jpeg\" style=\"width:80%\"/>\n<figcaption>Fig.3: Acceptor's Timeline</figcaption>\n</figure>\nThe arrows from the proposors' timelines to <code>A</code>'s timeline\nrepresent messages sent by the proposer and accepted by\n<code>A</code>. Lost messages are not shown.\n\n</p><p>\nThe number next to the message is the timestamp of the message. A\ntimestamp is a pair (integer, proposer id); however, the proposer id\nis not shown.\n\n</p><p>\n<code>A</code>'s timestamp is initially 0. When <code>A</code>\nreceives a message with timestamp 4 it increases its\ntimestamp to 4. Then <code>A</code> receives a message with timestamp\n3 which it ignores because the message timestamp is lower\nthan its time. When <code>A</code> gets a message with\ntimestamp 6, <code>A</code> increases its timestamp to 6.\n\n</p><p>\nAn acceptor's timeline can be partitioned into intervals where the\nacceptor's time within each interval remains constant and increases\nfrom an interval to the following one.\n</p>\n<p> In step A.1 if <code>t &lt; A.t</code>, and in step A.2 if <code>t &lt;=\nA.t</code>, the message received by the acceptor is ignored. In step\nP.2, an <code>updated_time</code> message received by a proposer\n<code>P</code> is ignored if the <code>time</code> component of the\nmessage is different from <code>P.t</code> Hereafter, we won't\nconsider messages that are ignored by agents. A diagram showing acceptor\n<code>A</code>'s timeline without ignored or lost messages is shown\nbelow where the blue arrows are messages from proposers.\n\n<figure>\n<img alt=\"Fig3\" src=\"raw_webcrawl_data/Paxos/./PaxosLecture/PaxosLecture.023.jpeg\" style=\"width:80%\"/>\n<figcaption>Fig.4: Acceptor's Timeline without Ignored Messages</figcaption>\n</figure>\n</p><hr class=\"new2\"/>\n<p style=\"color:blue;\">\n  Ignored messages are treated as lost messages.\n</p>\n<hr class=\"new2\"/>\nHereafter, a <i>received message</i> is one that is not lost and is not ignored by the\nreceiver: it is a message in which the receiver takes some action upon\nreceiving the message.\n\n\n", "12": "<h5 class=\"w3-text-teal\">Proposer's Timeline</h5>\nAn example of a proposer's timeline is shown in the figure below.\n\n<figure>\n<img alt=\"Fig4\" src=\"raw_webcrawl_data/Paxos/./PaxosLecture/PaxosLecture.024.jpeg\" style=\"width:80%\"/>\n<figcaption>Fig.5: Proposer's Timeline</figcaption>\n</figure>\nThe proposer's timeline can also be partitioned into intervals within\n    which the proposer's time remains constant. In this diagram,\n    proposer <code>p</code>'s time is initially 0, and the proposer\n    increases its time in step P.1. Let the new time be 10. Then the\n    proposer sends <code>new_time(10)</code> to all acceptors. The\n    proposer ignores all messages it gets next except for\n    <code>updated_time(10, ..,)</code> messages with the same\n    timestamp, 10. If it gets a majority of such messages, it sends \n<code>proposal(10, ..)</code> to all acceptors.\n<p>\nLikewise, in the next interval, while the proposer's time is 35, all\n    the messages that the proposer sends and receives have the same\n    timestamp: 35. (Recall that ignored messages are treated as lost\nmessages, and are treated as having been not received.)\n\n</p><p>\n  From the algorithm, the only new values of time are created by\n  proposers in step P.1. Message timestamps and agent times in all\n  other steps of the algorithm are obtained from messages received by\n  agents.\n  \n</p><p>\nAs you can see: \n</p><hr class=\"new2\"/>\n<p style=\"color:blue;\">\nWhile a proposer's time is <code>t</code>, the timestamps of \nall messages that the proposer sends and receives is\n<code>t</code>.\n</p>\n<hr class=\"new2\"/>\n\n\nThe algorithm has the following easily-checked properties, for all acceptors \\(A\\) and\nproposers \\(P\\):\n<p>\ninvariant: \\( \\; A.accept\\_t \\leq A.t \\)\n</p><p>\ninvariant: \\( \\; (A.accept\\_t = 0) \\; \\equiv \\; (A.accept\\_v = null) \\)\n</p><p>\n\\(A.t\\), \\(A.accept\\_t\\) and \\(P.t\\) never decrease.\n\n</p>", "13": "<h4 class=\"w3-text-teal\">Ignored messages</h4>\nMessages that are lost or ignored do not change the states of\nagents. <i>Processed</i> messages are messages that an agent receives\nand then carries out an action such as sending a message. Processed\nmessages are:\n<ol>\n<li>\n    \\(new\\_time(T)\\) messages received by an\n    acceptor \\(A\\) when \\(T &gt; A.t\\). See step A.1.\n\t\t\t     </li>\n<li>\n  \\(proposal(T)\\) messages received by an\n    acceptor \\(A\\) when \\(T \\geq A.t\\). See step A.2.\n  </li>\n<li>\n  \\(updated\\_time(T)\\) messages received by a proposer \\(P\\) when\n  \\(T = P.t\\). See step P.2.\n  </li>\n</ol>\n\n\nBecause ignored messages don't change states of agents: \n<hr class=\"new2\"/>\n<p style=\"color:blue;\">\n  We only consider processed messages in trajectories.\n</p>\n<hr class=\"new2\"/>\n\nEvery event in the algorithm sends a  \\(new_time\\),\n\\(updated_time\\), \\(proposal\\), or \\(accept\\) message. Define the time\nof the event as the timestamp of the messages sent in the event. This\nis also the timestamp of the agent when it sends the message.\n\n", "14": "<h4 class=\"w3-text-teal\">Semi-synchronous trajectories</h4>\nWe begin by proving that the Paxos algorithm satisfies its\nspecifications for semi-synchronous trajectories and later carry out\nthe proof for all trajectories.\n<hr class=\"new2\"/>\n<p style=\"color:blue;\">\nA semi-synchronous trajectory is one that can be partitioned\ninto epochs where all events with the same timestamp are in the same\nepoch. \n</p>\n<hr class=\"new2\"/>\n<p>\n  An epoch in which events have timestamp \\(t\\) is called epoch\n  \\(t\\). Because timestamps are pairs (integer, process id), and all\n  timestamps are created by proposers when they <i>send new_time</i>\n  messages, it follows that exactly one proposer sends messages within\n  an epoch.\n\n  \n</p>", "15": "<h5 style=\"color:red;\">Example</h5>\n<figure>\n<img alt=\"Fig4\" src=\"raw_webcrawl_data/Paxos/./PaxosProof/PaxosProof.008.jpeg\" style=\"width:80%\"/>\n<figcaption>Fig.6: Example of epochs in a semi-synchronous trajectory</figcaption>\n</figure>\nThe diagram shows a semi-synchronous trajectory with three epochs with\ntimes \\(t^{0}\\), \\(t^{1}\\), and \\(t^{2}\\), shown\nin brown, blue and green, respectively. All the events and messages in epoch\n\\(t^{i}\\) have timestamp \\(t^{i}\\). In epochs \\(t^{0}\\) and \\(t^{2}\\)\nthe only proposer that sends and receives messages is P1, while in\nepoch \\(t^{1}\\) the only proposer that sends and receives messages is P0.\n\n<p>\n  In epoch \\(t^{0}\\), proposer P1 increases its time (see step P.1) to  \\(t^{0}\\)\n  and sends \\(new\\_time(t^{0})\\) messages --- shown as brown arrows ---\n  to acceptors. The message to acceptor A0 is lost. An acceptor \\(A\\)\n  that gets the message replies with \\(updated\\_time(t^{0},\n  A.accept\\_v, A.accept\\_t)\\). Proposer P1 waits to receive at least 2\n  \\(updated\\_time\\) messages because 2 is a majority for a set of 3\n  agents. When it receives the 2 messages, P1 sends  \\(proposal(t^{0},\n  P1.v)\\) messages to acceptors. In this diagram, the \\(proposal\\)\n  messages sent to A0 and A1 are lost while A2 receives the\n  message which, in turn, sends \\(accept\\) messages. In this epoch, a\n  majority of acceptors do not send \\(accept\\) messages.\n\n</p><p>\n  In epoch \\(t^{1}\\), proposer P0 increases its time, and sends\n  \\(new\\_time(t^{1})\\) messages. P0 timesout waiting to receive\n  \\(updated\\_time\\) messages from a majority of acceptors, and so it\n  sends no \\(proposal\\) messages.\n\n</p><p>\n  In epoch \\(t^{2}\\), proposer P1 increases its time, and sends\n  \\(new\\_time(t^{2})\\) messages. P1 receives\n  \\(updated\\_time\\) messages from a majority of acceptors; sends\n  proposals to acceptors; receives replies from a majority of\n  acceptors, and then sends \\(proposal\\) messages. The\n  \\(proposal\\) messages are received by a majority of acceptors, each\n  of which sends \\(accept\\) messages. So, learners that receive\n  \\(accept\\) messages from a majority of acceptors, set the values\n  that they learned to the value in the \\(accept\\) messages.\n  \n  \n\n\n\n</p>", "16": "<h3 class=\"w3-text-teal\">Theorem</h3>\nIf a majority of acceptors accept the same value, \\(V^{*}\\) in an\nepoch \\(T^{*}\\) of a semi-synchronous \ntrajectory, then in all subsequent epochs:\n<ol>\n<li>\n    the value in all proposal and accept messages is \\(V^{*}\\), and\n  </li>\n<li>\n  for all acceptors \\(A\\):\n    \\( (A.accept\\_t &lt; T^{*}) \\vee (A.accept\\_v =  V^{*}) \\)\n\t\t      </li>\n</ol>\n", "17": "", "18": "<h5 style=\"color:red;\">Example</h5>\n<figure>\n<img alt=\"Fig4\" src=\"raw_webcrawl_data/Paxos/./PaxosProof/PaxosProof.008.jpeg\" style=\"width:80%\"/>\n<figcaption>Fig.6: Example of epochs in a semi-synchronous trajectory</figcaption>\n</figure>\nThe diagram shows a semi-synchronous trajectory with three epochs with\ntimes \\(t^{0}\\), \\(t^{1}\\), and \\(t^{2}\\), shown\nin brown, blue and green, respectively. All the events and messages in epoch\n\\(t^{i}\\) have timestamp \\(t^{i}\\). In epochs \\(t^{0}\\) and \\(t^{2}\\)\nthe only proposer that sends and receives messages is P1, while in\nepoch \\(t^{1}\\) the only proposer that sends and receives messages is P0.\n\n<p>\n  In epoch \\(t^{0}\\), proposer P1 increases its time (see step P.1) to  \\(t^{0}\\)\n  and sends \\(new\\_time(t^{0})\\) messages --- shown as brown arrows ---\n  to acceptors. The message to acceptor A0 is lost. An acceptor \\(A\\)\n  that gets the message replies with \\(updated\\_time(t^{0},\n  A.accept\\_v, A.accept\\_t)\\). Proposer P1 waits to receive at least 2\n  \\(updated\\_time\\) messages because 2 is a majority for a set of 3\n  agents. When it receives the 2 messages, P1 sends  \\(proposal(t^{0},\n  P1.v)\\) messages to acceptors. In this diagram, the \\(proposal\\)\n  messages sent to A0 and A1 are lost while A2 receives the\n  message which, in turn, sends \\(accept\\) messages. In this epoch, a\n  majority of acceptors do not send \\(accept\\) messages.\n\n</p><p>\n  In epoch \\(t^{1}\\), proposer P0 increases its time, and sends\n  \\(new\\_time(t^{1})\\) messages. P0 timesout waiting to receive\n  \\(updated\\_time\\) messages from a majority of acceptors, and so it\n  sends no \\(proposal\\) messages.\n\n</p><p>\n  In epoch \\(t^{2}\\), proposer P1 increases its time, and sends\n  \\(new\\_time(t^{2})\\) messages. P1 receives\n  \\(updated\\_time\\) messages from a majority of acceptors; sends\n  proposals to acceptors; receives replies from a majority of\n  acceptors, and then sends \\(proposal\\) messages. The\n  \\(proposal\\) messages are received by a majority of acceptors, each\n  of which sends \\(accept\\) messages. So, learners that receive\n  \\(accept\\) messages from a majority of acceptors, set the values\n  that they learned to the value in the \\(accept\\) messages.\n  \n  \n\n\n\n</p><h3 class=\"w3-text-teal\">Theorem</h3>\nIf a majority of acceptors accept the same value, \\(V^{*}\\) in an\nepoch \\(T^{*}\\) of a semi-synchronous \ntrajectory, then in all subsequent epochs:\n<ol>\n<li>\n    the value in all proposal and accept messages is \\(V^{*}\\), and\n  </li>\n<li>\n  for all acceptors \\(A\\):\n    \\( (A.accept\\_t &lt; T^{*}) \\vee (A.accept\\_v =  V^{*}) \\)\n\t\t      </li>\n</ol>\n<h4 class=\"w3-text-teal\">Proof</h4>\nThe proof is by induction on successive epochs in the sequence of\nepochs that follows epoch \\(T^{*}\\). Let \\(T^{i}\\) be the time of the\n\\(i\\)-th epoch after epoch \\(T^{*}\\), and let \\(T^{0} = T^{*}\\).\n\n<p class=\"w3-text-teal\">Base Case: epoch \\(T^{*}\\)</p>\n<i>Part 1</i>: All proposals are sent by the\nsame proposer in an epoch, and all proposals have the same timestamp\nand value. Therefore, the value in all proposal and accept messages in\nepoch \\(T^{*}\\) is \\(V^{*}\\).\n\n<p>\n<i>Part 2</i>:  In epoch \\(T^{*}\\), \\(A.accept\\_t\\) is changed only when \n\\(A.accept\\_v\\) is set to \\(V^{*}\\), and so part 2 follows.\n\n</p><p class=\"w3-text-teal\">Induction step: epoch \\(T^{i+1}\\)</p>\nAssume that the theorem holds for epochs \\(T^{0}, T^{1}, \\ldots,\nT^{i}\\), and prove that it holds for epoch \\(T^{i+1}\\).\nThe result is vacuously true if no proposals are sent in\n   this epoch. Consider the case where proposals are sent.\n\n   <p>\n     Because  \\(A.accept\\_t\\) is never decreased, it follows\n     that in all epochs after epoch \\(T^{*}\\) there is a majority of\n     acceptors for which \\(A.accept\\_t \\geq T^{*}\\). \n   </p><p>\n     A proposer \\(p\\) sends a proposal only if it receives\n     \\(updated\\_time\\) messages from a majority of acceptors. Any two\n     majorities have at least one element in common. So, there exists\n     an acceptor \\(A\\) that sends an \\(updated\\_time\\) message to\n     \\(p\\) and for which \\(A.accept\\_t \\geq T^{*}\\). Because\n     \\(A.accept\\_t\\) is positive, \\(A.accept\\_v\\) \n     is not null (see the invariants).\n\n   </p><p>\n     So, proposer \\(p\\) identifies the \\(updated\\_time\\) message with\n     the largest \\(A.accept\\_t\\). This value is greater than or equal\n     to \\(T^{*}\\), and so the corresponding \\(A.accept\\_v \\) is\n     \\(V^{*}\\), and therefore the  proposal's value is\n     \\(V^{*}\\). So, the value of accept messages is also \\(V^{*}\\),\n     and if \\(A.accept\\_v\\) is changed then the new value is \\(V^{*}\\).\n\n\n     </p><h5 style=\"color:red;\">Example</h5>\n     This example illustrates the idea underlying the proof. In the\n     next diagram, proposer P0 sends a \\(new\\_time(T^{*})\\)\n     message which is received by acceptors A0 and A1 which respond\n     with updated time messages with timestamp \\(T^{*}\\). Proposer P0\n     receives both messages, and sends proposals to all acceptors. In\n     this case the proposals are received by A0 and A2 which then send\n     accept messages. When A0 and A2 receive the proposals they set\n     \\(A.accept\\_t, A.accept\\_v = T^{*}, V^{*}\\), and these values\n     remain unchanged until the next proposal is received. \n<figure>\n<img alt=\"Fig4\" src=\"raw_webcrawl_data/Paxos/./PaxosProof/PaxosProof.009.jpeg\" style=\"width:60%\"/>\n<figcaption>Fig.7: Idea underlying the proof</figcaption>\n</figure>\n\nThe diagram below shows an epoch T1 that follows epoch T0. In this\nepoch, proposer P1 sends \\(new\\_time(T^{1})\\) messages which are\nreceived by acceptors A0 and A1 which respond with updated time\nmessages with timestamp \\(T^{1}\\).\nWhen P1 gets both messages it sends proposals to all acceptors.\nLet's use the argument of the proof\nto determine the value of proposals and the accept message sent by A1 in this\nepoch. \n\n\n<figure>\n<img alt=\"Fig4\" src=\"raw_webcrawl_data/Paxos/./PaxosProof/PaxosProof.010.jpeg\" style=\"width:60%\"/>\n<figcaption>Fig.8: What can be proved about the next accept message?</figcaption>\n</figure>\n\n\nWhen P1 gets both updated time messages it\nidentifies the message with the largest value of \\(A.accept\\_t\\). The\nmessage sent by A0 has timestamp \\(T^{*}\\) whereas the message sent by\nA1 has an earlier timestamp. So, the updated time message with the\nlargest \\(A.accept\\_t\\) is the message sent by A0. \n\nP1 sends proposals with a value of\n\\(A0.accept\\_t\\) which is \\(V^{*}\\). The green circles show how the\nvalue \\(V^{*}\\) is sent from A0 to P1 to A1.\n\n   <p>\n     We see that proposals and accept\nmessages have value \\(V^{*}\\) in the next epoch as well. \n<figure>\n<img alt=\"Fig4\" src=\"raw_webcrawl_data/Paxos/./PaxosProof/PaxosProof.012.jpeg\" style=\"width:60%\"/>\n<figcaption>Fig.9: The role of the accept time: accept_t </figcaption>\n</figure>\n</p>", "19": "<h3 class=\"w3-text-teal\">Theorem: Paxos is Correct for\n  Semi-Synchronous Trajectories</h3>\nThe specification of the algorithm is repeated here for convenience.\n<ol>\n<li>\n<i>Learners learn only proposed values.</i>\n</li>\n<li>\n<i>A learner doesn't change the value that it learns.</i>\n</li>\n<li>\n<i>All learners learn the same value.</i>\n</li>\n</ol>\n<p class=\"w3-text-teal\">Proof</p>\n   The value of\n<code>v</code> in every message is either null or <code>P.VALUE</code>\nfor some proposer P, and so part 1 follows. Parts 2 and 3 follow from\nthe previous theorem: if a majority of acceptors send identical\n \\(accept(T^{*}, V^{*})\\) messages in an epoch, then in all succeeding\nepochs, the value \\(v\\) of every accept message remains \\(V^{*}\\).\n\n\n", "20": "<h3 class=\"w3-text-teal\">Paxos is Correct for\n  all Trajectories</h3>\n\nWe have shown that the Paxos algorithm is correct for semi-synchronous\ntrajectories. Now we show the correctness of the algorithm for all\ntrajectories.\n\nLet \\([past(T), future(T)]\\) be a cut of a timeline where \\(past(T)\\)\nis the set of events with time \\(T\\) or\nless. Therefore \\(future(T)\\) is the set of events with time greater than \\(T\\).\n\n", "21": "<h4 class=\"w3-text-teal\">Theorem: Consistent cuts</h4>\nThe cut \\([past(T), future(T)]\\) is consistent.\n\n", "22": "", "23": "<h5 style=\"color:red;\">Example</h5>\n<figure>\n<img alt=\"Fig4\" src=\"raw_webcrawl_data/Paxos/./PaxosProof/PaxosProof.008.jpeg\" style=\"width:80%\"/>\n<figcaption>Fig.6: Example of epochs in a semi-synchronous trajectory</figcaption>\n</figure>\nThe diagram shows a semi-synchronous trajectory with three epochs with\ntimes \\(t^{0}\\), \\(t^{1}\\), and \\(t^{2}\\), shown\nin brown, blue and green, respectively. All the events and messages in epoch\n\\(t^{i}\\) have timestamp \\(t^{i}\\). In epochs \\(t^{0}\\) and \\(t^{2}\\)\nthe only proposer that sends and receives messages is P1, while in\nepoch \\(t^{1}\\) the only proposer that sends and receives messages is P0.\n\n<p>\n  In epoch \\(t^{0}\\), proposer P1 increases its time (see step P.1) to  \\(t^{0}\\)\n  and sends \\(new\\_time(t^{0})\\) messages --- shown as brown arrows ---\n  to acceptors. The message to acceptor A0 is lost. An acceptor \\(A\\)\n  that gets the message replies with \\(updated\\_time(t^{0},\n  A.accept\\_v, A.accept\\_t)\\). Proposer P1 waits to receive at least 2\n  \\(updated\\_time\\) messages because 2 is a majority for a set of 3\n  agents. When it receives the 2 messages, P1 sends  \\(proposal(t^{0},\n  P1.v)\\) messages to acceptors. In this diagram, the \\(proposal\\)\n  messages sent to A0 and A1 are lost while A2 receives the\n  message which, in turn, sends \\(accept\\) messages. In this epoch, a\n  majority of acceptors do not send \\(accept\\) messages.\n\n</p><p>\n  In epoch \\(t^{1}\\), proposer P0 increases its time, and sends\n  \\(new\\_time(t^{1})\\) messages. P0 timesout waiting to receive\n  \\(updated\\_time\\) messages from a majority of acceptors, and so it\n  sends no \\(proposal\\) messages.\n\n</p><p>\n  In epoch \\(t^{2}\\), proposer P1 increases its time, and sends\n  \\(new\\_time(t^{2})\\) messages. P1 receives\n  \\(updated\\_time\\) messages from a majority of acceptors; sends\n  proposals to acceptors; receives replies from a majority of\n  acceptors, and then sends \\(proposal\\) messages. The\n  \\(proposal\\) messages are received by a majority of acceptors, each\n  of which sends \\(accept\\) messages. So, learners that receive\n  \\(accept\\) messages from a majority of acceptors, set the values\n  that they learned to the value in the \\(accept\\) messages.\n  \n  \n\n\n\n</p><h3 class=\"w3-text-teal\">Theorem</h3>\nIf a majority of acceptors accept the same value, \\(V^{*}\\) in an\nepoch \\(T^{*}\\) of a semi-synchronous \ntrajectory, then in all subsequent epochs:\n<ol>\n<li>\n    the value in all proposal and accept messages is \\(V^{*}\\), and\n  </li>\n<li>\n  for all acceptors \\(A\\):\n    \\( (A.accept\\_t &lt; T^{*}) \\vee (A.accept\\_v =  V^{*}) \\)\n\t\t      </li>\n</ol>\n<h4 class=\"w3-text-teal\">Proof</h4>\nThe proof is by induction on successive epochs in the sequence of\nepochs that follows epoch \\(T^{*}\\). Let \\(T^{i}\\) be the time of the\n\\(i\\)-th epoch after epoch \\(T^{*}\\), and let \\(T^{0} = T^{*}\\).\n\n<p class=\"w3-text-teal\">Base Case: epoch \\(T^{*}\\)</p>\n<i>Part 1</i>: All proposals are sent by the\nsame proposer in an epoch, and all proposals have the same timestamp\nand value. Therefore, the value in all proposal and accept messages in\nepoch \\(T^{*}\\) is \\(V^{*}\\).\n\n<p>\n<i>Part 2</i>:  In epoch \\(T^{*}\\), \\(A.accept\\_t\\) is changed only when \n\\(A.accept\\_v\\) is set to \\(V^{*}\\), and so part 2 follows.\n\n</p><p class=\"w3-text-teal\">Induction step: epoch \\(T^{i+1}\\)</p>\nAssume that the theorem holds for epochs \\(T^{0}, T^{1}, \\ldots,\nT^{i}\\), and prove that it holds for epoch \\(T^{i+1}\\).\nThe result is vacuously true if no proposals are sent in\n   this epoch. Consider the case where proposals are sent.\n\n   <p>\n     Because  \\(A.accept\\_t\\) is never decreased, it follows\n     that in all epochs after epoch \\(T^{*}\\) there is a majority of\n     acceptors for which \\(A.accept\\_t \\geq T^{*}\\). \n   </p><p>\n     A proposer \\(p\\) sends a proposal only if it receives\n     \\(updated\\_time\\) messages from a majority of acceptors. Any two\n     majorities have at least one element in common. So, there exists\n     an acceptor \\(A\\) that sends an \\(updated\\_time\\) message to\n     \\(p\\) and for which \\(A.accept\\_t \\geq T^{*}\\). Because\n     \\(A.accept\\_t\\) is positive, \\(A.accept\\_v\\) \n     is not null (see the invariants).\n\n   </p><p>\n     So, proposer \\(p\\) identifies the \\(updated\\_time\\) message with\n     the largest \\(A.accept\\_t\\). This value is greater than or equal\n     to \\(T^{*}\\), and so the corresponding \\(A.accept\\_v \\) is\n     \\(V^{*}\\), and therefore the  proposal's value is\n     \\(V^{*}\\). So, the value of accept messages is also \\(V^{*}\\),\n     and if \\(A.accept\\_v\\) is changed then the new value is \\(V^{*}\\).\n\n\n     </p><h5 style=\"color:red;\">Example</h5>\n     This example illustrates the idea underlying the proof. In the\n     next diagram, proposer P0 sends a \\(new\\_time(T^{*})\\)\n     message which is received by acceptors A0 and A1 which respond\n     with updated time messages with timestamp \\(T^{*}\\). Proposer P0\n     receives both messages, and sends proposals to all acceptors. In\n     this case the proposals are received by A0 and A2 which then send\n     accept messages. When A0 and A2 receive the proposals they set\n     \\(A.accept\\_t, A.accept\\_v = T^{*}, V^{*}\\), and these values\n     remain unchanged until the next proposal is received. \n<figure>\n<img alt=\"Fig4\" src=\"raw_webcrawl_data/Paxos/./PaxosProof/PaxosProof.009.jpeg\" style=\"width:60%\"/>\n<figcaption>Fig.7: Idea underlying the proof</figcaption>\n</figure>\n\nThe diagram below shows an epoch T1 that follows epoch T0. In this\nepoch, proposer P1 sends \\(new\\_time(T^{1})\\) messages which are\nreceived by acceptors A0 and A1 which respond with updated time\nmessages with timestamp \\(T^{1}\\).\nWhen P1 gets both messages it sends proposals to all acceptors.\nLet's use the argument of the proof\nto determine the value of proposals and the accept message sent by A1 in this\nepoch. \n\n\n<figure>\n<img alt=\"Fig4\" src=\"raw_webcrawl_data/Paxos/./PaxosProof/PaxosProof.010.jpeg\" style=\"width:60%\"/>\n<figcaption>Fig.8: What can be proved about the next accept message?</figcaption>\n</figure>\n\n\nWhen P1 gets both updated time messages it\nidentifies the message with the largest value of \\(A.accept\\_t\\). The\nmessage sent by A0 has timestamp \\(T^{*}\\) whereas the message sent by\nA1 has an earlier timestamp. So, the updated time message with the\nlargest \\(A.accept\\_t\\) is the message sent by A0. \n\nP1 sends proposals with a value of\n\\(A0.accept\\_t\\) which is \\(V^{*}\\). The green circles show how the\nvalue \\(V^{*}\\) is sent from A0 to P1 to A1.\n\n   <p>\n     We see that proposals and accept\nmessages have value \\(V^{*}\\) in the next epoch as well. \n<figure>\n<img alt=\"Fig4\" src=\"raw_webcrawl_data/Paxos/./PaxosProof/PaxosProof.012.jpeg\" style=\"width:60%\"/>\n<figcaption>Fig.9: The role of the accept time: accept_t </figcaption>\n</figure>\n</p><h3 class=\"w3-text-teal\">Theorem: Paxos is Correct for\n  Semi-Synchronous Trajectories</h3>\nThe specification of the algorithm is repeated here for convenience.\n<ol>\n<li>\n<i>Learners learn only proposed values.</i>\n</li>\n<li>\n<i>A learner doesn't change the value that it learns.</i>\n</li>\n<li>\n<i>All learners learn the same value.</i>\n</li>\n</ol>\n<p class=\"w3-text-teal\">Proof</p>\n   The value of\n<code>v</code> in every message is either null or <code>P.VALUE</code>\nfor some proposer P, and so part 1 follows. Parts 2 and 3 follow from\nthe previous theorem: if a majority of acceptors send identical\n \\(accept(T^{*}, V^{*})\\) messages in an epoch, then in all succeeding\nepochs, the value \\(v\\) of every accept message remains \\(V^{*}\\).\n\n\n<h3 class=\"w3-text-teal\">Paxos is Correct for\n  all Trajectories</h3>\n\nWe have shown that the Paxos algorithm is correct for semi-synchronous\ntrajectories. Now we show the correctness of the algorithm for all\ntrajectories.\n\nLet \\([past(T), future(T)]\\) be a cut of a timeline where \\(past(T)\\)\nis the set of events with time \\(T\\) or\nless. Therefore \\(future(T)\\) is the set of events with time greater than \\(T\\).\n\n<h4 class=\"w3-text-teal\">Theorem: Consistent cuts</h4>\nThe cut \\([past(T), future(T)]\\) is consistent.\n\n<h5 class=\"w3-text-teal\">Proof</h5>\nThere is no message from an event in \\(future(T)\\)\nto an event in \\(past(T)\\).\n\n<p>\n  If a proposer sends a message in the future, i.e., with a timestamp\n  \\(T' &gt; T\\) then the event in which an acceptor processes the message also\n      has timestamp \\(T'\\), and is also in the future.\n      Likewise, if a proposer sends a message with a timestamp\n  \\(T' &gt; T\\) then the event in which an acceptor receives the message also\n      has timestamp \\(T'\\).\n\n</p><h5 style=\"color:red;\">Example</h5>\n<figure>\n<img alt=\"Fig8\" src=\"raw_webcrawl_data/Paxos/./PaxosProof/PaxosProof.018.jpeg\" style=\"width:80%\"/>\n<figcaption>Fig.8: Consistent Cuts of Paxos Trajectories</figcaption>\n</figure>\n\nThe diagram shows a cut \\([past(T), future(T)]\\) of a trajectory. The\ngreen circles represent events in the past, and the yellow circles\nrepresent events in the future. A little analysis shows that there are\nno edges from the future to the past. (Indeed, you can show that there\nare no edges from the past to the future, and so the graph can be\npartitioned into independent subgraphs.)\n\n", "24": ""}, "raw_webcrawl_data/Paxos/PaxosExamples.html": {"0": "<h1 class=\"w3-text-teal\">Paxos Examples</h1>\n", "1": "<h2 class=\"w3-text-teal\">Question 1</h2>\nConsider a system with 2 proposers P1 and P2 and 3 acceptors R1, R2,\n    R3. Associated with each proposer P is a local variable P.v. This\n    local variable is a color which is \"red\" for P1 and \"blue\" for\n    P2. The system uses the Paxos algorithm by which a majority of\n    acceptors come to a consensus about the colors of P1 and P2.\n    <p>\n    Write down a scenario by which acceptor R1 accepts (T, V) = (1,\n    \"red\") and also accepts (T, V) = (2, \"blue\")\n\n    </p>", "2": "<h3 class=\"w3-text-teal\">Answer to Question 1\n    \n    </h3><p class=\"w3-text-teal\">First Round</p>\n<ul>\n<li>\n      Step P.1: P1 sends prepare(1)\n      </li>\n<li>\n      Step A.1: Acceptors R1, R2 receive prepare(1) and reply with\n      \n      <br/>\n      promise(R.t, R.v, R.acceptT) = promise(1, null, null).\n      </li>\n<li> \n    Step P.2: P1 receives promise(1, null, null) from 2 of the 3\n    acceptors. So P1\n      sends\n      \n      <br/>propose(P.t, P.v) = propose(1, \"red\")\n      \n      <br/>\n      to all\n    acceptors. This message reaches acceptor R1 but does not reach the\n    other acceptors.\n      </li>\n<li>\n    Step A.2: Acceptor R1 accepts (1, \"red\") and sends accept(1,\n      \"red\") to all learners.\n    </li></ul>\n<p class=\"w3-text-teal\">Next Round</p>\n<ul>\n<li>\n    Step P.1: P2 sends prepare(2)\n      </li>\n<li>\n      Step A.1: Acceptors R2, R3 receive prepare(2) and reply with\n      <br/>\n      promise(R.t, R.v, R.acceptT) = promise(2, null, null).\n      <br/>\n      Acceptor R1\n    does not receive this prepare message.\n      </li>\n<li>\n    Step P.2: P2 receives promise(2, null, null) from 2 of the 3\n    acceptors. So P2\n      sends\n      <br/>\n      propose(P.t, P.v) = propose(2, \"blue\")\n      <br/>\n      to all\n    acceptors. This message reaches acceptor R1 but does not reach the\n    other acceptors.\n      </li>\n<li>\n    Step A.2: Acceptor R2 accepts (2, \"blue\") and sends accept(2,\n      \"blue\") to all learners.\n  \n\n    \n  </li></ul>", "3": "<h2 class=\"w3-text-teal\">Question 2</h2>\n  Is the following true? If a majority of acceptors accepts (T, V) and\n  another majority of \n  acceptors accepts (T', V') then (T, V) = (T', V')?\n\n  "}, "raw_webcrawl_data/Paxos/StableMajority.html": {"0": "<h2 class=\"w3-text-teal\">Paxos: Consensus in Faulty Systems</h2>\n", "1": "<h3 class=\"w3-text-teal\">Part 1</h3>\n\n\n<p class=\"w3-text-red\">\n  Algorithms by which agents reach a consensus on a value are central\n  in many applications.\n\n  Paxos is an algorithm by which agents attempt to reach a consensus\n  in distributed systems in which agents may halt, be arbitrarily\n  slow, and messages may be duplicated, lost, and delivered out of\n  order.\n\n  In this page we describe and prove a nondeterministic sequential\n  representation of Paxos, and describe the distributed algorithm in\n  the next page.\n\n\n\n  \n</p>", "2": "<h3 class=\"w3-text-teal\">Introduction</h3>\n\n\n  \n  Paxos is a consensus algorithm for systems in which messages may be lost; multiple\n  copies of a message may be delivered; messages may not be\n  delivered in the order sent; agents may be arbitrarily slow; and\n  agents may stop.\n  \n  From\n  <a href=\"../ConsensusImpossible/ConsensusImpossible.html\">the\n  FLP theorem</a>,\n  \n  there is no algorithm that guarantees that consensus among agents\n  will be reached in such systems. \n\n\n  Paxos may not terminate. \n  We will discuss ways to improve the likelihood of termination.\n\n\n  <p>\n  Consider the problem of maintaining a ledger consisting\n  of a sequence of transfers of funds into and out of accounts.\n  Assume that \n  the ledger is implemented using copies at multiple agents.\n\n  The copies may not be synchronized; however there must exist a\n  consensus copy. \n\n  We call the sequence of operations on the ledger a <i>chain</i> of\n  operations and we call the the consensus value a <i>consensus chain</i>.\n\n  </p><p>\n  Suppose two clients\n  simultaneously request extensions to a consensus chain by transferring funds\n  from an account x to two different accounts.\n\n  Account x may not have funds to allow both transfers, and so the\n  consensus chain can be extended by at most one of the transfers.\n\n  Clearly, the system must maintain a consensus regarding the order of\n  operations in the chain.\n\n  </p><p>\n\n  Agents that propose extensions to consensus chains are called <i>clients</i>.\n\n  Copies of the chain are stored at agents called\n  <i>servers</i>.\n\n  </p><p class=\"w3-text-teal\">Notation: prefix</p>\n  A <i>prefix</i> of a sequence S is an initial subsequence\n  of S.\n  For example, [A, B] is a prefix of [A, B, C] but [A, C] is not a\nprefix of [A, B, C].\n\n<p>\nThe empty sequence is a prefix of all sequences.\nA sequence is a prefix of itself.\n\nWe use the notation \\(\\leq\\) for prefix, as in: \n[A, B] \\(\\leq \\) [A, B, C]\n\n\n\n</p>", "3": "<h3 class=\"w3-text-teal\">Specification of Consensus</h3>\n\n\n", "4": "<h4 class=\"w3-text-teal\">1. A consensus isn't changed.</h4>\n\n\n\n For all consensus chains \\(c\\) and \\(d\\) in a computation: either\n\\(c\\) is a prefix of \\(d\\) or \\(d\\) is a prefix of \\(c\\).\n\n<p>\n\\( (c \\leq d) \\; \\vee \\; (d \\leq c) \\)\n\n</p><p>\nFor example, a consensus chain [A, B] can be extended to form a\nconsensus chain [A, B, C].\n\n</p><p>\n[A, B] and [A, C, B] cannot both be consensus chains in\nthe same computation.\n\n\n\n\n\n</p>", "5": "<h4 class=\"w3-text-teal\">2. Every consensus chain is proposed by a\nclient</h4>\n\n\nFor all consensus chains \\(c\\) in a computation:\nThere exists a client that proposed \\(c\\) in the computation.\n\n<p>\n\nThis part of the specification merely says that consensus chains cannot be\narbitrary.\n\nLater, we define the specification in terms of algorithm\nvariables.\n\n</p><p>\nNext, we describe the agents -- servers and clients -- in the system. \n\n\n</p>", "6": "<h3 class=\"w3-text-teal\">Servers</h3>\n\n  \n  Each server <code>q</code> has a local variable <code>q.v</code>\nwhich has two fields:\n<ol>\n<li>\n<code>q.v.s</code>: the chain stored at <code>q</code>, and\n  </li>\n<li>\n<code>q.v.t</code>: a sequence number that indicates when\n  <code>q.v</code> was last written.\n  </li>\n</ol>\n<p>\nA server <code>q</code> receives requests to read or write\n<code>q.v</code>.\n\nThe only actions of a server are to respond to requests from clients.\n\n\n\n\n\n</p>", "7": "<h3 class=\"w3-text-teal\">Clients</h3>\n\n\nEach client receives a sequence of <i>clock tick</i> messages.\n\nThe intervals between successive clock ticks are irrelevant for\nthe correctness of the algorithm but impact performance.\n\n  A client avoids waiting forever for a reply from a server by only accepting replies\nthat the client receives before it receives its next clock tick message.\n\n\n\n<p>\nA client repeatedly executes actions in which it reads and then writes\ncopies of chains stored at servers.\n\n\n\n</p>", "8": "<h5 class=\"w3-text-teal\">Read</h5>\nA client sends read requests to all servers.\n\nLet <code>R</code> be the set of servers from which the client\nreceives replies before the client receives its next clock tick.\n\nIf <code>R</code> has fewer than <code>M</code> elements, where\n<code>M</code> is a given constant,\nthen the action terminates without executing the write step.\n\nIf <code>R</code> has <code>M</code> or more elements then a client\nproceeds to the write step.\n\n\n\n", "9": "<h5 class=\"w3-text-teal\">Write</h5>\nA client <code>p</code> \nsends a request to all servers to write:\n<pre>\np.f(q.v for q in R)\n</pre>\nwhere <code>f</code> is a given function, and the argument of the function\nis the set of replies that <code>p</code> received.\n\n<p>\nWrite requests may get lost.\n\nLet <code>W</code> be the set of servers that execute write requests.\n\n\n\n</p>", "10": "<h3 class=\"w3-text-teal\">Designing Paxos in Stages</h3>\n\nWe develop the algorithm in two stages.\n\n\n", "11": "<h5 class=\"w3-text-teal\">Paxos as a Nondeterministic Sequential\nAlgorithm</h5>\n\nFirst we represent Paxos as nondeterministic sequential algorithm\n\\(P\\).\n\nExactly one client executes actions at a time in the sequential\nprogram.\n\nFailures of agents and channels are represented by nondeterministic\nselection of servers that read and write values.\n\nWe will prove that \\(P\\) satisfies specifications of consensus.\n\n\n\n", "12": "<h5 class=\"w3-text-teal\">Paxos as a Serializable Distributed\nAlgorithm</h5>\nIn the second stage we develop a distributed algorithm.\n\nClients and servers execute concurrently, and so the action of one\nclient may interfere with the action of another.\n\nA client <code>p</code> may read server values and another client may\nmodify these values before <code>p</code> writes these values.\n\n<p>\nWe will develop a serializable distributed algorithm: for every\ncomputation \\(x\\) there exists a computation \\(y\\) with the same\nsteps as \\(x\\) where exactly one client takes actions at a time in\n\\(y\\).\n\nWe will show that the proof of the sequential program is also a proof\nof the distributed program.\n\n\n\n</p>", "13": "<h3 class=\"w3-text-teal\">A Sequential Representation of Paxos</h3>\n\nWe develop Paxos in stages.\n\nWe begin by proving properties of the following nondeterministic\nsequential program \\(P\\).\n\nNondeterminism in the sequential program represents nondeterminism in\nboth the distributed program and in failures of agents and channels.\n\n\n<hr class=\"new2\"/>\n<p style=\"color:blue;\">Nondeterministic Sequential Program \\(P\\)</p>\n<pre>\nfor n = 1 to infinity:\n   select a client p and subsets R, W of servers\n\n   for q in W:\n      q.v.s = p.f(q.v for q in R)\n      q.v.t = n\n</pre>\n<hr class=\"new2\"/>\n<p>\nMany clients and servers execute concurrently in Paxos.\n\nBy contrast, exactly one client executes an action at a time in\nprogram \\(P\\).\n\nWe will show that certain properties of \\(P\\) are\nalso properties of Paxos.\n\n</p><p>\nWe prove properties of sequences of server state changes.\n\nWe now restrict attention to iterations of \\(P\\) in which at least one\nserver changes state.\n\nSo, hereafter we only consider iterations in which <code>R</code> has\nat least <code>M</code> elements, and <code>W</code> is nonempty.\n\n\n\n</p>", "14": "<h3 class=\"w3-text-teal\">The Problem</h3>\n\nThe problem is to specify the following values so that the\nspecification for consensus is satisfied.\n<ol>\n<li>\n<code>M</code>: A client proceeds from the read step to the write\n  step only if it receives at least <code>M</code> replies to its read\n  requests. What should <code>M</code> be?\n  </li>\n<li>\n  Consensus: How is a consensus chain defined in terms of the server variable\n  <code>q.v</code> for all servers <code>q</code>?\n  </li>\n<li>\n<code>f</code>: A client executes a function <code>f</code> on the <code>M</code> or\n  more replies that it receives. What is <code>f</code>?\n  </li>\n</ol>\n\n", "15": "<h3 class=\"w3-text-teal\">How Should You Solve The Problem?</h3>\n\n\n", "16": "<h5 class=\"w3-text-teal\"><code>M</code>: Number of Replies</h5>\n\nA client needs to receive at least <code>M</code> copies of chains\nto proceed to the write step.\n\nWhat is a reasonable value of <code>M</code>?\n\n<p>\nLet <code>N</code> be the total number of servers.\n\nWe could set <code>M = N</code>, and in this case a client proceeds\nfrom the read step\nto the write step only if the client receives chains from all servers.\n\nThis seems safe; however, a client may never proceed to the write\nstep because the client may not receive a reply from a failed server.\n\nThe smaller the value of <code>M</code> the greater the likelihood of\nthe algorithm terminating sooner.\n\n</p><p>\nSetting <code>M = int(N/2) + 1</code>, a majority, is reasonable\nbecause any two majorities have at least one element in\ncommon. So, if two clients both proceed to the write step then both\nclients have received replies from at least one common server.\n\n\n\n</p>", "17": "<h5 class=\"w3-text-teal\">Consensus in terms of Server Variables</h5>\nWe define consensus as a value agreed upon by some number of\nservers. How many?\n\n<p>\nWe can define a consensus as a value agreed upon by all <code>N</code>\nservers.\n\nHere again, the smaller the number of servers required, the greater\nthe likelihood that a consensus will be reached.\n\nUsing the same argument that we used for setting <code>M</code> to be\na majority, let's define consensus as a value agreed upon by a\nmajority. \n\n\n\n\n</p><hr class=\"new2\"/>\n<p style=\"color:blue;\">\nConsensus: Majority of Servers Agree on a Consensus Value\n\n</p><p style=\"color:blue;\">\n<code>s*</code> is a consensus chain in a computation of \\(P\\) exactly\nwhen there exists a majority <code>W*</code> of servers and an\niteration number <code>t*</code>  where for all <code>q</code> in\n<code>W*</code>:\n</p><center>\n<p style=\"color:blue;\">\n<code>(q.v.s = s*)</code> and <code>(q.v.t = t*)</code>\n</p></center>\n<hr class=\"new2\"/>\n\nSo, <code>s*</code> is a consensus chain exactly when in iteration\n<code>t*</code>, for some <code>t*</code>: \nThe set <code>W</code> of servers written in the iteration\nis a majority and  <code>s*</code> is the chain assigned to\n<code>q.v.s</code> \nfor all <code>q</code> in <code>W</code>..\n\n\n\n", "18": "<h5 class=\"w3-text-teal\"><code>f</code>: Client's Write Requests</h5>\n\nThe argument of <code>f</code> is a list <code>lst</code>\n<code>\n[q.v for q in R]\n</code>\nwhich is a list of <code>M</code> or more elements.\n\n\n\n<p class=\"w3-text-teal\">Case 1: All elements of <code>lst</code> are\nidentical</p>\n\nIf all elements of <code>lst</code> are identical then every element of\n<code>lst</code> is a consensus chain.\n\nLet <code>c</code> be any element of <code>lst</code>, for example\n<code>c = lst[0]</code>.\n\nThe function returns the consensus chain <code>c</code>\nappended with some value <code>h()</code>.\n\n\n\n\n<p class=\"w3-text-teal\">Case 2: Not all elements of <code>lst</code> are\nidentical</p>\n\nIf not all elements of <code>lst</code> are identical then let\n<code>max_t</code> be the element of <code>lst</code> with the largest\n<code>t</code> value.\n\nReturn <code>max_t.s</code>.\n\n<p>\nThe algorithm is given below.\n\n</p><hr class=\"new2\"/>\n<pre>\ndef f(lst):\n  # if all elements of lst are identical:\n  if all(lst[0] == c for c in lst):\n     c = lst[0].s\n     return c.append(h())\n\n  else:\n     max_t = lst[0]\n     for c in lst:\n        if c.t &gt; max_t.t:  max_t = c \n     return max_t.s\n</pre>\n<hr class=\"new2\"/>\n\n\n", "19": "<h3 class=\"w3-text-teal\">Theorem: The Algorithm Satisfies the Specification</h3>\n\n\n\n", "20": "<h4 class=\"w3-text-teal\">Proof</h4>\nPart 2 of the specification follows directly from the algorithm\n  because the only assignments to <code>q.v.s</code> is\n  <code>p.f()</code>. \n\nNext,  we prove part 1. We will show that if <code>c</code> is a\nconsensus chain after an iteration and <code>d</code> is a\nconsensus chain after a later iteration then <code>c</code> is a\nprefix of <code>d</code>.\n\n<p>\nLet <code>s*</code> be a consensus at some point in a computation of\n\\(P\\).\n\nThen there exists  <code>t*</code> such that at the end of the\n<code>n</code>-th iteration,\n<code>n</code> \\(=\\) <code>t*</code>,\nthere exists a majority <code>W*</code> of servers such that:\n<br/>\nFor all <code>q</code> in\n<code>W*: </code> <code>q.v.s = s*</code> and <code>q.v.t = t*</code>. \n\n</p><p>\nBecause <code>q.v.t</code> does not decrease the following equation holds.\n\n</p><p class=\"w3-text-teal\">Equation 1</p>\nFor all\n<code>n</code> where <code>n</code> \\(\\geq\\) <code>t*</code>:\n\nFor all <code>q</code> in\n<code>W*: </code> <code>q.v.t</code> \\(\\geq\\)  <code>t*</code>\n<p>\nNext, we prove the following:\n\n</p>", "21": "<h5 class=\"w3-text-teal\">Invariant</h5>\n For all\nservers <code>q</code>: if <code>q.v.t</code> \\(\\geq\\) <code>t*</code>\nthen <code>s*</code> is a prefix of <code>q.v.s</code>.\n\n<p>\nThis can be written as:\n</p><p>\nFor all servers <code>q: </code>\n\n(<code>q.v.t</code> \\(&lt;\\) <code>t*</code>) \\(\\; \\vee \\;\\)\n(<code>s*</code> \\(\\leq\\) <code>q.v.s</code>)\n\n\n</p>", "22": "<h5 class=\"w3-text-teal\">Proof of the Invariant</h5>\n<p class=\"w3-text-teal\">Base Case: <code>n</code>-th Iteration where\n<code>n</code> \\(\\leq\\) <code>t*</code></p>\n<p>\nThe condition trivially holds for iteration <code>n</code> where\n<code>n</code> \\(&lt;\\) <code>t*</code>\nbecause <code>q.v.t</code> \\(&lt;\\) <code>t*</code> in\nthese iterations.\n\n</p><p>\nNext, consider the case where <code>n</code> \\(=\\) <code>t*</code>.\nIf <code>q.v</code> is modified in the iteration then\n<code>q.v.s</code> \\(=\\) <code>s*</code>.\n\nIf <code>q.v</code> is not modified in the iteration then\n<code>q.v.t</code> \\(&lt;\\) <code>t*</code>.\n\nSo, the condition holds.\n\n</p><p class=\"w3-text-teal\">Induction Step</p>\n\nAssume that the condition holds before iteration <code>n</code> for\n<code>n</code> \\(&gt;\\) <code>t*</code> and \nshow that it holds after the iteration.\n\n<p>\nFrom property 1 and the induction hypothesis, the following holds\nbefore iteration <code>n</code>.\n</p><p>\nFor all <code>q</code> in\n<code>W*: </code> (<code>q.v.t</code> \\(\\geq\\)  <code>t*</code>)\n\\(\\; \\wedge \\; \\)\n(<code>s*</code> \\(\\leq\\) <code>q.v.s</code>)\n\n</p><p>\nThe argument <code>lst</code> of <code>f</code> is\n\n<code>[q.v for q in R]</code>\nwhere <code>R</code> is a majority.\n\n\n</p><p>\nAny two majorities have at least one element in common.\nSo there is an\nelement that is in both <code>R</code> and <code>W*</code>.\n\nLet this element be <code>q*</code>.\n\nBecause <code>q*</code> is an element of <code>W*</code>:\n</p><p>\n(<code>q*.v.t</code> \\(\\geq\\)  <code>t*</code>)\n\\(\\: \\wedge \\;\\)\n(<code>s*</code> \\(\\leq\\) <code>q*.v.s</code>)\n</p><p>\nLet the value returned by function <code>f</code> be <code>d</code>.\n\nWe will show that <code>c</code> \\(\\leq\\) <code>d</code>.\n\nConsider the two cases of <code>lst</code>.\n\n</p><p class=\"w3-text-teal\">\nCase 1: All elements of <code>lst</code> are identical.\n</p>\n<p>\nBecause\n<code>q*.v</code> is an element of <code>lst</code>, every element of <code>lst</code>\nis identical to <code>q*.v</code>.\n\nSo <code>s*</code> is a prefix of the value <code>d</code>\nreturned by the function.\n\n</p><p class=\"w3-text-teal\">\nCase 2: Not all elements of <code>lst</code> are identical.\n</p>\n<p>\nIf not all elements of <code>lst</code> are identical then the\nfunction returns <code>q.v.s</code> where <code>q.v</code> is the element\nwith the highest iteration number <code>q.v.t</code>.\nThere is an element <code>q*</code> of <code>lst</code> where\n<code>q*.v.t</code> \\(\\geq\\) <code>t*</code>.\nSo the function returns <code>q.v.s</code> where <code>q.v.t</code>\n\\(\\geq\\) <code>t*</code>.\nFrom the induction hypothesis <code>s*</code> is a prefix of the\nreturned value <code>d</code>.\n\n\n\n</p>"}, "raw_webcrawl_data/Paxos/ConsensusImpossible.html": {"0": "<h1 class=\"w3-text-teal\">Consensus</h1>\n\n\n", "1": "<h4 class=\"w3-text-red\">\n<i>Central Ideas</i></h4>\n<p class=\"w3-text-red\">\n  (1) Importance of consensus (2) Impossibility of consensus in\n  distributed systems with a faulty agent.\n\n\n</p>", "2": "<h3 class=\"w3-text-teal\">Importance of Consensus</h3>\n<p>\nAlgorithms by which groups of agents <a href=\"https://en.wikipedia.org/wiki/Consensus_(computer_science)\">\ncome to a consensus </a> are among the most fundamental problems in\ndistributed computing.\n\n</p><p>\nWhy is consensus important? There are many\nproblems in which messages are sent to groups of agents who\ncollectively maintain a common <i>consensus</i> state. A bank may use\na group of agents, rather than a single agent, to maintain bank\nbalances. Multiple agents reduce the possibility of system-wide\nfailure due to the failure of a single agent.  Managing replicated\ndatabases requires the replications to come to a consensus on the\nsequence of transactions that is applied to the\ndatabase. Cryptocurrency transactions also require collections of\nagents to come to a consensus about sequences of the transactions.\n\n</p><p>\nIn a control system with multiple and actuators, the actuators have to\ncome to a consensus about the state of the environment so that they\ncan operate in concert. A vehicle would crash if some actuators caused\nthe vehicle to accelerate while other actuators applied brakes.  In\nsome applications, multiple agents have to elect a single leader.\nThere are many problems in which a collection of agents have to come\nto a <i>consensus</i> about something.\n\n    </p>", "3": "<h3 class=\"w3-text-teal\">Consensus: Impossible with a\n    faulty agent</h3>\n    Consensus is impossible with even a single faulty agent. This was\n    proved in a paper published by Fischer, Lynch and Patterson.\n\n    <p>\n    You can get the idea of why consensus is not possible by\n    considering the following problem in which\n    when message delays are finite but arbitrarily long.\n    A collection of 2N + 1 agents want to\n    come to a consensus about a color. N of the agents pick blue and\n    N+1 pick red. One of the red agents is arbitrarily slow. The\n    2N non-slow agents exchange messages among each other, and each of\n    these 2N agents gets N votes for red and N votes for blue. Agents\n    decide to take a majority vote, and in the event of a tie pick\n    blue.\n\n</p><p>\n<figure>\n<img alt=\"Fig1\" src=\"raw_webcrawl_data/Paxos/./Slide01.jpg\" style=\"width:80%\"/>\n<figcaption>Fig.1: Problem with a slow agent</figcaption>\n</figure>\n</p><p>\n    How long should they wait for the slow agent?\n\n    </p><p>\n    Consider an algorithm in which agent waits until its local clock\n    shows an elapsed time of T and then makes a decision based on the\n    votes that it has.  An agent Y gets N red and N blue votes when\n    its clock shows an elapsed time of T, and agent Y decides that the\n    consensus is blue. Another agent Z has a slower clock and gets a\n    red vote from the slow agent for a total of N+1 red votes, before\n    Z's clock shows an elapsed time of T. So Z determines that the\n    consensus is red. The algorithm fails because Y and Z have not\n    come to a consensus. \n\n</p><p>\nNo algorithm is guaranteed to come to a consensus in finite time if\nmessages can be arbitrarily slow or if agents can be arbitrarily slow.\nSystems with synchronized clocks don't have this particular\nproblem. We'll look at consensus in such systems later.\n\n</p>", "4": "<h2 class=\"w3-text-teal\">Best Effort Consensus\nThe theorem says that there is no algorithm that guarentees that\nconsensus can be reached in all scenarios; \nhowever, consensus can be reached in most practical situations. An idea\nto overcome the counterexample given above is:\nAgents keep trying repeatedly until they reach consensus. The theorem tells us\nthat the agents may have to keep trying for ever. We expect, however,\nthat in most practical situations their attempts will succeed at some point.\n\n</h2><p>\nWhat does keep trying mean? When does one trial\nend and the next one begin? If agents use timeouts to end a trial,\nthen --- because clocks aren't synchronized --- the timeouts may complete\nat different times.\nWe'll see that we can use the idea of time, even though clocks aren't\nsynchronized. We've done that before with logical clocks. The Paxos\nalgorithm shows how the idea of increasing values of timestamps (or\nids) are used for best-effort consensus.\n\n\n\n\n</p>", "5": "<h4 class=\"w3-text-red\">\n<i>Central Ideas: Review</i></h4>\n<p class=\"w3-text-red\">\nMany applications of distributed systems require agents to come to a\nconsensus.\nAgents cannot come to a consensus if an agent is faulty.\n\n\n</p><p class=\"w3-text-red\">\n<i>Concepts</i>:\nConsensus -&gt; impossibility with faulty agents\n\n\n</p>"}, "raw_webcrawl_data/Paxos/PaxosFinal.html": {"0": "<h1 class=\"w3-text-teal\">The Paxos Algorithm</h1>\n\n<p class=\"w3-text-red\">\n\n  This page describes\n  <a href=\"https://en.wikipedia.org/wiki/Paxos_(computer_science)\">\n<i>Paxos</i>\n</a>,\n  an algorithm by which a collection of servers execute identical\n  sequences of operations proposed by clients in a faulty system.\n\n  Agents may halt and messages may be duplicated, lost, and delivered\n  out of order.\n  </p>\n\n", "1": "<h3 class=\"w3-text-teal\">Introduction</h3>\n\n    \n  From\n  <a href=\"../ConsensusImpossible/ConsensusImpossible.html\">the\n  FLP theorem</a>,\n  \n  there is no algorithm that guarantees that consensus among agents\n  will be reached if messages can be lost and delayed, and if agents may\n  fail and be arbitrarily slow. \n\n  Paxos may not terminate though it enables consensus to be reached in\n  most situations.\n\n  <p>\n  The Paxos algorithm is an application of\n  <a href=\"StableMajority.html\">stable majorities in faulty\n  systems</a>. \n  \n  The algorithm is designed for systems in which messages may be lost; multiple\n  copies of a message may be delivered; messages may not be\n  delivered in the order sent; agents may be arbitrarily slow; and\n  agents may stop.\n\n  </p><p>\n  A system consists of agents called <i>servers</i> and agents called\n  <i>clients</i>.\n\n  Clients propose operations to be executed by servers.\n\nThe sequence of operations at a server is called a <a hef=\"https://en.wikipedia.org/wiki/Logging_(computing)\">\n<i>log</i></a> of the\n  server.\n\n  The problem is to ensure that the logs at different\n  servers are replicas of each other.\n\n  </p><p>\n  Let \\(x\\) and \\(y\\) be sequences.\n  \\(x\\) is a <i>prefix</i> of \\(y\\), written as \\(x \\leq y\\),\n  exactly when \\(x\\) is an initial subsequence of \\(y\\).\n\nFor example, the prefixes of \\([a, b, c]\\) are \\([], [a], [a, b]\\) and\n\\([a, b, c]\\).\n\nThe algorithm is required to have the following\n  invariant. For all logs \\(x, y\\) at servers:\n\n</p><p>\n\\(\n(x \\leq y) \\vee (y \\leq x)\n\\)\n\n</p><p>\nFor any set of logs with at least \\(n\\)\noperations the \\(n\\)-th operation in all logs must be identical.\n\nThe log at one server may be behind, the same as, or ahead of the log\nat another server.\n\n\n</p><pre>\nt = 0\nfor all servers q:\n   q.v.value = None\n   q.v.t = 0\n\nwhile True:\n   select an arbitrary client p\n   t = t + positive value\n\n   # p executes a transaction with epoch t\n\n   # p reads variables\n   select an arbitrary majority R of servers\n   server_items = [q.v for q in R]\n   if all server items are identical:\n      propose_value = server_log_t + self.next_value(server_log_t)\n   else:\n      propose_value = server_log_t\n\n   # p writes variables\n   select an arbitrary subset W of servers\n   for q in W:\n        q.v.value = propose_value\n        q.v.t = t\n  </pre>\n<p>Invariant</p>\nFor servers q, q': q.log[:-1] &lt; q'.log[:-1]\n\n\n<p>\n\n  Associated with each client <code>p</code> is a local constant\n  <code>p.VALUE</code>.\n\n  A manager <code>q</code> has a local variable <code>q.v</code>, and\n  <code>q</code> gets requests to read and write <code>q.v</code>.\n\n  In this algorithm, <code>q.v</code> has two fields,\n  <code>q.v.value</code> and <code>q.v.id</code>, which take on values\n  and transaction ids of clients, respectively.\n\n  Design an algorithm such that:\n  </p><ol>\n<li>\n    For each manager <code>q</code> of a shared variable,\n  <code>q.v.value</code> is <code>None</code> or <code>p.VALUE</code>\n    for some client <code>p</code>.\n    </li>\n<li>\n</li>\n</ol>\n<p> A system has agents called <i>proposers</i> and agents called\n  <i>learners</i>.\n\n  Proposers propose values and learners come to a consensus among the\n  proposed values. \n\n    </p><p>\nEach learner \\(L\\) has a local variable \\(L.v\\) \n  which is initially \\(None\\).\n  If \\(L.v\\) becomes any non-None value \\(w\\) then we\n  say that \\(L\\) has learned \\(w\\).\n\n  Associated with each proposer \\(P\\) is a\nlocal constant \\(P.V\\) which is its proposed value. The specification\n of the algorithm is that the \n following conditions must always hold.\n</p><p>\n</p><ol>\n<li>\n  Learners learn only proposed values. A learner's value is either\n  None or some proposer's value.\n  <br/>\n  \\(\\forall \\; \\textrm{learners} \\;  L: \\quad\n  (L.v = null) \\; \\vee \\;\n  (\\exists \\; \\textrm{proposer} \\: P : L.v = P.V)\n  \\)\n  </li>\n<li>\n  A learner doesn't change a value that it learns.\n  If \\(L.v = v*\\) at some point in any computation then \\(L.v = v*\\) in\n  succeeding states of the computation.\n  <br/>\n  \\(\n  \\forall \\; \\textrm{learners} \\; L, V \\neq null: \\quad stable (L.v = V)\n  \\)\n  </li>\n<li>\n  All learners learn the same value.\n  <br/>\n  \\(\n  \\forall \\; \\textrm{learners} \\; L, L': \\quad\n  (L.v = None) \\vee (L'.v = None) \\vee \n  ( L.v = L'.v)\n  \\)\n  </li>\n</ol>\nThe specification allows for the possibility that\nconsensus may never be reached; learners may never learn a value.\n\n<p>\nWe use the algorithm for accessing shared variables to solve this\nproblem.\nProposers are writers of shared variables.\nThe algorithm ensures that if a majority of shared variables have the\nsame value \\(v*, t*\\) in an iteration then in every succeeding\niteration with epoch time \\(t\\) a shared variable is either not assigned a value or is assigned a\nvalue \\(v*, t\\).\n\n\n\n\n\n</p>", "2": "<h4 class=\"w3-text-teal\">Algorithm Structure</h4>\n\n\nThe structure of the algorithm is as follows.\n\n<pre>\n  t = 0\n  for a in A: a.v, a.t = None, 0\n      \n  while True:\n    t = t + 1\n    p = any_element(P)\n    read_subset = any_subset(A)\n\n    if len(read_subset) &gt;= len(A)/2:\n        write_subset = any_subset(A)\n        for a in write_subset:\n           a.v, a.t = f(p, read_subset), t\n</pre>\n\nNext we describe function <code>f</code>.\n\nIf the time for all acceptors in <code>read_subset</code> is 0 then\nthe function returns <code>p.v</code>, the proposer's value.\n\nOtherwise the function returns the value of the acceptor in\n<code>read_subset</code> with the largest time.\n\n<pre>\ndef f(p, read_subset):\n   v, t = p.v, 0\n   for a in read_subset:\n      if a.t &gt; t:   v, t = a.v, a.t\n   return v\n</pre>\n", "3": "<h5 class=\"w3-text-teal\">Properties of the Sequential Program</h5>\nThe proofs of the following observations are straightforward.\n\n<p>\nIn all states of the computation, for all acceptors \\(a\\),\n</p><ol>\n<li>\n<code>a.t</code> does not decrease.\n  </li>\n<li>\n<code>a.t</code> is the iteration number in which <code>a.v</code>\n  was last assigned a value.\n  </li>\n<li>\n  \\(a\\)'s value is None or is a value of some proposer.\n  <p>\n  \\(\n  \\forall a:  (a.v = None) \\vee (\\exists p: a.v = p.v)\n  \\)\n  </p></li>\n</ol>\n\n  The program has the following important property which we prove\nnext.\n<hr class=\"new2\"/>\n<p style=\"color:blue;\">\n  If the write subset is a\nmajority in any iteration, let \\(u\\) be the value assigned to acceptors in\n  the write subset in that iteration;\nthen,\n</p><p style=\"color:blue;\">\nthereafter an acceptor's value either remains unchanged or changes to \\(u\\).\n</p>\n<hr class=\"new2\"/>\n<p>\nThe property can be written as follows. For all values \\(u\\),\niteration number \\(n\\) and majority \\(M\\) of acceptors:\n</p><p>\n\\(\n(\\forall a \\in M: a.v, a.t = u, n)\n\\)\n<br/>\n\\(\n\\Rightarrow\n\\)\n<br/>\n\\(\n\\forall a: (a.t &lt; n) \\vee (a.v = u)\n\\)\n  \n\n\n\n</p>", "4": "<h5 class=\"w3-text-teal\">Proof of the property</h5>\n\nLet \\(M\\) be a majority of acceptors, \\(u\\) be a value, and \\(n\\) be\nan iteration number where all acceptors in \\(M\\) have value \\(u\\) and\ntime \\(n\\).\n\nWe will prove that in all states an acceptor's time is less than \\(n\\)\nor its value is \\(u\\).\n\n<p>\nThe result holds after the \\(i\\)-th iteration for \\(i &lt; n\\) because \\(a.t &lt;\nn\\) in these iterations.\n\nThe result holds immediately after the \\(n\\)-th iteration, because\neither an acceptor \\(a\\)'s variables are not changed in the \\(n\\)-th\niteration in which case \\(a.t &lt; n\\) or it is modified in which case\n\\(a.v, a.t = u, n\\).\n\n</p><p>\nAssume that the result holds after the first \\(i \\geq n\\) iterations and\nprove that it holds for the \\(i+1\\)-th.\n\nWe need only consider the case where the read subset in the iteration\nis a majority, and \nin this case the read subset has an element in common\nwith majority \\(M\\).\n\nSo there is an element \\(a\\) in both the read subset and \\(M\\), and for\nthis element \\(a.t \\geq n\\), and therefore \\(a.v = u\\).\n</p><p>\n\n</p>", "5": "", "6": "<h5 class=\"w3-text-teal\">Properties of the Sequential Program</h5>\nThe proofs of the following observations are straightforward.\n\n<p>\nIn all states of the computation, for all acceptors \\(a\\),\n</p><ol>\n<li>\n<code>a.t</code> does not decrease.\n  </li>\n<li>\n<code>a.t</code> is the iteration number in which <code>a.v</code>\n  was last assigned a value.\n  </li>\n<li>\n  \\(a\\)'s value is None or is a value of some proposer.\n  <p>\n  \\(\n  \\forall a:  (a.v = None) \\vee (\\exists p: a.v = p.v)\n  \\)\n  </p></li>\n</ol>\n\n  The program has the following important property which we prove\nnext.\n<hr class=\"new2\"/>\n<p style=\"color:blue;\">\n  If the write subset is a\nmajority in any iteration, let \\(u\\) be the value assigned to acceptors in\n  the write subset in that iteration;\nthen,\n</p><p style=\"color:blue;\">\nthereafter an acceptor's value either remains unchanged or changes to \\(u\\).\n</p>\n<hr class=\"new2\"/>\n<p>\nThe property can be written as follows. For all values \\(u\\),\niteration number \\(n\\) and majority \\(M\\) of acceptors:\n</p><p>\n\\(\n(\\forall a \\in M: a.v, a.t = u, n)\n\\)\n<br/>\n\\(\n\\Rightarrow\n\\)\n<br/>\n\\(\n\\forall a: (a.t &lt; n) \\vee (a.v = u)\n\\)\n  \n\n\n\n</p>", "7": "<h5 class=\"w3-text-teal\">Proof of the property</h5>\n\nLet \\(M\\) be a majority of acceptors, \\(u\\) be a value, and \\(n\\) be\nan iteration number where all acceptors in \\(M\\) have value \\(u\\) and\ntime \\(n\\).\n\nWe will prove that in all states an acceptor's time is less than \\(n\\)\nor its value is \\(u\\).\n\n<p>\nThe result holds after the \\(i\\)-th iteration for \\(i &lt; n\\) because \\(a.t &lt;\nn\\) in these iterations.\n\nThe result holds immediately after the \\(n\\)-th iteration, because\neither an acceptor \\(a\\)'s variables are not changed in the \\(n\\)-th\niteration in which case \\(a.t &lt; n\\) or it is modified in which case\n\\(a.v, a.t = u, n\\).\n\n</p><p>\nAssume that the result holds after the first \\(i \\geq n\\) iterations and\nprove that it holds for the \\(i+1\\)-th.\n\nWe need only consider the case where the read subset in the iteration\nis a majority, and \nin this case the read subset has an element in common\nwith majority \\(M\\).\n\nSo there is an element \\(a\\) in both the read subset and \\(M\\), and for\nthis element \\(a.t \\geq n\\), and therefore \\(a.v = u\\).\n</p><p>\n\n</p><h5 class=\"w3-text-teal\">Majorities Accept Proposed Values</h5>\n\n\nNext, we give an example of a while loop of a\nsequential program which illustrates read and write subsets.\n\nWe later show how the while loop is converted to a distributed\nalgorithm for a system with lossy channels.\n\n<p>\nThe variables of the loop are sets <code>P</code> and <code>A</code>\ncalled the sets of proposers and acceptors, respectively.\n\nA proposer <code>p</code> has a field <code>p.v</code> called\n<code>p</code>'s value. \n\nAn acceptor <code>a</code> has fields <code>a.v, a.t</code> called\n<code>a</code>'s value and time, respectively.\n\nAn acceptor's value is the same type as a proposer's value, and an\nacceptor's time is an integer.\n\n\n</p><p>\nInitially <code>a.v, a.t = None, 0</code> for all  <code>a</code>.\nThe symbol <code>None</code> indicates unassigned.\nThe steps of the loop are as follows.\n</p><ol>\n<li>\n  Select any <code>p</code> from the set <code>P</code>\n  nondeterministically.\n  </li>\n<li>\n  Select a subset <code>read_subset</code> of <code>A</code>\n  nondeterministically.\n  </li>\n<li>\n  If <code>read_subset</code> is a majority of acceptors then\n  select a subset, <code>write_subset</code>, of <code>A</code>\n  nondeterministically, and assign values to all elements in <code>write_subset</code>.\n  </li>\n</ol>\n<pre>\n  t = 0\n  for a in A: a.v, a.t = None, 0\n      \n  while True:\n    t = t + 1\n    p = any_element(P)\n    read_subset = any_subset(A)\n\n    if len(read_subset) &gt;= len(A)/2:\n        write_subset = any_subset(A)\n        for a in write_subset:\n           a.v, a.t = f(p, read_subset), t\n</pre>\n\nNext we describe function <code>f</code>.\n\nIf the time for all acceptors in <code>read_subset</code> is 0 then\nthe function returns <code>p.v</code>, the proposer's value.\n\nOtherwise the function returns the value of the acceptor in\n<code>read_subset</code> with the largest time.\n\n<pre>\ndef f(p, read_subset):\n   v, t = p.v, 0\n   for a in read_subset:\n      if a.t &gt; t:   v, t = a.v, a.t\n   return v\n</pre>\n<h5 class=\"w3-text-teal\">Properties of the Sequential Program</h5>\nThe proofs of the following observations are straightforward.\n\n<p>\nIn all states of the computation, for all acceptors \\(a\\),\n</p><ol>\n<li>\n<code>a.t</code> is the iteration number in which <code>a.v</code>\n  was last assigned a value.\n  </li>\n<li>\n  \\(a\\)'s value is None or is a value of some proposer.\n  <p>\n  \\(\n  \\forall a:  (a.v = None) \\vee (\\exists p: a.v = p.v)\n  \\)\n  </p></li>\n</ol>\n\n  The program has the following important property which we prove\nlater.\n<p>\n  If on any iteration a value \\(u\\) is  assigned to acceptors in a\nwrite subset and the write subset is a majority,\n  then thereafter any acceptor's value remains\nunchanged or changes to \\(u\\).\n  </p><p>\n</p><p>\nThe property can be written as follows. For all values \\(u\\),\niteration number \\(n\\) and majority \\(M\\) of acceptors:\n</p><p>\n\\(\n(\\forall a \\in M: a.v, a.t = u, n)\n\\)\n<br/>\n\\(\n\\Rightarrow\n\\)\n<br/>\n\\(\n\\forall a: (a.t &lt; n) \\vee (a.v = u)\n\\)\n  \n\n\n\n</p><h5 class=\"w3-text-teal\">Proof of the property</h5>\n\nLet \\(M\\) be a majority of acceptors, \\(u\\) be a value, and \\(n\\) be\nan iteration number where all acceptors in \\(M\\) have value \\(u\\) and\ntime \\(n\\).\n\nWe will prove that in all states an acceptor's time is less than \\(n\\)\nor its value is \\(u\\).\n\n<p>\nThe result holds after the first \\(n-1\\) iterations because \\(a.t &lt;\nn\\) in these iterations.\n\nThe result holds after the \\(n\\)-th iteration, because if an\nacceptor \\(a\\)'s variables are modified in this iteration then\n\\(a.v, a.t = u, n\\).\n\n</p><p>\nAssume that the result holds after the first \\(i \\geq n\\) iterations and\nprove that it holds for the \\(i+1\\)-th.\n\nWe need only consider the case where the read subset in the iteration\nis a majority, and \nin this case the read subset has an element in common\nwith majority \\(M\\).\n\nSo there is an element \\(a\\) of read subset in which \\(a.t = n\\) and\n\\(a.v = n\\).\n</p><p>\n\n</p>", "8": "<h4 class=\"w3-text-teal\">Introduction of Acceptors and Timestamps</h4>\n\nThe algorithm uses agents called <i>acceptors</i> in addition to\nproposers and learners. \n\nEach acceptor <i>A</i> has two local variables, <i>A.v</i> and\n<i>A.t</i>, where <i>A.v</i> is either a special symbol null or a\nproposer's value, and <i>A.t</i> is a number which is initially\n\\(0\\).\n\\(A.v\\) and \\(A.t\\) are\ncalled \\(A\\)'s <i>value</i> and <i>timestamp</i>, respectively.\n\n<p>\nWe will see that an agent's timestamp does not decrease, and that\ntimestamps specify\n<a href=\"../ChannelSnapshots/TimeProperties.html\">\nrounds in a timeline.</a>\nWe shall say that acceptor \\(A\\) has accepted \\(A.v, A.t\\) if \\(A.v\\) is not null.\n\n\n</p>", "9": "<h3 class=\"w3-text-teal\">Basic Consensus</h3>\n  Before describing Paxos, we specify a subproblem, called\n  <i>basic consensus</i> that is solved in Paxos. Basic consensus is\nspecified in terms of proposers and acceptors without learners.\n\n", "10": "<h5 class=\"w3-text-teal\">Specification of Basic Consensus</h5>\n<ol>\n<li>\nAcceptors accept only proposer's values.\n<p>\n\\(\n(\\exists \\; \\textrm{proposer} \\; P: A.v = P.v) \\vee (A.v = null)\n\\)\n</p></li>\n<li>\nIf there exists there exists \\(vM, tM\\), and a point in the\ncomputation where all acceptors in a majority \\(S\\) accept \\(vM, tM\\)\n<p>\n\\(\n(\\forall C \\in S: C.v, C.t = vM, tM) \\wedge (vM \\neq null)\n\\)\n</p><p>\nthen at all points in the computation, every acceptor with timestamp\n\\(tM\\) or greater has value \\(vM\\)\n</p><p>\n\\(\n\\forall \\; \\textrm{acceptors} \\; A \\; \\textrm{where} \\; A.t \\geq tM: \\;\n  A.v = vM\n\\)\n\n</p></li>\n</ol>\n<p>\nBefore giving a distributed algorithm for basic consensus we analyze a\nfor-loop in a sequential algorithm. The sequential algorithm gives\ninsight into the distributed version.\n\n\n</p>", "11": "<h4 class=\"w3-text-teal\">A Sequential Simulation of the Basic\nConsensus Algorithm</h4>\n\nThe sequential program consists of a for-loop with loop index \\(k\\).\nInitially, \\(A.v, A.t = null, 0\\) for all acceptors \\(A\\).\n\n<p>\nThe loop body selects an arbitrary proposer <code>P</code>, and\narbitrary subsets <code>R, W</code> of acceptors; these selections simulate\nnondeterminism and faults in the distributed system.\n\nIn the loop, agents are treated as shared variables.\nFor example, the loop reads and writes <code>A.v, A.t</code> for an\nacceptor <code>A</code>.\n\n\n</p>", "12": "<h5 class=\"w3-text-teal\">Loop Body of the Basic Consensus\nAlgorithm</h5>\n\nThe loop body for the \\(k\\)-th iteration is as follows.\n\n<ol>\n<li>\n  Select an arbitrary proposer <code>P</code>, and arbitrary subsets\n  <code>R,W</code> of acceptors.\n  </li>\n<li>\n  If <code>R</code> is not a majority then take no action.\n  </li>\n<li>\n  If <code>R</code> is a majority then for all acceptors \\(A\\) in\n  <code>W:  A.v, A.t = f(R), k</code>\n</li>\n</ol>\n", "13": "<h5 class=\"w3-text-teal\">Function f</h5>\n<ol>\n<li>\nIf <code>A.v = null</code> for all <code>A</code> in <code>R</code>\n  then <code>f(R) = P.v</code>;\n  </li>\n<li>\n  else <code>f(R) = C.v</code>\n<br/>\n  where <code>C</code> is a member of\n  <code>R</code> with the largest  timestamp, i.e.,\n  <br/>\n<code>\n  C.t = max({r.t | r in R})\n  </code>\n</li>\n</ol>\n\n", "14": "<h3 style=\"color:red;\">Example of Sequential Simulation of Basic Consensus</h3>\nThis example has acceptors A0, A1, A2 and two proposers with values X\nand Y. Each row of the table shows the values of variables on the\n\\(k\\)-th iteration where \\(k = 0\\) represents initial conditions. The\nsymbol <i>-</i> is used instead of \\(null\\).\n\n<pre>\nk   P.v   R      W      A0         A1         A2\n0   -     -      -      (-, 0)     (-, 0)     (-, 0)\n1   X     0      0,1    (-, 0)     (-, 0)     (-, 0)\n2   X     0,1    1      (-, 0)     (X, 2)     (-, 0)\n3   Y     0,2    0      (Y, 3)     (X, 2)     (-, 0)\n4   Y     1,2    2      (Y, 3)     (X, 2)     (X, 4)\n5   X     0,1    0,2    (Y, 5)     (X, 2)     (Y, 5)\n6   X     1,2    1      (Y, 5)     (Y, 6)     (Y, 5)\n7   X     0,1    2      (Y, 5)     (Y, 6)     (Y, 7)\n8   X     1      1,2    (Y, 5)     (Y, 8)     (Y, 8)\n</pre>\n\nThe columns with headings \\(P.v, R, W\\) specify the proposer, and\nsubsets of acceptors that are read, \\(R\\), and written, \\(W\\). For\nexample, on iteration 1, \\(P.v = X\\), \\(R = \\{0\\}\\), \\(W = \\{0,1\\}\\). The\nvalues of the acceptors at the end of iteration \\(i\\) are shown in the row\nwhere \\(k = i\\).\n\n", "15": "<h5 style=\"color:red;\">Points to Note in the Example</h5>\nThe majority of \\(A.v\\) for acceptors \\(A\\) after iteration 4 is \\(X\\)\nbecause \\(A1, A2 = (X, 2), (X, 4)\\). The final majority, however, is\n\\(Y\\); see row 6.\n\n<p>\nIn row 4, \\(A1.t \\neq A2.t\\), and that's why the majority value does\nnot remain \\(X\\).\nIn row 5, the majority of acceptors have identical \\(A.v\\)\n<i>and</i> \\(A.t\\), and so, from row 5 onwards, the acceptor with the\nlargest timestamp has value \\(Y\\).\n\n\n\n</p>", "16": "<h3 class=\"w3-text-teal\">Sequential\nSimulator satisfies Specification of Basic Consensus</h3>\n\nThe proof of the invariance of part 1 -- acceptors accept only\nproposer's values -- is\nstraightforward. Next, we prove part 2.\n\n<p>\nConsider the case where at the end of the \\(k\\)-th iteration\nthere exists \\(vM \\neq null\\) such\nthat for all \\(C\\) in a majority \\(S\\) of acceptors:\n</p><p>\n\\(\nC.v, C.t = vM, k\n\\)\n</p><p>\nThen at the end of the \\(k\\)-th iteration conditions 1 and 2 hold:\n</p><p><i>Condition 1</i>:\nAcceptors have value\n\\(vM\\) or earlier timestamps:\n</p><p>\nfor all acceptors \\(A\\):\n\\(\n  ( A.v = vM) \\vee (A.t &lt; k)\n\\)\n</p><p><i>Condition 2</i>:\nValues of all acceptors in \\(S\\) are \\(vM\\), and their timestamps are\nat least \\(k\\).\n</p><p>\nfor all acceptors \\(C\\) in \\(S\\):\n\\(\n  (C.v = vM) \\wedge (C.t \\geq k)\n\\).\n\n</p><p>\nWe will show that if conditions 1 and 2 hold at the beginning of the \\(i\\)-th\niteration, for \\(i &gt; k\\), then they hold at the end of that iteration,\nand therefore continue to hold thereafter. We will prove that if both\nconditions hold and \\(R\\)\nis a majority then \\(f(R) = vM\\) and so both conditions continue to hold.\n\n</p><p>\nTwo majorities \\(R\\) and \\(S\\) have an element in common. Let\n\\(X\\) be common to \\(R\\) and \\(S\\).\n\nFrom condition 2,  \\(X.t \\geq k\\).\n\nTherefore, the element in \\(R\\) with the largest timestamp\n  has a timestamp of \\(k\\) or greater.\n\n</p><p>\nFrom condition 1, if \\(A.t \\geq k\\) then \\(A.v = vM\\).\n\nSo, the element in \\(R\\) with the largest timestamp has value\n\\(vM\\), and therefore \\(f(R) = vM\\).\n  \n\n\n\n\n\n</p>", "17": "<h3 class=\"w3-text-teal\">\nA Round of a Distributed Basic Consensus Algorithm\n</h3>\n\nNext, we describe a computation of a Paxos algorithm where the computation\nis a sequence of rounds, and each round is a distributed version of\nthe sequential simulation given earlier. The computation has\ntimelines for each of the proposers and acceptors. The reason for the\nnames of messages -- <code>prepare</code>, <code>promise</code>,\n<code>accept</code> -- will become clear later.\n\n", "18": "<h4 class=\"w3-text-teal\">Round \\(k\\) of the Computation</h4>\n<p>\nA round starts with the selection of a single proposer \\(P\\).\nLater, we describe how the proposer is selected.\nRound \\(k\\) has four steps which are as follows.\n\n</p><ol>\n<li>\n  \\(P\\) sends a message <code>prepare(k)</code> to all acceptors.\n  <p>\n</p></li>\n<li>\n  Each acceptor \\(A\\) that receives a <code>prepare(k)</code>\n  message from \\(P\\) sends a reply <code>promise(k, A.v, A.t)</code>\n  to \\(P\\).\n  <p>\n</p></li>\n<li>\n  If \\(P\\) receives <code>promise(k, _, _)</code> messages from a\n  majority \\(R\\) of acceptors then \\(P\\) sends <code>accept(k,\n  f(R))</code> messages to all acceptors. The symbol <code>-</code>\n  represents an arbitrary value.\n  <p>\n</p></li>\n<li>\n  If an acceptor \\(A\\) receives an <code>accept(k, v)</code>\n  message  then <code>A.v, A.t = v, k</code>.\n  <p>\n</p></li>\n</ol>\n", "19": "<h5 class=\"w3-text-teal\">Four Steps of Round \\(k\\) in Pseudo Code</h5>\n<pre>\n// Proposer P. Step P.1.\nsend prepare(k) to all acceptors\n\n// Acceptor A. Step A.1: \nupon arrival of prepare() from a proposer P:\n   send promise(k, A.v, A.t) to P\n\n// Proposer P. Step P.2.\nwait until\n   end of round or\n   arrival of promise(k, -, -) messages from any majority\n   R of acceptors;\n\nif promise messages are received from a majority R:\n   send accept(k, f(R)) to all acceptors\n\n// Acceptor A. Step A.2. \nupon arrival of accept(k, v): A.v, A.t = v, k\n</pre>\n\n", "20": "<h4 class=\"w3-text-teal\">Rounds in the Sequential Simulation and\nDistributed Computation are Equivalent</h4>\n\nNext we show that each round of the simulation is equivalent to each\nround in the timeline.\n\nTherefore a property that holds after a\nround in the simulation also holds in the corresponding round of the\ntimeline.\n\nSo, the round-based timeline satisfies the specification of basic consensus.\n\n<p>\nIn the distributed algorithm messages may get lost.\nSo proposer \\(P\\) receives\n<code>promise</code> from an arbitrary subset \\(R\\) of acceptors.\nLikewise, an arbitrary\nsubset \\(W\\) of acceptors receive <code>accept</code> messages.\n\n</p><p>\n  eading <code>A.v, A.t</code> from acceptors \\(A\\) in \\(R\\)\n  in the sequential simulation is equivalent to receiving \n<code>promise(k, A.v, A.t)</code> from acceptors \\(A\\) in \\(R\\) in the\n  distributed algorithm.\n\n\n  Similarly, assigning <code>v, k</code> to each acceptor \\(A\\) in\n  \\(W\\) in the sequential simulation\n  is equivalent to each acceptor \\(A\\) in \\(W\\) receiving <code>accept(k,\nv)</code> in the distributed algorithm.\n\n\n\n\n</p>", "21": "<h3 class=\"w3-text-teal\">The Distributed Basic Consensus Algorithm</h3>\nNext we described a distributed basic consensus algorithm.\nThe algorithm obeys the rules for assigning\n<a href=\"../ChannelSnapshots/TimeProperties.html\"></a>\nrounds to events.\nTherefore, the causality graph of a computation of the distributed\nalgorithm is the same\nas the causality graph of a computation that occurs as a sequence of\nrounds.\n\nA proof about states of agents in any computation\nis also a proof for all computations with the same causality graph.\nSee the\n<a href=\"../ChannelSnapshots/TimeProperties.html\">section:\n<i>A Technique for Analyzing Algorithms in Systems\nwith Message Loss</i>.\n</a>\n\n<p>\nA proposer \\(P\\) has a unique id \\(P.id\\) and a local integer variable\n\\(n\\). The pair \\([P.n, P.id]\\) is a proxy for the iteration index\n\\(k\\) of the sequential simulator, and the round id of the distributed\nalgorithm executing in rounds.\n\n</p><p>\nThe timestamp for a message is its first field.\n\nIn the algorithm given below this field has value \\(k\\).\n\nThe time of an acceptor \\(A\\) is \\(A.t\\), and the time of proposer\n\\(P\\) is \\(P.t\\).\n\n\n</p>", "22": "<h5 class=\"w3-text-teal\">The Basic Consensus Algorithm</h5>\n<pre>\n// Proposer P. Step P.1. \nP.n = P.n + any positive value\nP.t = [P.n, P.id]\n// P.t represents k in rounds of the timeline\nsend prepare(P.t) to all acceptors\n\n// Acceptor A. Step A.1: \nupon arrival of prepare(k) at A from a proposer P:\n   if k &gt; A.t:\n      A.t = k\n      send promise(k, A.v, A.t) to P\n\n// Proposer P. Step P.2.\nwait until timeout or:\n     arrival of promise(k, -, -) from any majority\n     R of acceptors where k = P.t\n\nIf timeout: return to step P.1\nelse: send accept(k, f(R)) to all acceptors\n      \n// Acceptor A. Step A.2. \nupon arrival of accept(k, v):\n    if k &gt; A.t: A.t, A.v = k, v\n</pre>\n", "23": "<h3 class=\"w3-text-teal\">The Existence of a Computation Executing in Rounds</h3>\nWe will prove that given any computation \\(C\\) of the Paxos algorithm\nthere exists a timeline, with the causality\ngraph of timelines the Paxos algorithm, in which events occur in rounds:\nAll events at agents when the agents' time equal \\(k\\) occur before\nall events at agents when the agents' time exceeds \\(k\\).\n\n\n", "24": "<h3 class=\"w3-text-teal\">Theorem</h3>\nGiven any computation \\(C\\) of an algorithm in which messages may be lost,\nwhere the algorithm follows the rules for assigning rounds, there\nexists a computation \\(C'\\):\n<ol>\n<li>\n  with the same causality graph as \\(C\\) and\n  </li>\n<li>\n  in which, for all \\(k\\), events with round id \\(k\\) occur before\n  events with round \n  ids that exceed \\(k\\), and \n  </li>\n<li>\n  messages sent in a round are received in the same round.\n  </li>\n</ol>\n\nThe proof follows from the rules for assigning round ids to events and\nmessages.\n\n<ol>\n<li>\n  A message sent in an event with round id \\(k\\) has\n  timestamp \\(k\\).\n  </li>\n<li>\n  A message with round id \\(k\\) is discarded if the message arrives\n  at the receiver when the receiver's round id exceeds \\(k\\).\n  </li>\n<li>\n  When a message with timestamp \\(k\\) arrives at an agent with\n  timestamp \\(k\\) or greater the message is either discarded or the\n  receiver's timestamp is set to \\(k\\).\n  </li>\n</ol>\n<figure>\n<img alt=\"Fig2\" src=\"raw_webcrawl_data/Paxos/PaxosFigures/PaxosFigures.012.jpeg\" style=\"width:100%\"/>\n<figcaption>Fig.1: Timelines</figcaption>\n</figure>\n<figure>\n<img alt=\"Fig4\" src=\"raw_webcrawl_data/Paxos/./PaxosProof/PaxosProof.008.jpeg\" style=\"width:80%\"/>\n<figcaption>Fig.2: Example of computation in rounds</figcaption>\n</figure>\n", "25": "<h4 class=\"w3-text-teal\">Learners</h4>\nThe algorithm for learners is simple:\nA learner determines that the consensus value is <code>V</code> if the\nlearner receives <code>accept(T, V)</code> messages from a majority of\nacceptors with identical values of <code>(T, V)</code>.\nWe ignore learners in the algorithm description, and restrict\nattention to whether and when a majority of\nacceptors sends <code>accept(T, V)</code> messages with identical\nvalues of <code>(T, V)</code>.\n\n\n\n\n\n    \n", "26": "<h2 class=\"w3-text-teal\">OLD OLD OLD</h2>\n\n\n", "27": "<h2 class=\"w3-text-teal\">The Problem</h2>\n\nIn each iteration of the sequential algorithm, given above, exactly\n  one writer reads and then writes all shared variables.\n\nWe will implement the following variant of the sequential algorithm\non a faulty distributed system.\n\nWe call the variant <i>Seq</i>.\n\n<p>\n<i>Seq</i> initializes variables and then executes a loop.\n\nA single writer <code>p</code> is selected nondeterministically in each\niteration.\n\nThe selected writer <code>p</code>\nreads and then writes subsets of shared variables.\n\nNo writer, other than <code>p</code>,\naccesses shared variables in this iteration.\n\n</p><p>\nThe writer <code>p</code>, selected on an iteration, determines a\nvalue <code>p.t</code>, called the <i>iteration id.</i>\n\nLater we give rules that ensure that the id of each iteration is unique.\n\n\n</p><p>\nThe manager <code>q</code> of a shared variable has a field\n<code>q.var</code> where <code>q.var</code> is a tuple <code>(v,\nt)</code> where\n<code>v</code> is the value of the shared variable and \n<code>t</code> is the id of the most recent iteration in which\n<code>v</code> was assigned a value.\n\n</p><p>\nEach writer <code>p</code> has a local variable <code>p.copy</code>, a\ndictionary, where <code>p.copy[q]</code> contains a (possibly stale)\ncopy of <code>q.var</code>.\n\nEach writer <code>p</code> also has a local variable <code>p.t</code>\nwhich is the id of the most recent iteration in which <code>p</code>\nwas selected as the writer that accesses shared variables.\n\n</p><p>\nWe describe the steps of the sequential program later.\n\n\n\n\n</p>", "28": "<h4 class=\"w3-text-teal\">Sequential Program Seq</h4>\n\n<pre>\nfor p in P: p.t = 0\nfor q in Q: q.var = (None, 0)\n\nwhile True:\n   select a writer p\n   p.t = p.t + pos()\n\n   p.copy = {}\n   for q in read_set: p.copy[q] = q.var\n\n   if g(p):\n      for q in write_set: q.var = (f(p, q), p.t)\n</pre>\n<code>None</code> is a value that indicates unassigned.\n\n<code>pos</code> is a function that returns a positive value.\n\nThe value returned by <code>pos</code> impacts the performance, but not\nthe correctness of the algorithm.\n\n\n<p>\n<code>read_set</code> and <code>write_set</code> are subsets of\nmanagers of shared variables.\n\nThese subsets are selected nondeterministically.\n\n\n</p><p>\nThe iteration id determined by a selected writer <code>p</code> is a\npair <code>(n,p.id)</code> where \n<code>n</code> is a number, and \n<code>p.id</code> is the id of <code>p</code>.\n\nSo, different writers cannot determine the same iteration id.\n\nWhen <code>p</code> starts an iteration it increases <code>p.t</code>.\n\nSo, different iterations in which <code>p</code> is the selected\nwriter have different ids.\n\n</p><p>\n(We don't include <code>p.id</code> explicitly in the algorithm\ndescription so that we can focus on more important aspects.)\n\n\n</p><p>\n<code>p</code> reads shared variables in <code>read_set</code>, and\n<code>p</code> writes shared variables in <code>write_set</code> only \nif condition <code>g(p)</code> holds.\n\nFor example, an algorithm may write shared variables only if \n<code>read_set</code> is not empty.\n\n</p><p>\nIn the statement that writes shared variables, <code>f(p,q)</code> is\nassigned to the value field, \nand <code>p.t</code> is assigned to the iteration id field of\n<code>q.var</code>.\n\n</p><p>\nThe nondeterministic statements of Seq \nare the statements that select <code>p</code>,\n<code>read_set</code> and <code>write_set</code> in each iteration;\ndifferent selections of these variables result in different executions\nof Seq.\nAn instance of Seq is a deterministic algorithm with a given selection of these\nvariables.\n\n\n\n</p>", "29": "<h3 class=\"w3-text-teal\">Problem Specification</h3>\n\n\nDesign a distributed algorithm in which agents are writers, managers of shared\nvariables and observers, and in which an observer receives\na message <code>m</code> \nfrom a manager <code>q</code> of a shared variable only if\n<code>q.var = m</code> at some point in the execution of an instance\nof Seq.\n\n\n<p>\nObservers cannot distinguish between messages that they receive in the\ndistributed algorithm and values of shared variables in Seq.\n\n</p><p>\nIn the observer pattern, agents sends descriptions of events to\nobservers which construct the dataflow of a computation.\n\nIn our algorithm, copies of shared variables are sent to observers.\nx\nMessages may, however, be lost, duplicated and delivered out of order.\n\nWhat can we deduce from the messages that observers receive?\n\n</p><p>\nIf an observer receives <code>(v, t)</code> and <code>(v', t)</code>\nfrom <code>q</code> and <code>q'</code>, respectively, \nthen we know that <code>q.var</code> and <code>q'.var</code> were\nassigned values <code>(v, t)</code> and <code>(v', t)</code>,\nrespectively, in the same iteration -- the iteration with unique id\n<code>t</code>. \n\n\n\n</p>", "30": "<h3 class=\"w3-text-teal\">Performance Issues</h3>\n\n<p>\nThe events in the dataflow of an iteration are as follows.\n\n</p><ol>\n<li>\n  A single writer <code>p</code> is selected.\n  </li>\n<li>\n  The epoch time <code>p.t</code> is increased and the resulting epoch\n  time uniquely identifies the iteration -- no other iteration has the\n  same epoch time.\n  </li>\n<li>\n<code>p</code> sends read requests to shared variables.\n  </li>\n<li>\n  A read request may get lost, or the manager may ignore the request,\n  or the manager may send a reply.\n  </li>\n<li>\n  A reply may get lost, or <code>p</code> may ignore the reply, or\n  <code>p</code> may copy the reply into a local variable and send\n  write requests if <code>g(p)</code> holds.\n  </li>\n<li>\n  A write request may get lost, or a manger may ignore the request, or\n  the manager may assign the value specified in the request to the\n  shared variable.\n  </li>\n</ol>\n<p>\nIgnoring a message has the same effect as losing the message -- the\nmessage is removed from the channel without changing agent states.\n\n</p><p>\n<code>read_set</code> is the set of managers that send replies that\n<code>p</code> copies in step 5.\n<code>write_set</code> is the set of managers that assign\nvalues to shared variables specified in write requests in step 6.\n\n\n\n</p><p>\n<i>The Problem</i>:\n\nDetermine how to select a single writer <code>p</code> for each\niteration and which requests and replies to ignore and which to process so that:\n</p><hr class=\"new2\"/>\n<p style=\"color:blue;\">\nValues assigned to shared variables in the distributed\nand sequential algorithms are identical.\n</p>\n<hr class=\"new2\"/>\n"}, "raw_webcrawl_data/Paxos/ReadWriteLossyChannels.html": {"0": "<h2 class=\"w3-text-teal\">Serializable Transactions in Faulty Systems</h2>\n\n\n<p class=\"w3-text-red\">\n  In this webpage we develop algorithms for systems in which agents\n  may halt or be arbitrarily slow; the same message may be\n  delivered multiple times and delivered out of order; and, messages\n  may be lost.\n\n  A transaction consists of two steps: a client reads and the writes\n  shared variables.\n\n  This page shows how a proxy for time can be used to develop\n  algorithms in which computations can be serialized -- for every\n  computation \\(x\\)  there exists a computation \\(y\\)\n  with the same steps as \\(x\\) and in which all steps of a transaction\n  appear consecutively.\n\n \n\n  \n  \n  \n</p>", "1": "<h3 class=\"w3-text-teal\">Overview</h3>\n\n  In this page we develop algorithms for systems in which messages may\n  be lost; multiple \n  copies of a message may be delivered; messages may not be\n  delivered in the order sent; agents may be arbitrarily slow; and\n  agents may stop.\n\n  <p>\n  A system consists of a set of agents called <i>clients</i> and a\n  set of agents called <i>servers</i>.\n\n  There is a channel from each server to each client, and from each\n  client to each server.\n\n  There are no other channels.\n  \n  </p><p>\n  Clients execute transactions in which they send requests to servers\n  to read and then write variables managed by servers.\n\n  We describe algorithms in which computations are serializable:\n  For every computation \\(x\\) there exists a computation \\(y\\)\n  with the same steps as \\(x\\) and where\n  steps of each transaction appear consecutively in \\(y\\). \n\n\n\n\n  \n\n</p>", "2": "<h4 class=\"w3-text-teal\">Servers</h4>\n\n  \nEach server <code>q</code> has a local variable <code>q.v</code>.\n\nClients send requests to <code>q</code> to read or write\n<code>q.v</code>.\n\nThe only actions of a server are to respond to requests from clients.\n\n\n<p>\nA server <code>q</code> replies to a read request from a client\n<code>p</code> by sending <code>p</code> a copy of <code>q.v</code>.\n\nA write request includes the value <code>v</code> to be\nwritten.\n\nWhen a server <code>q</code> receives a write request containing value\n<code>v</code>, the server assigns <code>v</code> to <code>q.v</code>.\n\n\n\n\n</p>", "3": "<h4 class=\"w3-text-teal\">Clients</h4>\n\n\nEach client receives a sequence of <i>clock tick</i> messages.\n\nThe intervals between successive clock ticks are irrelevant for\nthe correctness of the algorithm.\n\nThe intervals do, however, impact performance.\n\nThere are many ways of generating sequences of messages and we\npostpone discussion of them.\n\n<p>\nA read request and reply may get lost or be delayed for an arbitrary\ntime. \n\nA client avoids waiting forever for a reply from a server by only\naccepting replies that the client receives before it receives its next\nclock tick message.\n\nReplies that arrive after the next clock tick are treated as lost.\n\n\n\n</p>", "4": "<h5 class=\"w3-text-teal\">Read Step of a Transaction</h5>\n\n\n\nA client <code>p</code> sends read requests to all servers.\n\nSome requests may be lost.\n\nA server that receives a read request from <code>p</code> sends a\nreply to <code>p</code>.\n\nSome replies may be lost.\n\nLet <code>R</code> be the set of servers from which  <code>p</code>\nreceives replies before  <code>p</code> receives its next clock tick.\n\n\n\n<p>\n\nIf <code>R</code> has <code>M</code> or more elements, where\n<code>M</code> is a given constant, then  <code>p</code> proceeds to the\nwrite step.\n\nIf <code>R</code> has fewer than <code>M</code> elements, then the\ntransaction terminates without executing the write step.\n\n\n\n</p>", "5": "<h5 class=\"w3-text-teal\">Write Step of a Transaction</h5>\n\n<p>\nThe list of replies that <code>p</code> gets before <code>p</code>\nreceives its next clock tick is:\n<code>\n[q.v for q in R]\n</code>.\nThe value that a client requests servers to write in a transaction\ndepends only on the client's local variables and the values it\nreceives from servers in the read step of the transaction.\n\nClient <code>p</code> \nsends a request to all servers to write:\n<code>\np.f(q.v for q in R)\n</code>\nwhere <code>p.f</code> is a function of <code>p</code>.\n\n\n</p><p>\nWrite requests may get lost.\n\nLet <code>W</code> be the set of servers that execute write requests.\n\nA transaction is specified by the following statement:\n\n</p><hr class=\"new2\"/>\n", "6": "<h5 style=\"color:blue;\">Transaction</h5>\n<pre>\nif len(R) &gt;= M:\n   for q in W: q.v = p.f(q.v for q in R)\n</pre>\n<hr class=\"new2\"/>\n\n\nTransactions executed concurrently by multiple clients can interfere\nwith each other as illustrated by the following example.\n\n\n", "7": "<h4 style=\"color:red;\">Example</h4>\n\n\n  Let <code>x</code> be the amount of funds in an account and \n  assume that <code>x</code> has $110.\n  \n  Consider a computation in which clients <code>p</code> and\n<code>p'</code> both execute identical transactions concurrently.\n\nIn a transaction a client reads <code>x</code> and if\n  <code>x</code> has at least $100 then the client transfers $100 out\nof  <code>x</code> to an account <code>y</code>.\n\n<p>\nConsider a computation in which both clients\nread <code>x</code> and verify that <code>x</code> has at least $100,\nand then both clients transfer $100 out of <code>x</code> and set the amount\n  in <code>x</code> to $10.\n\nThe computation transfers $200 out of <code>x</code> but \ndebits <code>x</code> by $100.\n\n\n</p><p>\nThis situation does not occur if only one client executes a\ntransaction at a time.\n\nIf <code>p</code> executes its transaction first then when\n<code>p</code> completes the transaction it sets the amount in <code>x</code> to $10. \nIf <code>p'</code> executes its transaction next then <code>p'</code>\nfinds that <code>x</code> has only $10 and so <code>p'</code> does not\ntransfer $100 from <code>x</code>.\n\n\n\n\n\n\n</p>", "8": "<h3 class=\"w3-text-teal\">Serializable Computations</h3>\n\n\n  A computation \\(x\\) is \n  <a href=\"https://en.wikipedia.org/wiki/Database_transaction\">\n  serializable</a> exactly when there exists a\n  computation \\(y\\) which\n  is a permutation of \\(x\\)  and in which: <i>All steps\n  of each transaction appear consecutively</i>. \n\n<p>\nComputation \\(x\\) may\nhave steps from a transaction \\(u\\) followed by steps from a different\ntransaction \\(v\\) followed by more steps from \\(u\\).\n\nBy contrast, in \\(y\\) either (1) all steps of transaction \\(u\\) appear\nbefore all steps of \ntransaction \\(v\\) or (2) all steps of transaction \\(u\\) appear after all\nsteps of \ntransaction \\(v\\).\n\n\n\n\n</p>", "9": "<h4 class=\"w3-text-teal\">Problem: Design Algorithms with\nSerializable Computations</h4>\n\nThe problem is to develop a distributed algorithm, with multiple\nclients and servers, in which computations are serializable, and in\nwhich the system may be faulty.\n\n\n", "10": "<h5 class=\"w3-text-teal\">How Should You Solve The Problem?</h5>\n\n\nWhat method comes to mind to partition a computation into sequences\nwhere each sequence has all the steps of exactly one transaction?\n\n<p>\nLogical time partitions computations into a past and a future.\n\nLet's use a mechanism, similar to logical time,\nthat partition steps into past, <i>current</i>, and future, \nwhere <i>current consists of all the steps of exactly one\ntransaction.</i> \n\nWe call the value assigned to a step the <i>epoch</i> of the\nstep in analogy to the logical time of the step.\n\n\n\n</p>", "11": "<h3 class=\"w3-text-teal\">Epochs and Logical Times</h3>\n\n<p>\nThe logical time of a step \\(e\\) is a value \\(t(e)\\) assigned to\neach step \\(e\\) in a computation such \nthat for all edges \\((e, e')\\) of the dataflow graph:\n\\(t(e) &lt; t(e')\\).\n\n</p><p>\nThe <i>epoch</i> of a step \\(e\\) is a value \\(t(e)\\) assigned to\neach step \\(e\\) in a computation such \nthat for all edges \\((e, e')\\) of the dataflow graph:\n\\(t(e) \\leq t(e')\\).\n\n</p><p>\nEpochs are implemented using rules similar to rules\nfor logical time:\n(1)\n  Later steps at an agent have the same or higher epochs than earlier\n  steps at the agent.\n(2)\n  A message is received in a step with the same or higher epoch\n  time than the step in which the message is sent.\n\n\n</p><p>\nEpochs have many of the properties of logical time\nincluding the following:\n\n\n\n</p>", "12": "<h3 class=\"w3-text-teal\">Theorem</h3>\nGiven any assignment of epochs to steps:\n<p>\n<i>There exists a computation in which steps are executed in\nascending order of epochs</i>.\n\n</p><p class=\"w3-text-teal\">Proof</p>\nThe proof of this observation is the same as that for\n<a href=\"../ChannelSnapshots/LogicalClocks.html\">logical times:</a>\nThere exists a topological sort of a dataflow graph where vertices in\nthe sort occur in ascending order of epoch.\n\n<p>\nA consequence of this observation is the following sufficient\ncondition for serializability.\n\n\n</p><hr class=\"new2\"/>\n<p style=\"color:blue;\">\nA sufficient conditions for serializability is:\n\n</p><p style=\"color:blue;\">\n(1) each transaction has a unique epoch, and (2) all steps in a\ntransaction have the epoch of the transaction.\n</p>\n<hr class=\"new2\"/>\n\nNext, we describe an algorithm based on this idea.\n\nLet's consider the two issues posed by the condition for serializability.\n\n(1) How can the algorithm assign a unique epoch to each transaction?\n\n(2) How can the algorithm assign epochs to steps so that\nall steps in a transaction have the epoch of the transaction?\n\n\n\n\n", "13": "<h5 class=\"w3-text-teal\">Uniqueness of a Transaction's Epoch</h5>\n\n\n\nA client <code>p</code> initiates a new transaction when\n<code>p</code> gets a clock tick message.\n\nTo ensure that the epoch of the transaction is unique, an  epoch\n<code>t</code> is a pair <code>(n, p_id)</code> where <code>n</code> is a\nnumber and <code>p_id</code> is the id of client <code>p</code>.\n\nTransactions initiated by different clients have different epochs\nbecause their client ids are different.\n\nA client sets the epoch of a new transaction that it initiates to be\ngreater than epochs of all previous transactions that it initiated.\n\nSo, different transactions initiated by the same client have different\nepochs.\n\nTherefore each epoch is unique.\n\n<p>\nClient ids are totally ordered, and so epochs are also totally\nordered.\n\nFor brevity we refer to an epoch by a single value <code>t</code> rather than a\npair <code>(n, p_id)</code>.\n\n\n\n\n</p>", "14": "<h5 class=\"w3-text-teal\">All Steps in a Transaction have the\nTransaction's Epoch</h5>\n\n\nWe associate a field <code>t</code> with each agent -- client or\nserver -- where\n<code>t</code> is the epoch of the transaction that the agent is\nexecuting.\n\nLikewise, we associate a field <code>t</code>\nwith each message -- request or reply -- between clients and servers\nwhere \n<code>t</code> is the epoch of the transaction in which the message is\nsent.\n\nWe refer to these fields as the agent's or client's epochs.\n\n\n<p>\nNext we consider steps executed by clients and servers in a\ntransaction with an epoch \\(T\\). \n\n\n\n</p>", "15": "<h6 class=\"w3-text-teal\">All Steps Executed by A Client in a\nTransaction have the Transaction's Epoch</h6>\n<p class=\"w3-text-teal\">Client <code>p</code> initiates a new\ntransaction</p>\n\nWhen a client <code>p</code> gets a clock tick message it executes\n<pre>\np.t = p.t + pos()\n</pre>\nwhere <code>pos</code> returns a positive value.\n\nThen <code>p</code>\ninitiates a new transaction with epoch <code>p.t</code>.\n\n<code>p.t</code> remains unchanged until <code>p</code> gets its next\nclock tick message.\n\nNext, consider steps executed by the transaction with epoch <code>T</code>\nwhere <code>T</code> is the value that <code>p.t</code> has until\n<code>p</code> gets its next clock tick message.\n\nAll steps executed by <code>p</code> while <code>p.t</code> \\(=\\)\n<code>T</code> belong to the transaction with epoch <code>T</code>. \n\n\n\n<p class=\"w3-text-teal\">Client <code>p</code> sends requests </p>\nEvery message <code>m</code> sent between clients and servers has a field\n<code>m.t</code> called <code>m</code>'s epoch.\n\nWhen a client <code>p</code> sends a request <code>m</code> it sets \n<code>m.t</code> to <code>p.t</code>.\n\nSo, all requests sent by a client <code>p</code> while <code>p</code>\nis executing steps of the transaction with epoch <code>T</code> also\nhave epoch <code>T</code>.\n\n\n<p class=\"w3-text-teal\">Client <code>p</code> receives a reply </p>\nA server sends a reply  <code>r</code> with epoch  <code>r.t</code>,\nwhere <code>r.t</code> \\(=\\) <code>T</code>,\nwhen the server is executing the transaction with epoch\n<code>T</code>. \n\nA client <code>p</code> which is executing the transaction with\nepoch <code>T</code> takes no action when it receives a reply sent in\na different transaction, i.e., <code>p</code> discards a reply\n<code>r</code>, without \ntaking any action, when <code>r.t</code> \\(\\neq\\)\n<code>p.t</code>.\n\n<p>\n\nWhen a client <code>p</code> receives a reply <code>r</code> it\ntakes a next step if and only if <code>r.t</code> \\(=\\)\n<code>p.t</code>.\n\nSo, a client <code>p</code> executing a transaction with epoch\n<code>T</code> only accepts replies sent in the transaction with epoch\n<code>T</code>.\n\n\nTherefore, while a client <code>p</code> is\nexecuting a transaction with epoch <code>T</code>:\nall steps executed by the client, all requests it sends, and all\nreplies it accepts, also have epoch <code>T</code>.\n\n\n\n\n</p>", "16": "<h5 class=\"w3-text-teal\">All Steps Executed by A Server in a\nTransaction have the Transaction's Epoch</h5>\n\nLet's look at three cases: What actions should a server take when it\nreceives a request from (1) an earlier epoch, (2) the same epoch, and\n(3) a later epoch?\n\n\n", "17": "<h6 class=\"w3-text-teal\">Case 1: Server receives a request from an earlier epoch</h6>\nA server <code>q</code> takes no action when it receives a request\n<code>m</code> from an earlier epoch, i.e. when\n<p>\n<code>m.t</code> \\(&lt;\\) <code>q.t</code>.\n</p><p>\nSuch requests are treated as lost.\n\n\n</p>", "18": "<h6 class=\"w3-text-teal\">Case 2: Server receives a request in the same epoch</h6>\nA server takes the action specified in a request that has the same epoch\nas the server's epoch.\n<p>\nWhen a server <code>q</code> receives a read request\n<code>m</code> where <code>m.t</code> \\(=\\) <code>q.t</code>, the\nserver replies with a message <code>r</code> with two fields\n<code>r.v</code> and <code>r.t</code> where\n<code>r.v = q.v</code> \nand <code>r.t = q.t</code>.\n\nSo, the request, the step in which the request is processed, and the\nreply, all have the same epoch.\n\n</p><p>\nA write request <code>m</code> has two fields: <code>m.v</code> the\nvalue to be written, and <code>m.t</code> the epoch of the message.\nWhen a server <code>q</code> receives a write request\n<code>m</code> where <code>m.t</code> \\(=\\) <code>q.t</code>, the\nserver writes the specified value, i.e. executes <code>q.v =\nm.v</code>.\n\nSo, the write request and the step in which the request is processed\nhave the same epoch.\n\n\n\n</p>", "19": "<h6 class=\"w3-text-teal\">Case 3: Server receives a request from a later\nepoch</h6>\n\nWhat should server <code>q</code> do when it receives a request\n<code>m</code> where <code>m.t</code> \\(&gt;\\) <code>q.t</code>?\n\n<p>\nCan the server ignore the request?\n\nNo, the server cannot ignore the request because\nif it does then the server's epoch never\nchanges.\n\n\n</p><p>\nSo, in this case, a server <code>q</code> increases its epoch to that\nof the request <code>m</code> -- i.e. assigns <code>q.t = m.t</code>\n--  and responds to the request as described in Case 2.\n\n</p><p>\nNext, we give the algorithm based on this discussion.\n\n\n\n</p>", "20": "<h3 class=\"w3-text-teal\">The Algorithm</h3>\n\n\n", "21": "<h5 class=\"w3-text-teal\">Variables</h5>\n\nA server <code>q</code> has local variables <code>q.v</code> and\n<code>q.t</code> where <code>q.v</code> is the value of the server,\nand <code>q.t</code> is the server's epoch. \n\n<p>\nA client <code>p</code> has a local variable <code>p.copy</code> which\ncontains copies of servers' variables.\n<code>p.copy</code> is a dictionary and <code>p.copy[q]</code> is\n<code>p</code>'s (possibly stale) copy of <code>q.v</code>.\n\nA client <code>p</code> also has a local variable  <code>p.t</code>\nwhich is the client's epoch.\n\n\n\n\n\n</p>", "22": "<h5 class=\"w3-text-teal\">Messages</h5>\n\n<p>\nA server receives two classes of messages from clients: <code>ReadRequest</code> and\n<code>WriteRequest</code>.\n\nA <code>ReadRequest</code> message has a single field <code>t</code>\nwhich is the epoch of the message.\n\nA <code>WriteRequest</code> message has fields <code>v, t</code>\nwhere <code>v</code> is the value to be written and <code>t</code> is\nthe epoch of the message. \n\n\n</p><p>\nA client receives two classes of messages: <code>ClockTick</code> and\n<code>Reply</code>.\n\nA <code>ClockTick</code> message has no fields.\n\nA <code>Reply</code> message has fields <code>v, t</code>\nwhere <code>v</code> is the value that was read by the server\nand <code>t</code> is the epoch of the message. \n\n\n\n\n</p>", "23": "<h5 class=\"w3-text-teal\">Algorithm for a Server</h5>\n\n<pre>\n# initialization\nq.v, q.t = init, 0\n\nstart()\ndef receive(request, client):\n   if request.t &gt;= q.t\n      q.t = request.t\n      if isinstance(request, ReadRequest):\n         send(Reply(q.v, q.t), client)\n      else:\n         // message is a WriteRequest\n         q.v = request.v\n</pre>\n\nNext we give the algorithm for a client and explain statements in the\nalgorithm later.\n\n\n\n", "24": "<h4 class=\"w3-text-teal\">Algorithm for a Client </h4>\n\n<pre>\n# Initialization\np.t = 0\n\nstart()\ndef receive(message, sender):\n   if isinstance(message, ClockTick):\n      p.t = p.t + pos()\n      # Start new transaction with epoch p.t\n      p.copy = {}\n      for q in Q:  send(ReadRequest(p.t), q)\n\n   else:\n      # message is a reply to a read request\n      if message.t == p.t: \n         p.copy[sender] = message.v\n         # send writes if at least M replies received\n         if len(p.copy) &gt;= M:\n            v, t = p.f(), p.t\n            for q in Q: send(WriteRequest(v, t), q)\n</pre>\n\n", "25": "<h5 class=\"w3-text-teal\">Explanation of the Algorithm</h5>\n\n<p class=\"w3-text-teal\">When a Client Receives a Clock Tick\nMessage</p>\n\nA client <code>p</code> increases its epoch by some positive value\n<code>pos</code>  and starts a new transaction with epoch\n<code>p.t</code>. \n\n  Client <code>p</code> discards earlier copies of server variables by\nsetting <code>p.copy</code> to empty.\nThe client then sends read requests to each server.\nThe client then waits to receive a clock tick \nmessage and waits concurrently for replies to its read requests.\n\n\n<p class=\"w3-text-teal\">When a Client Receives a Reply</p>\n\n\nA client <code>p</code> accepts a <code>reply</code> if \nthe reply is in the same epoch\nas that of the transaction that <code>p</code> is processing.\ni.e., <code>p.t</code> \\(=\\) <code>reply.t</code>.\n\n\nClient <code>p</code> copies the value in the reply from\n<code>q</code> into the client's \nlocal variable, <code>p.copy[q]</code>.\n\nIf <code>p</code> has received replies from <code>M</code> or more \nservers then <code>p</code> sends write requests to all servers.\n\nA write request contains the value <code>p.f()</code> to be written\nand the epoch <code>p.t</code> of the request.\n\n\n\n\n", "26": "<h3 class=\"w3-text-teal\">Proof of Correctness</h3>\n\n<p>\nLet \\(x\\) be a computation of the distributed algorithm.\n\nLet the epochs of transactions in \\(x\\), in increasing order of epochs\nbe <code>T[0]</code> \\(&lt;\\) <code>T[1]</code> \\(&lt;\\) <code>T[0]</code>\n\\(&lt; \\ldots \\) <code>T[K]</code> where <code>K</code> is the number of\ntransactions in \\(x\\).\n\n</p><p>\nFrom the arguments given earlier, there exists a computation \\(y\\)\nwhere \\(y\\) is a permutation of \\(x\\) and where steps of the\ntransaction with epoch <code>T[i]</code> appear in \\(y\\) after all steps of\ntransactions with epoch  <code>T[i-1]</code>, all <code>i</code>.\n\n\n\n</p>", "27": "<h4 class=\"w3-text-teal\">Equivalence of Sequential Iterations</h4>\n\nLet <code>p[i]</code> be the client that initiates the\n<code>i</code>-th transaction, <code>R[i]</code> be the set of servers\nfrom which <code>p[i]</code> receives replies, and <code>W[i]</code>\nthe set of servers that receive write requests from <code>p[i]</code>\nin the transaction.\n\nThe sequence of values of server variables in computation\n\\(y\\) are the same as that in the following iterations:\n\n<pre>\nfor k = 0 to K:\n   if len(R[i]) &gt; M:\n      for q in W[i]: q.v = p.f(q.v for q in R[i])\n</pre>\n\n"}, "raw_webcrawl_data/Knowledge/Knowledge.html": {"0": "<h1 class=\"w3-text-teal\">What Agents Know</h1>\n", "1": "<h4 class=\"w3-text-teal\">\nThis module gives a formal definition for the frequently used informal\nphrase: \"An agent <i>knows</i> something about other agents and and\nchannels.\" \n</h4>\n", "2": "<h1 class=\"w3-text-teal\">Key Ideas</h1>\n    We sometimes use anthropmorphic arguments in reasoning about\n    systems --- we invest digital\n    agents with human characteristics. For example, a programmer may\n    say \"an agent <i>knows</i> that another agent is idle.\" Endowing software with\n    human capabilities can be dangerous when terms are ambiguous.\n    <p>\n    In this module we define a predicate \"agent \\(x\\)\n    knows \\(P\\)\" where \\(P\\) is a predicate on states of a system.\n    In later modules, we will use this definition to discuss\n    algorithms.\n\n    </p><p>\n    This module presents theorems about what agents know. The proofs\n    of these theorems follow from the definition of\n    <a href=\"../ChannelSnapshots/ChannelSnapshots.html\">consistent\n    cuts </a>.\n\n    </p>", "3": "<h2 class=\"w3-text-teal\">What an Agent Knows</h2>\n    Let \\(x\\) be an agent, \\(P\\) a predicate on system states, and\n    \\(Q\\) a predicate on states of \\(x\\).\n    <p>\n    \\(P\\) can be a <i>global predicate</i>, i.e., a predicate on\n    states of all agents and channels. \\(Q\\) is a <i>local\n    predicate</i> of \\(x\\) because it is a predicate only on the states of\n    agent \\(x\\) and is independent of other agents and only channels.\n\n</p><p>\nLet \\(init\\) be the predicate that defines the initial condition of\nthe system. \n\n    </p><p>\n    We\n    define the predicate \"\\(x\\) <i>knows</i> \\(P\\)\" as follows.\n    </p><hr class=\"new2\"/>\n", "4": "<h5 style=\"color:blue;\">\n</h5><p>\n    \\(x\\) knows \\(P\\) is the weakest local predicate \\(Q\\) of\n    \\(x\\) such that:\n    </p><p>\n    \\(\n    [init \\; \\Rightarrow \\; always([Q \\;\n    \\Rightarrow \\; P])]\n    \\)\n    </p><p>\n</p><hr class=\"new2\"/>\n<p>\n    \\(x\\) knows \\(P\\) holds in a local state \\(s_{x}\\) of agent \\(x\\)\nexactly when \\(P\\) holds in all states of all trajectories\n    (that start from an initial state) when the\nlocal state of agent \\(x\\) is \\(s_{x}\\).\n\n</p>", "5": "<h4 class=\"w3-text-teal\">Explanation</h4>\nFor any predicate \\(R\\):\n<p>\n\\(\n[init  \\; \\Rightarrow \\; always(R)]\n\\)\n</p><p>\nmeans that \\(R\\) holds in every state of every trajectory that starts\nin an initial state. So, \n</p><p>\n\\(\n[init  \\; \\Rightarrow \\; always([Q \\Rightarrow P])]\n\\)\n</p><p>\nmeans that in \\([Q \\Rightarrow P]\\) holds in every state of every\ntrajectory that starts in an initial state.\nSo, if local predicate \\(Q\\) of agent \\(x\\) holds in any\nstate of any trajectory that starts from an initial state then global predicate \\(P\\)\nalso holds in that state.\n\n\n</p><p>\n</p>", "6": "<h3 style=\"color:red;\">Example</h3>\n    A system consisting of agents \\(0, \\ldots,  N\\) has two indivisible\n    tokens which are not created or destroyed. \n\n<p>\n\"Agent \\(0\\) knows no other agent holds a token\" is a predicate on the states of agent\n\\(0\\); this predicate holds for a local state \\(s_{0}\\) of agent \\(0\\)\nif and only if no other agent holds a token when agent \\(0\\) is in\nstate \\(s_{0}\\).\n</p><p>\nSo, \"agent \\(0\\) knows P\" holds exactly when agent \\(0\\)\nholds both tokens. \n\n</p><p style=\"color:red;\">Example</p>\nThis example deals with a system consisting of two agents \\(x\\) and\n\\(y\\) and channels in both directions between the agents. The system\nhas a single indivisible token that is not created or destroyed.\nLet\n\\(P\\) be the predicate: \"\\(y\\) does <i>not</i> hold the token.\"\nAgent \\(x\\) knows \\(P\\) when \\(x\\) holds the token. \n<p>\n\\(\\neg\\) (\\(x\\) knows \\(P\\)) is a predicate too, and sometimes\nprogrammers refer to this predicate as \"\\(x\\) does not know \\(P\\).\"\n\n</p><p style=\"color:red;\">Example</p>\nIn the previous example, let \\(Q\\) be the predicate: \"\\(y\\) holds the token.\"\nIn what local states of agent \\(x\\) does \\(x\\) know that \"\\(y\\) holds the token?\"\n<p>\nThere are no local states of agent \\(x\\) in which \\(x\\) knows that\n\"\\(y\\) holds the token.\" Even when \\(x\\) does not hold the token,\n\\(x\\) does not know that \\(y\\) holds the token because\nthe token could be in a channel.\n</p><p>\nWhen \\(x\\) does not hold the token, \\(x\\) does not know that \\(y\\) holds\nthe token and \\(x\\) does not know that \\(y\\) does not hold the token.\n</p><p>\n\"NOT(x knows Q)\" AND \"NOT(x knows NOT Q)\"\n<br/>\nis a predicate which holds in the\nstate which \\(x\\) does not hold the token.\n\n</p>", "7": "<h5 class=\"w3-text-teal\">Notation</h5>\n    \"\\(x\\) knows \\(P\\) at a point \\(T\\)\" in a timeline means that at\n    point \\(T\\) agent \\(x\\) is in a state where the predicate \"\\(x\\)\nknows \\(P\\)\" holds.\n\n\n", "8": "", "9": "<h3 style=\"color:red;\">Example</h3>\n    A system consisting of agents \\(0, \\ldots,  N\\) has two indivisible\n    tokens which are not created or destroyed. \n\n<p>\n\"Agent \\(0\\) knows no other agent holds a token\" is a predicate on the states of agent\n\\(0\\); this predicate holds for a local state \\(s_{0}\\) of agent \\(0\\)\nif and only if no other agent holds a token when agent \\(0\\) is in\nstate \\(s_{0}\\).\n</p><p>\nSo, \"agent \\(0\\) knows P\" holds exactly when agent \\(0\\)\nholds both tokens. \n\n</p><p style=\"color:red;\">Example</p>\nThis example deals with a system consisting of two agents \\(x\\) and\n\\(y\\) and channels in both directions between the agents. The system\nhas a single indivisible token that is not created or destroyed.\nLet\n\\(P\\) be the predicate: \"\\(y\\) does <i>not</i> hold the token.\"\nAgent \\(x\\) knows \\(P\\) when \\(x\\) holds the token. \n<p>\n\\(\\neg\\) (\\(x\\) knows \\(P\\)) is a predicate too, and sometimes\nprogrammers refer to this predicate as \"\\(x\\) does not know \\(P\\).\"\n\n</p><p style=\"color:red;\">Example</p>\nIn the previous example, let \\(Q\\) be the predicate: \"\\(y\\) holds the token.\"\nIn what local states of agent \\(x\\) does \\(x\\) know that \"\\(y\\) holds the token?\"\n<p>\nThere are no local states of agent \\(x\\) in which \\(x\\) knows that\n\"\\(y\\) holds the token.\" Even when \\(x\\) does not hold the token,\n\\(x\\) does not know that \\(y\\) holds the token because\nthe token could be in a channel.\n</p><p>\nWhen \\(x\\) does not hold the token, \\(x\\) does not know that \\(y\\) holds\nthe token and \\(x\\) does not know that \\(y\\) does not hold the token.\n</p><p>\n\"NOT(x knows Q)\" AND \"NOT(x knows NOT Q)\"\n<br/>\nis a predicate which holds in the\nstate which \\(x\\) does not hold the token.\n\n</p><h5 class=\"w3-text-teal\">Notation</h5>\n    \"\\(x\\) knows \\(P\\) at a point \\(T\\)\" in a timeline means that at\n    point \\(T\\) agent \\(x\\) is in a state where the predicate \"\\(x\\)\nknows \\(P\\)\" holds.\n\n\n<h3 class=\"w3-text-teal\">Theorem: Knowledge and Consistent Cuts</h3>\nLet \\(P\\) be a predicate on the states of a system.\n<p>\n<i style=\"color:blue;\">\nIf  \\(x\\) knows \\(P\\) at a point \\(T\\) in \\(x\\)'s timeline\nthen \\(P\\) holds in every consistent cut through that point.</i>\n</p><p class=\"w3-text-teal\">Proof</p>\nThe states corresponding to all consistent cuts that pass through the same\npoint on \\(x\\)'s timeline have the same common value for \\(x\\)'s local\nstate.\n\n    <h3 style=\"color:red;\">Example</h3>\n<figure>\n<img alt=\"Fig2\" src=\"raw_webcrawl_data/Knowledge/KnowledgeDiagrams/Slide02.jpg\" style=\"width:60%\"/>&gt;\n    <figcaption>Fig. 2. Agent A knows P in all consistent cuts that cross\n    point T</figcaption>\n</figure>\n<p>\nThe top figure in both diagrams above show a time T at which agent A knows\nthat P holds. This implies that P holds in all consistent cuts through\npoint T. The lower figures in the diagrams show consistent cuts which\npasses point T on A's timeline; the theorem says that P holds for the\nstate at these cuts too. \n\n</p>", "10": "<h3 class=\"w3-text-teal\">Theorem: A Silent Agent retains\nKnowledge</h3>\n<hr class=\"new2\"/>\n", "11": "<h5 style=\"color:blue;\">\nAn agent that sends no information between a point \\(T\\) and a later\npoint \\(T'\\) retains all the knowledge it has at \\(T\\) at \\(T'\\).\n</h5>\n<hr class=\"new2\"/>\n\nLet \\(x\\) be an agent in a system, and let \\(P\\) be a\npredicate on a subsystem that does not include \\(x\\).\nLet \\(T\\) and \\(T'\\) be points on \\(x\\)'s timeline with \\(T &lt; T'\\). If\n\\(x\\) knows \\(P\\) at point \\(T\\) and \\(x\\) sends no messages in the\ninterval \\([T, T']\\) then \\(x\\) knows \\(P\\) at \\(T'\\).\n\n<p class=\"w3-text-teal\">Proof</p>\nLet \\(c'\\) be any consistent cut through point \\(T'\\) on \\(x\\)'s\ntimeline. We will prove that \\(P\\) holds for the state at cut \\(c'\\).\n\n<p>\nLet \\(c\\) be the\ncut that is identical to \\(c'\\) except that it passes through point \\(T\\) on\n\\(x\\)'s timeline. \\(c\\) is consistent because there are no\noutgoing edges from \\(x\\)'s timeline between cuts \\(c\\) and\n\\(c'\\).\n\n</p><p>\nBecause \\(x\\) knows \\(P\\) at \\(T\\), \\(P\\) holds at \\(c\\). Since \\(c\\) and \\(c'\\)\nare identical except for the intersection with \\(x\\)'s timeline,\nit follows that \\(P\\) holds \\(c'\\).\n\n</p>", "12": "<h5 style=\"color:red;\">Example</h5>\n<figure>\n<img alt=\"Fig3\" src=\"raw_webcrawl_data/Knowledge/KnowledgeDiagrams/Slide04.jpg\" style=\"width:60%\"/>&gt;\n<figcaption>Fig. 3. Illustration of Proof of Silent Agents\n</figcaption>\n</figure>\n", "13": "<h5 class=\"w3-text-teal\">Consequence of the Theorem</h5>\nAn agent doesn't lose knowledge by getting information from other\nagents.\n<p>\nAn agent can only lose knowledge by sending information to\nother agents. This seems counterintuitive; we'll look at the reasoning\nunderlying this in a later theorem.\n\n</p>", "14": "<h3 class=\"w3-text-teal\">Theorem: Agents who don't listen remain\nIgnorant</h3>\n<hr class=\"new2\"/>\n", "15": "", "16": "<h5 class=\"w3-text-teal\">Consequence of the Theorem</h5>\nAn agent doesn't lose knowledge by getting information from other\nagents.\n<p>\nAn agent can only lose knowledge by sending information to\nother agents. This seems counterintuitive; we'll look at the reasoning\nunderlying this in a later theorem.\n\n</p><h3 class=\"w3-text-teal\">Theorem: Agents who don't listen remain\nIgnorant</h3>\n<hr class=\"new2\"/>\n<h5 style=\"color:blue;\">\nAn agent that receives no information between a point \\(T\\) and a later\npoint \\(T'\\) learns no new knowledge between \\(T\\) and \\(T'\\).\n</h5>\n<hr class=\"new2\"/>\nLet \\(x\\) be an agent in a system, and let \\(P\\) be a\npredicate on a subsystem that does not include \\(x\\).\nLet \\(T\\) and \\(T'\\) be points on \\(x\\)'s timeline with \\(T &lt; T'\\). If\n\\(x\\) knows \\(P\\) at point \\(T'\\) and \\(x\\) received no messages in the\ninterval \\([T, T']\\) then \\(x\\) knows \\(P\\) at \\(T\\).\n\n<p>The proof has exactly the same structure as the proof of the\nprevious theorem.\n\n</p><h5 class=\"w3-text-teal\">Consequence of the Theorem</h5>\n\n\\(x\\) didn't learn anything in the interval  \\([T, T']\\); everything\n\\(x\\) knows at the later point \\(T'\\) is knowledge it already had at\nthe earlier point \\(T\\).\n\n<p>The only way for an agent to gain knowledge is\nto receive messages. An agent cannot learn about other agents by only\nsending messages or making internal state transitions.\n\n\n</p>", "17": "", "18": "<h5 style=\"color:red;\">Example</h5>\n<figure>\n<img alt=\"Fig3\" src=\"raw_webcrawl_data/Knowledge/KnowledgeDiagrams/Slide04.jpg\" style=\"width:60%\"/>&gt;\n<figcaption>Fig. 3. Illustration of Proof of Silent Agents\n</figcaption>\n</figure>\n", "19": "<h5 class=\"w3-text-teal\">Consequence of the Theorem</h5>\nAn agent doesn't lose knowledge by getting information from other\nagents.\n<p>\nAn agent can only lose knowledge by sending information to\nother agents. This seems counterintuitive; we'll look at the reasoning\nunderlying this in a later theorem.\n\n</p><h3 class=\"w3-text-teal\">Theorem: Agents who don't listen remain\nIgnorant</h3>\n<hr class=\"new2\"/>\n<h5 style=\"color:blue;\">\nAn agent that receives no information between a point \\(T\\) and a later\npoint \\(T'\\) learns no new knowledge between \\(T\\) and \\(T'\\).\n</h5>\n<hr class=\"new2\"/>\nLet \\(x\\) be an agent in a system, and let \\(P\\) be a\npredicate on a subsystem that does not include \\(x\\).\nLet \\(T\\) and \\(T'\\) be points on \\(x\\)'s timeline with \\(T &lt; T'\\). If\n\\(x\\) knows \\(P\\) at point \\(T'\\) and \\(x\\) received no messages in the\ninterval \\([T, T']\\) then \\(x\\) knows \\(P\\) at \\(T\\).\n\n<p>The proof has exactly the same structure as the proof of the\nprevious theorem.\n\n</p><h5 class=\"w3-text-teal\">Consequence of the Theorem</h5>\n\n\\(x\\) didn't learn anything in the interval  \\([T, T']\\); everything\n\\(x\\) knows at the later point \\(T'\\) is knowledge it already had at\nthe earlier point \\(T\\).\n\n<p>The only way for an agent to gain knowledge is\nto receive messages. An agent cannot learn about other agents by only\nsending messages or making internal state transitions.\n\n\n</p><h3 class=\"w3-text-teal\">Theorem: Knowledge implies Control</h3>\nLet \\(x\\) and \\(y\\) be agents in a system, and let \\(P\\) be a\npredicate on the states of \\(y\\).\nLet \\(T\\) and \\(T'\\) be instants in a trajectory with \\(T &lt; T'\\). If\n\\(x\\) knows \\(P\\) at  \\(T\\), and \\(\\neg P\\) holds at \\(T'\\), then there\nis a path in the timeline diagram from point \\(T\\) on \\(x\\)'s timeline\nto point \\(T'\\) on \\(y\\)'s timeline.\n\n<p>Proof: If there is no path from point \\(T\\) on \\(x\\)'s timeline\nto point \\(T'\\) on \\(y\\)'s timeline then there exists a consistent cut\nwhich crosses \\(x\\)'s timeline at \\(T\\) and\ncrosses \\(y\\)'s timeline at \\(T'\\).\n\n</p><h5 style=\"color:red;\">Example</h5>\nIn the figure below, agent \\(A\\) at point \\(T\\) knows that agent \\(C\\)\nholds no tokens. At a later point \\(T'\\) agent \\(C\\) holds a\ntoken. What must happen between points \\(T\\) and \\(T'\\)?\n<figure>\n<img alt=\"Fig4\" src=\"raw_webcrawl_data/Knowledge/KnowledgeDiagrams/Slide08.jpg\" style=\"width:60%\"/>&gt;\n<figcaption>Fig. 4. What must happen between T and T'?\n</figcaption>\n</figure>\n\nThere must be a path in the timeline diagram from point \\(T\\) on agent\n\\(A\\)'s timeline to point \\(T'\\) on agent \\(C\\)'s timeline. This path\nis represented by edges that show time elapsing on a timeline and\nmessage edges between timelines.\n<figure>\n<img alt=\"Fig5\" src=\"raw_webcrawl_data/Knowledge/KnowledgeDiagrams/Slide09.jpg\" style=\"width:60%\"/>&gt;\n<figcaption>Fig. 5. There must be a path from A at T to C at T'?\n</figcaption>\n</figure>\n<h5 class=\"w3-text-teal\">Consequence of the Theorem</h5>\nSuppose you and your friend communicate only by means of messages\nthat are delayed by arbitrary (finite) amounts. Consider a situation\nwhere your friend knows\nthat you are in the library at 9 pm. Then, from our definition of\nknowledge, because agents only know truth, you must be in the library\nat 9 pm. Moreover, you can't leave the library until\nyou receive a message from your friend; this message may go through\nintermediate agents.\n\n<p>\nIn one of the exercises we'll look at knowledge when agents have\nclocks that may drift from each other but are not more than a some\nconstant \\(M\\) units apart. If your friend knows that you will be in\nthe library till her watch reads 9:00 pm, and your watches may drift apart\nby a minute, then you can leave the library at 9:01 pm. Clocks are\nuseful even if they aren't perfect. More about clocks later.\n\n\n</p>", "20": "", "21": "<h5 style=\"color:red;\">Example</h5>\n<figure>\n<img alt=\"Fig3\" src=\"raw_webcrawl_data/Knowledge/KnowledgeDiagrams/Slide04.jpg\" style=\"width:60%\"/>&gt;\n<figcaption>Fig. 3. Illustration of Proof of Silent Agents\n</figcaption>\n</figure>\n<h5 class=\"w3-text-teal\">Consequence of the Theorem</h5>\nAn agent doesn't lose knowledge by getting information from other\nagents.\n<p>\nAn agent can only lose knowledge by sending information to\nother agents. This seems counterintuitive; we'll look at the reasoning\nunderlying this in a later theorem.\n\n</p><h3 class=\"w3-text-teal\">Theorem: Agents who don't listen remain\nIgnorant</h3>\n<hr class=\"new2\"/>\n<h5 style=\"color:blue;\">\nAn agent that receives no information between a point \\(T\\) and a later\npoint \\(T'\\) learns no new knowledge between \\(T\\) and \\(T'\\).\n</h5>\n<hr class=\"new2\"/>\nLet \\(x\\) be an agent in a system, and let \\(P\\) be a\npredicate on a subsystem that does not include \\(x\\).\nLet \\(T\\) and \\(T'\\) be points on \\(x\\)'s timeline with \\(T &lt; T'\\). If\n\\(x\\) knows \\(P\\) at point \\(T'\\) and \\(x\\) received no messages in the\ninterval \\([T, T']\\) then \\(x\\) knows \\(P\\) at \\(T\\).\n\n<p>The proof has exactly the same structure as the proof of the\nprevious theorem.\n\n</p><h5 class=\"w3-text-teal\">Consequence of the Theorem</h5>\n\n\\(x\\) didn't learn anything in the interval  \\([T, T']\\); everything\n\\(x\\) knows at the later point \\(T'\\) is knowledge it already had at\nthe earlier point \\(T\\).\n\n<p>The only way for an agent to gain knowledge is\nto receive messages. An agent cannot learn about other agents by only\nsending messages or making internal state transitions.\n\n\n</p><h3 class=\"w3-text-teal\">Theorem: Knowledge implies Control</h3>\nLet \\(x\\) and \\(y\\) be agents in a system, and let \\(P\\) be a\npredicate on the states of \\(y\\).\nLet \\(T\\) and \\(T'\\) be instants in a trajectory with \\(T &lt; T'\\). If\n\\(x\\) knows \\(P\\) at  \\(T\\), and \\(\\neg P\\) holds at \\(T'\\), then there\nis a path in the timeline diagram from point \\(T\\) on \\(x\\)'s timeline\nto point \\(T'\\) on \\(y\\)'s timeline.\n\n<p>Proof: If there is no path from point \\(T\\) on \\(x\\)'s timeline\nto point \\(T'\\) on \\(y\\)'s timeline then there exists a consistent cut\nwhich crosses \\(x\\)'s timeline at \\(T\\) and\ncrosses \\(y\\)'s timeline at \\(T'\\).\n\n</p><h5 style=\"color:red;\">Example</h5>\nIn the figure below, agent \\(A\\) at point \\(T\\) knows that agent \\(C\\)\nholds no tokens. At a later point \\(T'\\) agent \\(C\\) holds a\ntoken. What must happen between points \\(T\\) and \\(T'\\)?\n<figure>\n<img alt=\"Fig4\" src=\"raw_webcrawl_data/Knowledge/KnowledgeDiagrams/Slide08.jpg\" style=\"width:60%\"/>&gt;\n<figcaption>Fig. 4. What must happen between T and T'?\n</figcaption>\n</figure>\n\nThere must be a path in the timeline diagram from point \\(T\\) on agent\n\\(A\\)'s timeline to point \\(T'\\) on agent \\(C\\)'s timeline. This path\nis represented by edges that show time elapsing on a timeline and\nmessage edges between timelines.\n<figure>\n<img alt=\"Fig5\" src=\"raw_webcrawl_data/Knowledge/KnowledgeDiagrams/Slide09.jpg\" style=\"width:60%\"/>&gt;\n<figcaption>Fig. 5. There must be a path from A at T to C at T'?\n</figcaption>\n</figure>\n<h5 class=\"w3-text-teal\">Consequence of the Theorem</h5>\nSuppose you and your friend communicate only by means of messages\nthat are delayed by arbitrary (finite) amounts. Consider a situation\nwhere your friend knows\nthat you are in the library at 9 pm. Then, from our definition of\nknowledge, because agents only know truth, you must be in the library\nat 9 pm. Moreover, you can't leave the library until\nyou receive a message from your friend; this message may go through\nintermediate agents.\n\n<p>\nIn one of the exercises we'll look at knowledge when agents have\nclocks that may drift from each other but are not more than a some\nconstant \\(M\\) units apart. If your friend knows that you will be in\nthe library till her watch reads 9:00 pm, and your watches may drift apart\nby a minute, then you can leave the library at 9:01 pm. Clocks are\nuseful even if they aren't perfect. More about clocks later.\n\n\n</p><h3 class=\"w3-text-teal\">Theorem: Communication to learn about\nChange</h3>\nThis theorem is similar to the \"knowledge is control\" theorem.\n\n<p>\nLet \\(x\\) and \\(y\\) be agents in a system, and let \\(P\\) be a\npredicate on the states of \\(y\\).\nLet \\(T\\) and \\(T'\\) be instants in a trajectory with \\(T &lt; T'\\). If\n\\(\\neg P\\) holds at \\(T\\) and \\(x\\) knows \\(P\\) at \\(T'\\) then there\nis a path in the timeline diagram from point \\(T\\) on \\(y\\)'s timeline\nto point \\(T'\\) on \\(x\\)'s timeline.\n\n</p><h5 style=\"color:red;\">Example</h5>\nIn the figure below, agent \\(C\\) holds a token at point \\(T\\).\nAt a later point \\(T'\\) agent \\(A\\) knows that agent \\(C\\) holds no\ntokens. What must happen between points \\(T\\) and \\(T'\\)?\n<figure>\n<img alt=\"Fig6\" src=\"raw_webcrawl_data/Knowledge/KnowledgeDiagrams/Slide10.jpg\" style=\"width:60%\"/>&gt;\n<figcaption>Fig. 6. What must happen between T and T'?\n</figcaption>\n</figure>\n\nThere must be a path in the timeline diagram from point \\(T\\) on agent\n\\(C\\)'s timeline to point \\(T'\\) on agent \\(A\\)'s timeline. This path\nis represented by edges that show time elapsing on a timeline and\nmessage edges between timelines.\n<figure>\n<img alt=\"Fig7\" src=\"raw_webcrawl_data/Knowledge/KnowledgeDiagrams/Slide11.jpg\" style=\"width:60%\"/>&gt;\n<figcaption>Fig. 7. There must be a path from C at T to A at T'?\n</figcaption>\n</figure>\n", "22": "<h3 class=\"w3-text-teal\">What Agents Know about Channel States</h3>\nNext, let's look at systems in which messages are acknowledged.\nFor a pair of agents \\(x, y\\), let \\(ms\\) and \\(mr\\)\nbe the number of messages that \\(x\\) has sent to \\(y\\), and  the number \nof messages that \\(y\\) has received from \\(x\\), respectively.\nLet \\(as\\) and \\(ar\\) be\nthe number of acknowledgements that \\(y\\) has sent to \\(x\\), and\nthe number of acknowledgements that \\(x\\) has received from \\(y\\),\nrespectively.\n\nThe following is an invariant:\n<p>\n\\(ms \\geq mr \\geq as \\geq ar \\)\n\n</p><p>\nThe number of messages in the channel from \\(x\\) to \\(y\\) is \\(ms -\nmr\\).\nBecause \\(ms\\) and \\(ar\\) are variables of agent \\(x\\),\nagent \\(x\\) <i>knows</i> an upper bound,\\(\\; ms - ar\\), on the number of\nmessages in channel \\((x, y)\\).\nSo, \\(x\\) knows that the channel is empty when \\(\\; ms = ar\\).\n\n</p>", "23": "<h3 class=\"w3-text-teal\">What an agent cannot know</h3>\nAn agent cannot know that there are\nexactly \\(n\\) messages in a channel, for \\(n &gt; 0\\).\nYou can prove this result using the concept of consistent\ncuts. Intuitively, the agent cannot know whether a message is in the\nchannel or has been received.\n\n", "24": "<h3 class=\"w3-text-teal\">Chains of Knowledge</h3>\nLet \\(x, y, z\\) be agents of a system and \\(P\\) be a predicate on\nstates of the system. Then the following are all predicates:\n<ol>\n<ul>\n    \\(z\\) knows \\(P\\)\n  </ul>\n<ul>\n    \\(y\\) knows that \\(z\\) knows \\(P\\)\n  </ul>\n<ul>\n    \\(x\\) knows that \\(y\\) knows that \\(z\\) knows \\(P\\)\n  </ul>\n</ol>\nThe theorems given earlier apply to any predicate. For example, if\n\\(x\\) knows that \\(y\\) knows that \\(z\\) knows \\(P\\) at a point \\(t\\) in a\ntrajectory, and \\(\\neg P\\) holds at a later point \\(t'\\)\nthen there must be path in the timeline diagram from point \\(t\\) on\n\\(x\\)'s timeline to point \\(t'\\) on \\(z\\)'s timeline.\n\n", "25": "<h5 class=\"w3-text-teal\">Concurrent Systems with Shared Variables</h5>\nThe theorems\nand proofs given in this module apply to systems with shared\nvariables, and indeed any system with trajectories that are\nrepresentable by timeline diagrams and with consistent cuts.\n\n", "26": "<h3 class=\"w3-text-teal\">Summary</h3>\nMany people working on distributed systems use the phrase \"an agent\nknows.\" This module gives a definition of the concept that is\nconsistent with intuitive definitions of knowledge.\nThe central idea in this module is the\nrelationship between what agents know and consistent cuts of\ntimelines. We presented several theorems about agent knowledge which\nare intuitive when applied to human agents. The proofs are\nstraightforward and are all based on consistent cuts of timelines.\n\n"}, "raw_webcrawl_data/Knowledge/KnowledgeSelfTest.html": {"0": "<h1 class=\"w3-text-teal\">Self Test</h1>\n", "1": "<h2 class=\"w3-text-teal\">Problem 1</h2>\n", "2": "<h4 class=\"w3-text-teal\">Do Agents only know Truth?</h4>\n    True or False?\n    <p>\n    \\(\n    [x \\; \\textrm{knows} \\; P \\quad \\Rightarrow \\quad P]\n    \\)\n\n\n    </p>", "3": "<h2 class=\"w3-text-teal\">Problem 2</h2>\n<figure>\n<img alt=\"Fig1\" src=\"raw_webcrawl_data/Knowledge/KnowledgeSelfTest/Slide1.jpg\" style=\"width:60%\"/>&gt;\n    <figcaption>Fig. 1. Agent A knows P at point T\n    </figcaption>\n</figure>\n    The figure is a timeline diagram for agents A, B. Message lines\n    are colored red. T is a time represented as a vertical line.\n\n    <p> Let P be a predicate on the states of agent B. If at time T,\n    agent A knows P, then P must hold at all points between what point\n    and T?\n\n    </p><ol>\n<li>\n      between point U and T, but not necessarily before U\n      </li>\n<li>  \n      between point V and T, but not necessarily before V\n      </li>\n<li>  \n      between point W and T, but not necessarily before W,\n      </li>\n</ol>\n", "4": "<h2 class=\"w3-text-teal\">Problem 3</h2>\n<figure>\n<img alt=\"Fig2\" src=\"raw_webcrawl_data/Knowledge/KnowledgeSelfTest/Slide2.jpg\" style=\"width:60%\"/>&gt;\n    <figcaption>Fig. 2. Agent A knows P at point T\n    </figcaption>\n</figure>\n    \n    The figure is a timeline diagram for agents A, B. Message lines\n    are colored red. T is a time represented as a vertical line.\n\n    <p> Let P be a predicate on the states of agent B. If at time T,\n    agent A knows P, then P must hold at all points between T and what\n    point?\n\n    </p><ol>\n<li>\n      between point T and point 1, but not necessarily before 1\n      </li>\n<li>  \n      between point T and point 2, but not necessarily before 2\n      </li>\n<li>  \n      between point T and point 3, but not necessarily before 3\n      </li>\n<li>  \n      between point T and point 4, but not necessarily before 4\n      </li>\n<li>  \n      between point T and point 5, but not necessarily before 5\n      </li>\n</ol>\n", "5": "<h2 class=\"w3-text-teal\">Problem 4</h2>\n    Let \\(P\\) be a predicate on the states of an agent \\(y\\), and let \\(x\\)\n    be a different agent in the system. Let \\(T\\) and \\(T'\\) be points on a\n    timeline with \\(T &lt; T'\\).\n    <p class=\"w3-text-teal\">Part a</p>\n    True or False? If \\(\\neg(x \\; \\textrm{knows} \\; P)\\) holds at \\(T\\)\n    and \\(x \\; \\textrm{knows} \\; P\\) holds at \\(T'\\) then \\(x\\) must\n    have received a message between \\(T\\) and \\(T'\\).\n    <p class=\"w3-text-teal\">Part b</p>\n    True or False? If \\(\\neg(x \\; \\textrm{knows} \\; P)\\) holds at \\(T\\)\n    and \\(x \\; \\textrm{knows} \\; P\\) holds at \\(T'\\) then \\(x\\) must\n    have sent a message between \\(T\\) and \\(T'\\).\n\n    \n\n    \n", "6": "", "7": "<h2 class=\"w3-text-teal\">Problem 1</h2>\n<h4 class=\"w3-text-teal\">Do Agents only know Truth?</h4>\n    True or False?\n    <p>\n    \\(\n    [x \\; \\textrm{knows} \\; P \\quad \\Rightarrow \\quad P]\n    \\)\n\n\n    </p>", "8": "<h2 class=\"w3-text-teal\">Problem 2</h2>\n<figure>\n<img alt=\"Fig1\" src=\"raw_webcrawl_data/Knowledge/KnowledgeSelfTest/Slide1.jpg\" style=\"width:60%\"/>&gt;\n    <figcaption>Fig. 1. Agent A knows P at point T\n    </figcaption>\n</figure>\n    The figure is a timeline diagram for agents A, B. Message lines\n    are colored red. T is a time represented as a vertical line.\n\n    <p> Let P be a predicate on the states of agent B. If at time T,\n    agent A knows P, then P must hold at all points between what point\n    and T?\n\n    </p><ol>\n<li>\n      between point U and T, but not necessarily before U\n      </li>\n<li>  \n      between point V and T, but not necessarily before V\n      </li>\n<li>  \n      between point W and T, but not necessarily before W,\n      </li>\n</ol>\n", "9": "<h2 class=\"w3-text-teal\">Problem 3</h2>\n<figure>\n<img alt=\"Fig2\" src=\"raw_webcrawl_data/Knowledge/KnowledgeSelfTest/Slide2.jpg\" style=\"width:60%\"/>&gt;\n    <figcaption>Fig. 2. Agent A knows P at point T\n    </figcaption>\n</figure>\n    \n    The figure is a timeline diagram for agents A, B. Message lines\n    are colored red. T is a time represented as a vertical line.\n\n    <p> Let P be a predicate on the states of agent B. If at time T,\n    agent A knows P, then P must hold at all points between T and what\n    point?\n\n    </p><ol>\n<li>\n      between point T and point 1, but not necessarily before 1\n      </li>\n<li>  \n      between point T and point 2, but not necessarily before 2\n      </li>\n<li>  \n      between point T and point 3, but not necessarily before 3\n      </li>\n<li>  \n      between point T and point 4, but not necessarily before 4\n      </li>\n<li>  \n      between point T and point 5, but not necessarily before 5\n      </li>\n</ol>\n"}, "raw_webcrawl_data/Byzantine/ByzantineWritten.html": {"0": "<h1 class=\"w3-text-teal\">Byzantine Consensus: Written\nMessages</h1>\n", "1": "<h4 class=\"w3-text-teal\">\nThis module introduces Byzantine consensus algorithms in which agents\nreach consensus in a sequence of synchronized rounds <i>even though some\nagents don't follow the protocol</i>.\n</h4>\n\n\n    In this module we study algorithms by which a collection of agents\n    reach a consensus among alternative values.\n    <a href=\"../Paxos/Paxos.html\">The Paxos algorithm</a> is\n    an example of how agents reach consensus.\n    <a href=\"https://en.wikipedia.org/wiki/Consensus_(computer_science)\">\n    Agents cannot reach consensus if message delays or agent\n    operations are arbitrarily slow.</a> Next, we study a consensus\n    algorithm in which message delays are bounded and in which agents\n    are guaranteed to come to a consensus <i>even though some agents\n    do not follow the protocol.</i>\n<p>\n    The algorithm operates in a sequence of steps called <i>rounds</i>.\n    All messages sent in a round are delivered in the next round.\n    Agents execute actions in each round after receiving\n    messages sent in the previous round; these actions may include\n    sending messages. So, the Byzantine algorithm is synchronous.\n\n    \n\n    </p>", "2": "<h3 class=\"w3-text-teal\">Byzantine Generals Problem: Overview</h3>\n    A general has \\(N\\) army units each of which is led by a\n    lieutenant general, herafter referred to merely as lieutenant. We\n    refer to the general and the lieutenants, collectively, as\n    agents. An agent may be either loyal or disloyal. A loyal general\n    gives the same command to all lieutenants. A loyal general's\n    command is either <i>attack</i> or <i>retreat</i>.  A disloyal\n    general may give different commands to different lieutenants and\n    may give no commands to some. The lieutenants receive commands\n    from the general and then communicate among themselves to reach a\n    consensus.  Loyal lieutenants follow an algorithm while disloyal\n    lieutenants may or may not.\n    <p>\n    The figure below illustrates the difference between loyal and\n    disloyal generals.\n\n    <figure>\n<img alt=\"Fig1\" src=\"raw_webcrawl_data/Byzantine/Slide01.jpg\" style=\"width:100%\"/>\n<figcaption>Fig.1: Loyal and disloyal general behavior</figcaption>\n</figure>\n</p>", "3": "<h4 class=\"w3-text-teal\">Byzantine Generals Problem:\nSpecification</h4>\n<p>\n  A loyal general sends attack messages to all lieutenants or sends\nretreat messages to all lieutenants.\n\nA disloyal general sends arbitrary messages to lieutenants.\n\n    </p><ol>\n<li><i>Validity</i>: Loyal lieutenants must obey a loyal general. If a loyal general\n    gives the command to attack then all loyal lieutenants must\n    attack. Likewise, if a loyal general\n    gives the command to retreat then all loyal lieutenants must\n    retreat.</li>\n<li><i>Consensus</i>: Loyal lieutenants come to a consensus: either all of them\n    attack or all of them retreat.</li>\n</ol>\n<p>\n  \n    The specification does not require that traitors be\n    discovered. For example, the algorithm doesn't have to determine\n    whether the general or a lieutenant is loyal or disloyal.\n\n    </p><p>\n    If the only requirement is validity, and consensus isn't\n    required, then the solution is trivial: all loyal lieutenants obey\n    the general whether the general is loyal or disloyal. If the only\n    requirement is consensus then the\n    solution is trivial: all loyal lieutenants agree on a predefined\n    value, say retreat, regardless of the command issued by the\n    general. The conjunction of both requirements makes the problem\n  difficult.\n\n  \n    </p>", "4": "<h4 class=\"w3-text-teal\">Oral and Written Messages</h4>\n  There are two versions of the problem.\n\n  <ol>\n<li>\n<i>Written Messages</i>:\n    In this version an agent may send copies of messages that it\n    receives to other agents but cannot modify the\n    messages. Also, an agent cannot forge signatures. So, an agent can\n    receive a message M signed by lieutenant \\(A\\) only if \\(A\\) sent M to\n    some agent.\n    </li>\n<li>\n<i>Oral Messages</i>:\n    In this version an agent can modify messages and forge\n    signatures.So, an agent can receive a message M signed by agent\n    \\(A\\) even if \\(A\\) never sent M to any agent.\n    </li>\n</ol>\n  The algorithm for written messages is simpler and requires fewer\n    messages.\n\n\n    ", "5": "<h2 class=\"w3-text-teal\">Algorithm with Written Messages</h2>\n<p>\nA reliable lieutenant who does not get a message from in round 0 treats the\nabsence of the message as the same as receiving a retreat message.\n  Likewise, if a loyal lieutenant gets a message that is not an attack\n  or a retreat message then the lieutenant treats the message as a\n  retreat message.\n\n  (We use retreat as a default. We could just as well have used attack\n  as the default.)\n\n  </p><p>\n  Next, we give an overview of the algorithm.\n\n  </p>", "6": "<h5 class=\"w3-text-teal\">Commit to Attack</h5>\n  At each round in the algorithm, a loyal lieutenant has either\n  committed to attack or not.\n\n  A lieutenant that has not committed to attack on a round may commit\n  to attack on a later round.\n\n  If any loyal lieutenant commits to attack in any round then it remains\n  committed to attack thereafter.\n  A loyal lieutenant retreats if it has not committed to attack at the\n  end of the last round.\n\n  ", "7": "<h5 class=\"w3-text-teal\">Messages</h5>\n  A message is an attack message from the general or a commitment to\n  attack by a lieutenant. We call commitment messages\n  attack messages. An attack message is identified by the\n  agent (general or lieutenant) that created the message.\n\n  ", "8": "<h5 class=\"w3-text-teal\">Evidence for Attack</h5>\n  A loyal lieutenant commits to attack on round \\(r \\geq 1 \\), for the first\n  time, if the lieutenant has received an attack message from the\n  general and at\n  least \\(r - 1\\) lieutenants. \n  When the lieutenant commits to attack it sends copies of these\n  messages, and its own attack message, to all\n  other lieutenants.\n  So, in round \\(r+1\\), each lieutenant receives an attack messages from\n  the general and at least \\(r\\)\n  lieutenants.\n  And so, all loyal lieutenants commit to attack in this round.\n\n\n  ", "9": "<h4 class=\"w3-text-teal\">The Algorithm</h4>\n  We will use the tactic that is helpful in analyzing distributed\nalgorithms that operate in rounds.\nWe will prove properties of a sequential algorithm and then show the\n  equivalence of the sequential and distributed algorithms.\nThe sequential algorithm is given next.\n\n<p class=\"w3-text-teal\">Local Variables</p>\n  Associated with each lieutenant <code>C</code> are local variables\n<code>C.received</code> and <code>C.sent</code> which are sets of\n  agents (general or lieutenants).\n\n<code>C.received</code> is the set of agents from which \n<code>C</code> has received attack messages (or their copies).\n\n<code>C</code> sends attack messages (or copies) from agents in\n<code>C.sent</code>.\n\nThe symbol <code>g</code> represents the general.\n\n", "10": "<h5 class=\"w3-text-teal\">Initialization: Round 0</h5>\n<code>A.sent = {}</code> for all lieutenants\n<code>A</code>.\n\n<ol>\n<li>\n  If the general is loyal and sends attack messages then the set of\n  agents from which a lieutenant has received attack messages is the\n  singleton set consisting of the general. So, for all\n  lieutenants <code>A</code>:\n  <br/>\n<code>A.received = {g}</code>\n</li>\n<li>\n  If the general is loyal and sends retreat messages then for all\n  lieutenants <code>A</code>:\n  <br/>\n<code>A.received = {}</code>\n</li>\n<li>\n  If the general is disloyal, then \n  <code>A.received = {g}</code> for some lieutenants <code>A</code>,\n  and <code>A.received = {}</code> for the others.\n  </li>\n</ol>\n  \nThe algorithm operates in a sequence of rounds with round-number\n<code>r</code> stepping from 1 to <code>t+1</code>. The\n<code>r</code>-th iteration for loyal agent <code>c</code> consists of\nthe following two steps.\n", "11": "<h5 class=\"w3-text-teal\">Round <code>r &gt; 0</code></h5>\n<p class=\"w3-text-teal\">Step 1</p>\n<pre>\nif (|C.received| &gt;= r) AND  (g in C.received):\n    C.commit = True\n    C.sent = C.received UNION {C}\n</pre>\nIf <code>C.received</code> has messages that show that:  (1) the number of\nagents that have committed to attack on round\n<code>r</code> is at least <code>r</code>, and (2) the general has sent an\nattack message, then <code>C</code> commits to attack and sends these\nmessages as well as an additional message that <code>C</code> has\ncommitted to attack.\n\n<p class=\"w3-text-teal\">Step 2</p>\n<pre>\nC.received = (\n    (UNION over all loyal agents B of B.sent)\n             UNION\n     an arbitrary subset of disloyal agents)\n</pre>\n<code>C</code> receives the messages sent by loyal lieutenants and messages\nsent by disloyal lieutenants.\n\nA disloyal lieutenant, <code>A</code>, can send messages to some lieutenants\nthat <code>A</code> is committed to attack, and not send these messages\nto other lieutenants.\n\nLieutenant <code>C</code> receives attack messages from an arbitrary subset\nof disloyal lieutenants.\n\n<p>\nLieutenant <code>C</code> does not know which agents are loyal and\nwhich are disloyal.\n\n<code>C.received</code> is the set of all messages sent to\n<code>C</code> regardless of whether the senders are loyal or\ndisloyal.\n\n\n\n</p>", "12": "<h3 class=\"w3-text-teal\">Proof of Correctness</h3>\n", "13": "<h5 class=\"w3-text-teal\">Case 1: General is loyal and sends attack\nmessages</h5>\nIn this case, at the start round 1, <code>C.received = {g}</code>.\n\nSo, the if-clause in step 1 of round 1 is True, and therefore all\nloyal lieutenants commit to attack at the end of round 1.\n\n", "14": "<h5 class=\"w3-text-teal\">Case 2: General is loyal and sends retreat\nmessages</h5>\nA disloyal lieutenant cannot forge the general's signature, and so it is\nimpossible for any lieutenant to have a copy of an attack message from the\ngeneral.\nTherefore the if-clause of step 1 is never satisfied, and so no loyal\nlieutenant commits to attack in any step.\n\n", "15": "<h5 class=\"w3-text-teal\">Case 3: General is disloyal</h5>\n<p class=\"w3-text-teal\">Part 1</p>\nWe first show that if any loyal lieutenant commits on round\n<code>r</code> then all loyal lieutenants commit by the end of round\n<code>r+1</code>.\n\n<p>\nA loyal lieutenant <code>C</code> commits on round <code>r</code>\nexactly when the set <code>C.received</code> has at least\n<code>r</code> attack messages including one from the general.\nSo, in round <code>r</code>, <code>C.sent</code> has at least\n<code>r+1</code> attack messages including one from the general.\nTherefore, the if-clause of step 1 evaluates to True in round\n<code>r+1</code>, and so all\nloyal lieutenants commit by the end of round <code>r+1</code>.\n\n</p><p>\nTherefore, if any loyal lieutenant commits by the end of round\n<code>t-1</code> then all loyal lieutenants commit by the end of round\n<code>t</code>.\n\n</p><p class=\"w3-text-teal\">Part 2</p>\nWe next show that if no loyal lieutenant has committed by the end of\nround <code>t-1</code> then no loyal lieutenant commits in round\n<code>t</code>.\n\n<p>\n<code>C.sent = {}</code> at the end\nof round <code>t-1</code> if no loyal lieutenant has committed to\nattack.\nTherefore in round <code>t</code>,\n<code>C.received</code> is an \narbitrary subset of disloyal lieutenants.\nThere are at most <code>t-1</code> disloyal lieutenants because there\nare at most <code>t</code> disloyal agents, and the general is\ndisloyal.\nSo, on round <code>t</code>, the if-clause of step 1 evaluates to False.\n</p><p>\n</p><p class=\"w3-text-teal\">Part 3</p>\nFrom parts 1 and 2, by the end of\nround <code>t</code>, either all loyal lieutenants commit, or no loyal\nlieutenant commits.\n\n\n", "16": "<h3 style=\"color:red;\">Example</h3>\n\nThe figure below illustrates a situation in which a loyal lieutenant C commits to\nattack on round 3, if it hasn't already committed to attack on rounds\n1 and 2.\nThe figure shows C getting attack messages (red boxes) signed by the\ngeneral and lieutenants A and B on round 2. The general and\nlieutenants A and B may be disloyal or loyal.\n\n    <figure>\n<img alt=\"Fig2\" src=\"raw_webcrawl_data/Byzantine/Slide02.jpg\" style=\"width:100%\"/>\n<figcaption>Fig.2: Example of a lieutenant committing to attack</figcaption>\n</figure>\n<p>\n Because C is loyal it follows the algorithm, and C\ncommits to attack at the end of round 3 because C receives a signed attack\n message from the general and signed attack messages from two\n different lieutenants.\nSo, in round 3, C broadcasts copies of attack messages\nsigned by the general and attack\nmessages signed by A and B and attack messages, and also an attack\n message signed by C itself. All loyal\nlieutenants commit to attack in round 4 because they receive attack\nmessages signed by the general and 3 different lieutenants (A, B, C).\n\n\n\n</p>"}, "raw_webcrawl_data/Byzantine/ByzantineOral.html": {"0": "<h1 class=\"w3-text-teal\">Byzantine Consensus: Oral Messages</h1>\n<p class=\"w3-text-teal\">\n\nThis module describes a Byzantine consensus algorithm in which\nmessages are not encrypted. An agent <i>x</i> that receives a message signed by\nan agent <i>y</i> cannot tell whether <i>y</i> signed the message or\nwhether some other agent forged <i>y</i>'s signature and corrupted the\n  message.\n</p>\n<p>\nThis module describes solutions to the Byzantine problem with oral\nmessages whereas the previous module studied the problem with written\nmessages.  For convenience we repeat the problem specification next.\n\n  </p><p>\n  An agent is a general or a lieutenant.\n  An agent may be loyal or disloyal.\n  Let \\(N\\) be the number of agents and \\(t\\) the number of disloyal\n  agents. \n  \nThe general sends a command to\neach lieutenant where the command is either attack or retreat.\nA loyal general sends the same command to all lieutenants whereas a\ndisloyal general may send different commands to different lieutenants.\nEach lieutenant decides to attack or retreat at the end of the\nalgorithm.\n\n  </p><p>\n  If all loyal lieutenants get the same command then each loyal\n  lieutenant obeys the commands that it received; if the command is\n  attack then each loyal lieutenant attacks, and if the command is\n  retreat then each loyal lieutenant retreats.\n  </p><p>\n Even if loyal lieutenants receive different commands, all loyal\n  lieutenants make the same the decision; either all loyal lieutenants\n  attack or all loyal lieutenants retreat.\n\n  </p>", "1": "<h4 class=\"w3-text-teal\">Notation</h4>\n\n  We use indices \\(i, j, k\\) for loyal agents; \\(x, y\\) for\n  generic agents who may be loyal or disloyal; and \\(e\\) for\n  disloyal agents.\n  Nothing in an agent's id or data identifies the agent as loyal or\n  disloyal.\n  Moreover, the algorithm does not have to discover which agents act\n  disloyally.\n\n  <p>\n  For a nonempty list \\(L\\), we use the Python notation \\(L_{*}\\) to\n  refer the last element of the list.\n  For example, if \\(L = [5, 6]\\), then \\(L_{*} = 6\\).\n\n  </p><p>\n  For a list \\(L\\) and an element \\(x\\), the notation \\(L , x\\) represents a list\nconsisting of \\(x\\) appended to the tail of \\(L\\).\nFor example, if \\(L\\) is the list \\([1, 2]\\) then \\(L , 3\\) is the\n  list \\([1, 2, 3]\\), and \\(L, 3, 4\\) is the list \\([1, 2, 3, 4]\\).\n\n</p><p>\nThe general is the agent with index \\(0\\), and the lieutenants have indices \\(1,\n\\ldots, N-1\\).\n\n</p><p>\n   Let \\(m[x]\\) be the message that the general sends\nlieutenant \\(x\\), and let \\(a[x]\\) be the decision the lieutenant\n  \\(x\\) makes.\n\n</p>", "2": "<h3 class=\"w3-text-teal\">Specification</h3>\nThe specification has two parts, validity and consensus.\n\n  ", "3": "<h4 class=\"w3-text-teal\">Validity: Loyal lieutenants obey a loyal general.</h4>\n<p>\n  If all loyal lieutenants get the same message then each loyal\n    lieutenant obeys the message that it receives.\n  </p><p>\n  \\(\n  (\\forall i, j: m[i] = m[j]) \\quad \\Rightarrow\n\\quad (\\forall i: a[i] = m[i])\n\\)\n\n</p>", "4": "<h4 class=\"w3-text-teal\">Consensus: Loyal lieutenants make the same\ndecision.</h4>\n\\(\n\\forall i, j: a[i] = a[j]\n\\)\n\n", "5": "<h3 class=\"w3-text-teal\">Assumptions</h3>\nThe oral Byzantine version makes fewer assumptions than the written\n  version. The assumptions made are as follows: \n<ol>\n<li>\n<i>Synchrony:</i> The algorithm operates in a synchronous fashion in a sequence of\n  rounds or synchronous steps. If an agent \\(x\\) does not send\n  a message to an agent \\(y\\) in a given round then \\(y\\) can detect\n  that \\(x\\) did not send a message to it in that round.\n  </li>\n<li>\n<i>Reliability:</i> If an agent \\(y\\) sends a message \\(m\\) to an agent \\(y\\) in a given\n  round then \\(z\\) receives \\(m\\) in that round.\n  </li>\n<li>\n<i>Receiver knows sender:</i> An agent that receives a message knows which agent sent it. If an\n  agent \\(z\\) receives a message \\(m\\) from an agent \\(y\\) in a round\n  then \\(z\\) knows that \\(y\\) sent \\(m\\) in that round.\n  </li>\n</ol>\n", "6": "<h4 class=\"w3-text-teal\">Why oral messages are harder</h4>\n\nIn the written version of the problem, if an agent\n\\(z\\) receives a message \\(m\\) from any agent where \\(m\\) is signed by the\ngeneral then \\(z\\) knows that the general did send \\(m\\).\nAn agent cannot forge the general's signature and send a false\nmessage.\nBy contrast, in the oral, or unencrypted version, any agent can\nforge any agent's signature and send corrupted messages.\n\n\n\n\n\n", "7": "<h2 class=\"w3-text-teal\">Byzantine Generals Algorithm</h2>\nMessages in the algorithm are either <i>attack</i> or <i>retreat</i>\nmessages.  If an agent \\(x\\) does not receive a message from an agent\n\\(y\\) on a round then \\(x\\) treats the absence of the message from\n\\(y\\) in the same way as if \\(x\\) received a <i>retreat</i> message\nfrom \\(y\\). So, the algorithm only deals with <i>attack</i> and\n<i>retreat</i> messages and does not deal with steps that an agent\ntakes if it does not receive a message.\n\n<p>\nFirst we describe the flow of message in the algorithm and then\ndescribe the algorithm\n\n\n</p>", "8": "<h4 class=\"w3-text-teal\">Message Flow</h4>\nMessages flow along a tree of height \\(t + 1\\).\n\nThe root node is \\(m[0]\\) which represents the general's command.\nEach node of the tree is of the form \\(m[L]\\) where \\(L[0] = 0\\) and\n\\(L[1, \\ldots, ]\\) is a list of lieutenants where each lieutenant\nappears at most once.\n\n<p>\nThe next figure illustrates a part of the messaging tree for \\(N = 7,\nt = 2\\).\n(There is insufficient space to show the complete tree.)\n\n<figure>\n<img alt=\"Fig1\" src=\"raw_webcrawl_data/Byzantine/Byzantine_5/Byzantine_5.001.jpeg\" style=\"width:100%\"/>\n<figcaption>Fig.1: A Part of the Message Tree for General and 6 Lieutenants\n</figcaption>\n</figure>\n</p><p>\nEach non-leaf node \\(m[L]\\) in the tree has a child \\(m[L,\nx]\\) for each lieutenant \\(x\\) that is not in \\(L\\).\nFor example, \\(m[0]\\) has children \\(m[0,1], m[0,2], \\ldots, m[0,\nN-1]\\).\n\n\n\n</p><p>\n\\(m[0,x]\\) is the message that lieutenant \\(x\\) receives from the general.\n\\(m[0, x_{0}, \\ldots, x_{k}]\\) is the message that\nlieutenant \\(x_{0}\\) receives from the general and forwards to\nlieutenant \\(x_{1}\\),\n...\nwhich in turn forwards the message to lieutenant \\(x_{n-1}\\),\nwhich in turn forwards the message to lieutenant \\(x_{n}\\).\n\n\n</p><p>\nIf the general is loyal then it sends the same message to all\nlieutenants: \\(m[0,x] = m[0]\\) for all \\(x\\).\nLoyal lieutenants forward the messages that they receive; however,\ndisloyal lieutenants may send arbitrary messages.\n\n\n</p>", "9": "<h4 style=\"color:red;\">Example</h4>\n\nThe diagram below shows a situation in which the general is loyal and\nsends attack messages to all lieutenants.\nLieutenant 1 is disloyal (shown as a dashed circle).\nLieutenant 1 sends retreat messages to some lieutenants and attack\nmessages to others.\nLieutenant 2 is loyal, and so it broadcasts the message that\nit receives.\n\n\n<figure>\n<img alt=\"Fig2\" src=\"raw_webcrawl_data/Byzantine/Byzantine_5/Byzantine_5.002.jpeg\" style=\"width:100%\"/>\n<figcaption>Fig.2: Edges Aggregation Dataflow Trees\n</figcaption>\n</figure>\n", "10": "<h4 class=\"w3-text-teal\">Aggregating Phase</h4>\nMessages received by a lieutenant are processed by the lieutenant in\nsteps that are also represented by a tree called the <i>aggregation tree</i>.\nThe aggregation tree has a node \\(a[L]\\) for each node \\(m[L]\\) of the\nmessage tree.\n<p>\nThe diagram below shows a part of the aggregation tree for \\(N=7,\nt=2\\); these are the processing steps of lieutenant 1.\n\n\n<figure>\n<img alt=\"Fig3\" src=\"raw_webcrawl_data/Byzantine/Byzantine_5/Byzantine_5.003.jpeg\" style=\"width:100%\"/>\n<figcaption>Fig.3: Aggregation Tree: Steps for Lieutenant 1\n</figcaption>\n</figure>\n</p><p>\nEach node \\(a[L, i]\\) of the tree has a child \\(a[L, x, i]\\) for each\n\\(x\\) that is not in \\([L, i]\\).\nFor example, \\(a[0, 1]\\) has children \\(a[0, 2, 1], a[0, 3, 1], \\ldots\na[0, 6, 1]\\).\n\n</p><p class=\"w3-text-teal\">Connections from Messaging to Aggregating Nodes</p>\nThere is an edge directed from each message node \\(m(L)\\) to the aggregation\nnode \\(a(L)\\).\nFor example, there are edges from \\(m[2, 3, 1]\\) to \\(a[2, 3,\n1]\\), and from \\(m[2, 1]\\) to \\(a[2, 1]\\), and from  \\(m[1]\\) to\n\\(a[1]\\). \n\n<p class=\"w3-text-teal\">Output of Aggregating Nodes</p>\nThe output of an aggregation node is the majority of its inputs.\nFor example:\n<p>\n\\(\n  a[2, 1] = \\textrm{majority}(m[2, 1], a[2, 3, 1], a[2, 4, 1], \\ldots, a[2, 6, 1])\n\\)\n</p><p>\nIf there are an equal number of attack and retreat inputs, then the\nmajority value is defined to be any default value.\n\n</p>", "11": "<h3 style=\"color:red;\">Example</h3>\n\nThe next diagram shows the data flow -- messaging and aggregation\ntrees, and the connections between them -- for a system where\n\\(N=4, t=1\\).\n\n<figure>\n<img alt=\"Fig4\" src=\"raw_webcrawl_data/Byzantine/Byzantine_5/Byzantine_5.004.jpeg\" style=\"width:100%\"/>\n<figcaption>Fig.4: Dataflow for 4 agents 1 of which is disloyal</figcaption>\n</figure>\n\n", "12": "<h3 class=\"w3-text-teal\">Inductive Generation of the Data Flow</h3>\n\nThe basic unit of data flow, which is replicated many times, is shown\nin the top diagram of the figure below.\nThe input to the unit is a node \\(m[L]\\) of the message tree; this\nunit is specified by \\(L\\),and the message \\(m[L]\\).\nThe output of the unit are nodes \\(a[L, x]\\) of the\naggregation tree, for all lieutenants \\(x\\) not in \\(L\\).\n\n<p>\nThe base case of the induction is a node at depth \\(t\\).\nThe input for the base case is \\(m[L]\\) where \\(L\\) is a list starting\nwith \\(0\\) and followed by \\(t\\) lieutenants.\nFor the base case, \\(a[L, x] = m[L, x]\\) for all \\(x\\).\nThe base case is illustrated in the lower diagram.\n\n<figure>\n<img alt=\"Fig5\" src=\"raw_webcrawl_data/Byzantine/Byzantine_5/Byzantine_5.005.jpeg\" style=\"width:100%\"/>\n<figcaption>Fig.5: Structure of Dataflow</figcaption>\n</figure>\n\nThe data flow connecting a message node \\(m[L]\\) at depth \\(d &lt; t\\) to\naggregation nodes \\(a[L,x]\\) is shown below.\n<figure>\n<img alt=\"Fig6\" src=\"raw_webcrawl_data/Byzantine/Byzantine_5/Byzantine_5.006.jpeg\" style=\"width:100%\"/>\n<figcaption>Fig.6: Structure of Dataflow</figcaption>\n</figure>\n\nMessage node \\(m[L]\\) feeds message nodes \\(m[L,x]\\).\nThe connections between \\(m[L,x]\\) and aggregation nodes \\(a[L,x,y]\\)\nare specified by the data flow connecting nodes of depth \\(d + 1\\),\nshown in the diagram by blue dotted lines.\n\n</p><p>\nThe value of an aggregation node is the majority of its inputs.\n</p><p>\n\\(\na[L,i] = \\textrm{majority}(m[L,i], [\\forall x \\notin [L, i]: a[L, x, i]])\n\\)\n</p><p>\nFor example,\n</p><p>\n\\(\na[0, 1, 2] = \\textrm{majority}(m[0,1,2], a[0, 1, 3, 2], a[0, 1, 4, 2],\na[0, 1, 5, 2], \\ldots)\n\\)\n\n\n\n</p>", "13": "<h3 class=\"w3-text-teal\">Proof of Validity</h3>\nWe prove that for all nodes \\(m[L]\\) of the message tree, if \\(L_{*}\\)\nis loyal then for all \\(i\\) not in \\(L\\):\n<p>\n\\(\na[L, i] = m[L]\n\\)\n\n</p><p>\nThe proof is by induction.\nThe base case is for message nodes at depth \\(t\\).\nWe prove that if validity holds for message nodes at depth \\(d &gt; 0\\)\nthen it holds for message nodes at depth \\(d-1\\).\n\n</p>", "14": "<h4 class=\"w3-text-teal\">Base Case</h4>\nSee the lower diagram of figure 5. \nFor a message node \\(m[L]\\) at depth \\(t\\),\n<p>\n\\(\na[L, i] = m[L, i]\n\\)\n</p><p>\nIf \\(L_{*}\\) is loyal then \\(m[L, i] = m[L]\\).\nand the result follows.\n\n\n\n\n</p>", "15": "", "16": "<h3 style=\"color:red;\">Example</h3>\n\nThe next diagram shows the data flow -- messaging and aggregation\ntrees, and the connections between them -- for a system where\n\\(N=4, t=1\\).\n\n<figure>\n<img alt=\"Fig4\" src=\"raw_webcrawl_data/Byzantine/Byzantine_5/Byzantine_5.004.jpeg\" style=\"width:100%\"/>\n<figcaption>Fig.4: Dataflow for 4 agents 1 of which is disloyal</figcaption>\n</figure>\n\n<h3 class=\"w3-text-teal\">Inductive Generation of the Data Flow</h3>\n\nThe basic unit of data flow, which is replicated many times, is shown\nin the top diagram of the figure below.\nThe input to the unit is a node \\(m[L]\\) of the message tree; this\nunit is specified by \\(L\\),and the message \\(m[L]\\).\nThe output of the unit are nodes \\(a[L, x]\\) of the\naggregation tree, for all lieutenants \\(x\\) not in \\(L\\).\n\n<p>\nThe base case of the induction is a node at depth \\(t\\).\nThe input for the base case is \\(m[L]\\) where \\(L\\) is a list starting\nwith \\(0\\) and followed by \\(t\\) lieutenants.\nFor the base case, \\(a[L, x] = m[L, x]\\) for all \\(x\\).\nThe base case is illustrated in the lower diagram.\n\n<figure>\n<img alt=\"Fig5\" src=\"raw_webcrawl_data/Byzantine/Byzantine_5/Byzantine_5.005.jpeg\" style=\"width:100%\"/>\n<figcaption>Fig.5: Structure of Dataflow</figcaption>\n</figure>\n\nThe data flow connecting a message node \\(m[L]\\) at depth \\(d &lt; t\\) to\naggregation nodes \\(a[L,x]\\) is shown below.\n<figure>\n<img alt=\"Fig6\" src=\"raw_webcrawl_data/Byzantine/Byzantine_5/Byzantine_5.006.jpeg\" style=\"width:100%\"/>\n<figcaption>Fig.6: Structure of Dataflow</figcaption>\n</figure>\n\nMessage node \\(m[L]\\) feeds message nodes \\(m[L,x]\\).\nThe connections between \\(m[L,x]\\) and aggregation nodes \\(a[L,x,y]\\)\nare specified by the data flow connecting nodes of depth \\(d + 1\\),\nshown in the diagram by blue dotted lines.\n\n</p><p>\nThe value of an aggregation node is the majority of its inputs.\n</p><p>\n\\(\na[L,i] = \\textrm{majority}(m[L,i], [\\forall x \\notin [L, i]: a[L, x, i]])\n\\)\n</p><p>\nFor example,\n</p><p>\n\\(\na[0, 1, 2] = \\textrm{majority}(m[0,1,2], a[0, 1, 3, 2], a[0, 1, 4, 2],\na[0, 1, 5, 2], \\ldots)\n\\)\n\n\n\n</p><h3 class=\"w3-text-teal\">Proof of Validity</h3>\nWe prove that for all nodes \\(m[L]\\) of the message tree, if \\(L_{*}\\)\nis loyal then for all \\(i\\) not in \\(L\\):\n<p>\n\\(\na[L, i] = m[L]\n\\)\n\n</p><p>\nThe proof is by induction.\nThe base case is for message nodes at depth \\(t\\).\nWe prove that if validity holds for message nodes at depth \\(d &gt; 0\\)\nthen it holds for message nodes at depth \\(d-1\\).\n\n</p><h4 class=\"w3-text-teal\">Base Case</h4>\nSee the lower diagram of figure 5. \nFor a message node \\(m[L]\\) at depth \\(t\\),\n<p>\n\\(\na[L, i] = m[L, i]\n\\)\n</p><p>\nIf \\(L_{*}\\) is loyal then \\(m[L, i] = m[L]\\).\nand the result follows.\n\n\n\n\n</p><h4 class=\"w3-text-teal\">Inductive Step</h4>\n<p>\nSee figure 6.\n\n</p><p>\nA node \\(L\\) at depth \\(d\\) consists of \\(d+1\\) agents.\nTherefore, there are at least \\(3t+1 - (d+1)\\) lieutenants that are not in\n\\(L\\).\nBecause \\(d \\leq t\\) there are at least \\(2t\\) not in \\(L\\).\nSo, at least \\(t\\) lieutenants not in \\(L\\) are loyal and at most\n\\(t\\) of them are disloyal.\n\n</p><p>\nBecause \\(L_{*}\\) is loyal, \\(m[L,i] = m[L]\\).\n</p><p>\nBy the induction assumption, for each loyal lieutenant \\(j\\) not in \\(L\\), and for each loyal\nlieutenant \\(i\\) not in \\([L, j]\\):\n\\(\na[L, j, i] = m[L, i] = m[L]\n\\)\n</p><p>\n\\(\na[L, i] = \\textrm{majority}(m[L,i], [\\forall j \\notin [L, i]: a[L, j,\ni]])\n\\)\n</p><p>\nThe majority is taken over at least \\(t + 1\\) values equal to\n\\(m[L]\\),\nand at most \\(t\\) values that are different from it. And\ntherefore \\(a[L, i] = m[L]\\)\n\n</p><h3 style=\"color:red;\">Example</h3>\n\nThe next illustrates the proof of validity. Message \\(m[L]\\) is\nshown in red, and the flow of correct messages is shown in red edges\nand red nodes. For example, nodes \\(m[L, i], m[L, j], m[L, i, j],\nm[L, j, i]\\) are red because agents \\(i, j\\) are loyal.\n\n<p>\nA disloyal agent is represented by the symbol \\(e\\).\nThe output of a disloyal agent is unknown and is shown in black.\nNode \\(a[l. i]\\) gets more than \\(2t\\) red inputs and at most \\(t\\)\nblack inputs, and hence the majority of its inputs is red.\n\n\n<figure>\n<img alt=\"Fig7\" src=\"raw_webcrawl_data/Byzantine/Byzantine_5/Byzantine_5.007.jpeg\" style=\"width:100%\"/>\n<figcaption>Fig.7: Illustration of Validity</figcaption>\n</figure>\n</p>", "17": "<h3 class=\"w3-text-teal\">Proof of Consensus</h3>\nWe will prove consensus for nodes at depth \\(d\\) if the number of\nfaulty nodes is at most \\(t-d\\).\n\n", "18": "<h4 class=\"w3-text-teal\">Base Case \\(d = t\\)</h4>\nIn this case there are no disloyal lieutenants. For each \\(i, j\\),\n\\(m[L,i]\\) and \\(m[L,j]\\) are arbitrary; however \\(m[L, i] = m[L,\nj]\\).\n<p>\n\\(a[L, i, j] = m[L, i]\\)\nand\n\\(a[L, j, i] = m[L, j]\\).\nTherefore the inputs to \\(a[L,i]\\) and to \\(a[L,j]\\) are identical and\nthe result follows.\n\n</p>"}, "raw_webcrawl_data/SelfStabilization/SelfStabilization.html": {"0": "<h1 class=\"w3-text-teal\">Self Stabilization</h1>\n", "1": "<h4 class=\"w3-text-teal\">\nThis module describes one example of a self-stabilization system --- a\nsystem that recovers automatically from transient errors.\n</h4>\n\n    A <i>self stabilizing</i> system\n    recovers automatically from transient errors.\n    If a self-stabilizing system enters an unsafe state then the\n    system will correct itself and eventually enter a safe state.\n\n    <p>\n    The literature on self stabilization is extensive. Let's look at\n    one example of a self-stabilizing system to get an idea of its\n    design.\n\n    </p>", "2": "<h3 class=\"w3-text-teal\">Self Stabilizing Token Passing</h3>\n    A ring of agents passes a single token around the ring. An\n    agent that holds the token knows that no other agent has the token\n    at that point. So, the system can be used to implement mutual\n    exclusion.\n\n    <p>\n    Examples of errors are the disappearance of the single token and\n    the creation of additional tokens. A self stabilizing algorithm\n    ensures that eventually the system gets back to a safe state,\n    i.e., one in which it has exactly one token.\n\n    </p><p>\n    Let's begin with a model in which each agent in the ring can read\n    the state of its predecessor in the ring. Later, we will modify the\n    algorithm to work with message-passing.\n\n\n    </p><p>\n    \\(N\\) agents, indexed \\(j\\), are organized in a ring where agent\n    \\((j+1) \\: \\textrm{mod} \\: N\\) can read the state of agent\n    \\(j\\). Hereafter, we will not write \"\\(\\textrm{mod} \\; N\\);\" it is\n    to be understood.\n\n    </p><p>\n    An agent is either <i>idle</i> or <i>active</i>. The system is\n    required to have exactly one active process. The active process\n    can be thought of as having the single token in the system.  The\n    token is passed from agent \\(j\\) to agent \\(j+1\\) when agent \\(j\\)\n    becomes idle and agent \\(j+1\\) becomes active.\n\n    </p><p>\n    For convenience in visualizing diagrams, let's assume that the\n    state of an agent is a color. Next we describe the basic algorithm\n    that assumes that errors do not occur; later we will modify the\n    algorithm to obtain a self-stabilizing algorithm that recover from\n    errors.\n\n\n</p><p class=\"w3-text-teal\">The Algorithm</p>\n    The algorithm for agent \\(0\\) is different from that of\n    the other agents. Agent \\(0\\) has the token exactly when its color\n    is <i>the same</i> as that of its predecessor. Any other agent holds\n    the token exactly when its color is <i>different</i> from that of its\npredecessor.\n\n<p>\nAn agent \\(j\\) passes the token to\nagent \\(j+1\\) when agent \\(j\\) changes its color.\nAgent \\(0\\) has the token when all the agents\nin the ring have the same color.\n\n</p><p>\nThe diagram below illustrates an example with 4 agents where an\nagent's color is either red or blue.\nThe diagram shows how the token is passed from\neach agent to its sucessor.\n</p><p>\n</p><p>\nIn the figure on the top left, all the agents are red; so agent 0 has\nthe token. When agent 0 changes its color we get the diagram at the\ntop center. In this diagram, agent 0 is blue and all the other agents\nare red. Agent 0 no longer has the token because its color is\ndifferent from that of its predecessor; however agent 1 does have the token\nbecause its color is different from that of its predecessor. The\nsequence of diagrams shows what happens when the agent holding the\ntoken --- which is the only active agent --- changes its color.\n\n<figure>\n<img alt=\"Fig1\" src=\"raw_webcrawl_data/SelfStabilization/Slide01.jpg\" style=\"width:100%\"/>\n<figcaption>Fig.1: The token is passed by an agent changing color</figcaption>\n</figure>\n</p>", "3": "<h4 class=\"w3-text-teal\">Faults</h4>\nLet's look at the state of a system after a fault occurs.  The next\nset of diagrams shows how errors --- once they occur --- can propagate\nfor ever.  These diagrams show a system with three tokens whereas an\nerror-free system should have exactly one.  The three agents holding\ntokens are shown with large yellow numbers and the agent that does not\nhold a token is shown with a smaller black number.\n\n<figure>\n<img alt=\"Fig2\" src=\"raw_webcrawl_data/SelfStabilization/Slide02.jpg\" style=\"width:100%\"/>\n<figcaption>Fig.2: Errors can propagate forever</figcaption>\n</figure>\n<p>\nThe figure on the top left of the above diagram shows agents 1, 2 and\n3 with tokens because their colors are different from those of their\npredecessors. The next diagram, top right, shows agents 1 and 3 with\ntokens because their colors are different from those of their\npredecessors, and agent 0 with a token because its color is the same\nas that of its predecessor. The transition to the diagram on the top\nright from the one on the top left occurs when agent 3 changes its\ncolor. \n\n</p><p>\nThe sequence of state transitions gets the system to the figure on the\nbottom left which is the same as that on the top left with the colors\nreversed. This cycle of state transitions can repeat forever, with the\nsame system always having three tokens. So, this system is not self\nstabilizing.\n\n</p>", "4": "<h2 class=\"w3-text-teal\">A Self-Stabilizing Algorithm</h2>\n<i>The solution: add more colors!</i>\n<p>\nWe will modify the design to have as many colors as there are\nagents. In our example, we will have 4 colors because it has 4\nagents. The algorithms for all agents, other than agent 0, remains\nunchanged. As before agent 0 has the\ntoken when its color is the same as that of its predecessor and agent 0\nsends the token by changing agent 0's color. The difference in the\nself-stabilizing algorithm is the color to which agent 0 transits.\n\n\n</p><p>\nAssume that the 4 colors are numbered 0, 1, 2, 3. If agent 0's color\nis \\(k\\) it makes a transition by changing its color to \\(k \\:\n\\textrm{mod} \\: N\\).\n\n</p><p>\nThe diagram below gives an example of how agent 0's color changes. Its\nsequence of colors, 0, 1, 2, 3 are red, blue, green, yellow,\nrespectively. The diagram on the top left shows a configuration in\nwhich agent 0 holds a token because its color (green --- number 2) is\nthe same as that of its predecessor. Agent 0 passes the token by\nchanging its color to 3 (yellow), as show on the diagram on the top\nright.\n\n</p><p>\nThe diagram in the middle left shows agent 0 with a token. It passes\nthe token by changing its color from 1 (blue) to 2 (green), as shown\nin the diagram in the middle right.\n\n\n\n<figure>\n<img alt=\"Fig3\" src=\"raw_webcrawl_data/SelfStabilization/Slide03.jpg\" style=\"width:100%\"/>\n<figcaption>Fig.3: Changes in color of agent 0</figcaption>\n</figure>\n</p>", "5": "<h3 class=\"w3-text-teal\">Proof</h3>\nThe proof has the following three ideas that we first describe\ninformally.\n<ol>\n<li>\n  In all states at least one agent holds a token.\n  </li>\n<li>\n  All trajectories from all states lead to a state in which agent 0\n  holds a token.\n  </li>\n<li>\n  A trajectory from a system state in which agent 0's color is\n  different from that of the other agents leads to a state in which\n  all agents have the same color (in \n  which case the system is in a safe state). \n  </li>\n</ol>\n<p class=\"w3-text-teal\">Part 1</p>\nIf all agents have the same color then agent 0 holds a token.\nIf there is more than one color in the ring then there is at least one\nagent, other than agent 0, whose color is different from that of its\npredecessor's; so, that agent holds a token.\n\n<p class=\"w3-text-teal\">Part 2</p>\nAgent 0 and agent \\(n-1\\) will\nget the same color at some point because \nagent 0's color will propagate all the way around the ring unless it\ngets to agent \\(n-1\\) sooner.\n\n\n<p class=\"w3-text-teal\">Part 3</p>\nIf agent 0's color is different from the colors of agents \\(1, \\ldots,\nn-1\\) then the only\ntrajectory that leads to agents 0 and agent \\(n-1\\) having the same\ncolor is for agent 0's color to propagate all the way around the ring.\n\n<p class=\"w3-text-teal\">The system always reaches a safe state</p>\nLet \\(S\\) be any state. Let the \\(C\\) be the set of agent colors in state\n\\(S\\). If \\(C\\) has all \\(N\\) colors, then the color of agent \\(0\\) is\ndifferent from the colors of the other agents, and so the result\nfollows from part 3.\n\n<p>\nIf \\(C\\) has fewer than \\(N\\) colors then the color of each agent\nremains a color in \\(C\\) until agent 0 gets a color that is not in\n\\(C\\). At this point agent 0's color is different from that of the\nother agents and the result follows.\n\n</p>"}, "raw_webcrawl_data/DiffusingComputations/ApplicationsDiffusingComputationSelfTest.html": {"0": "<h1 class=\"w3-text-teal\">Applications of Diffusing Computations: Self Test</h1>\n\n<p>\n    This problem is to apply the\n    <a href=\"ApplicationsDiffusingComputationExample.html\">algorithm in which the initiator takes a\nglobal snapshot and then acquires the global snapshot.</a>\n</p><p class=\"w3-text-teal\">The Problem</p>\n    The system has a set of indivisible indestructible tokens. Tokens\n    are not created. So the number of tokens in the system is a\n    constant.\n\n    <p>\n    Agents may hold tokens and agents may send tokens. So, at any\n    point tokens may be at agents or in transit in\n    channels.\n\n    </p><p>\n    Design an algorithm by which the initiator detects the number of\n    tokens in the system.\n\n    \n</p>", "1": "<h4 class=\"w3-text-teal\">Solution</h4>\n\nUse the algorithm in which the initiator takes a\nglobal snapshot and then acquires the global snapshot.\nThe state of an agent is the number of tokens it holds.\nThe state of a channel is the number of tokens in transit along the\n    channel. The local information \\(x.v\\) that agent \\(x\\) sends to the\n    initiator is the total number of tokens in agent \\(x\\) and in\nincoming channels of agent \\(x\\).\n\n<p>\nWe don't need to prove the correctness of this algorithm because it is\na special case of the algorithm in which the initiator takes and then\nacquires a global snapshot.\n\n    \n</p>"}, "raw_webcrawl_data/DiffusingComputations/DiffusingComputations.html": {"0": "<h1 class=\"w3-text-teal\">Diffusing Computations</h1>\n\n", "1": "<h4 class=\"w3-text-teal\">\nThis module describes diffusing computation algorithms by which an\nagent can learn about the\n    structure of the  network in which the agent operates.\n</h4>\n\n\n    This module describes diffusing computations.\n    An agent can use diffusing computations to learn about the\n    structure of the  network\n    in which the agent operates. For example, an agent can use\n    diffusing computations to determine the number of agents in the\n    network or to determine if the system is deadlocked.\n    \n<p class=\"w3-text-teal\">Data Structures in Distributed Algorithms</p>\n<i>The module shows how data structures\n    play critical roles in distributed algorithms just as they do in\n    sequential algorithms.</i>\n    The algorithm maintains the invariants that\n    define the data structure --- in this case a tree --- even though the structure is modified\nconcurrently by multiple agents.\n\n<p class=\"w3-text-teal\">Nondeterministic Iteration in Sequential and\nDistributed Algorithms</p>\n<i>This module shows how nondeterministic iteration is used in exactly\nthe same way for reasoning about sequential and distributed\nalgorithms.</i> Whether the algorithm operates across multiple agents and\nchannels or is a sequential program is immaterial to reasoning about\nits correctness.\n\n    ", "2": "<h3 class=\"w3-text-teal\">The Problem</h3>\n    In this module we deal with systems in which an agent is either\n    <i>idle</i> or <i>active</i>. An idle agent remains idle until it\n    receives a message at which point it becomes active. An idle agent\n    does not send messages. An active agent may send messages. An\n    active agent may become idle at any time.\n\n    <p>\n    Initially, the system has a single active agent. This agent is\n    called <i>the initiator</i>. Initially all channels are empty.\n\n    The computation terminates exactly when all agents\n    are idle and all channels are empty.\n\n    </p><p>\n    The computation may never\n    terminate. Our first problem is to design an algorithm that\n    enables the initiator to determine \n    that the computation has terminated if it terminates. Later, we\n    will extend this algorithm to enable the initiator to learn about\n    the network.\n\n    </p><p>\n    In this system, for every channel from an agent \\(x\\) to an agent\n    \\(y\\), there is a channel from \\(y\\) to \\(x\\). For any pair \\(x,\n    y\\) of agents there exists at most one channel from \\(x\\) to\n    \\(y\\), and at most one channel from \\(y\\) to \\(x\\).\n                                                                                                      \n    </p><p>\n    An agent \\(y\\) sends an <i>ack</i> (acknowledgment) along channel\n\\((y, x)\\) after receiving a message along channel \\((x, y)\\).\nAn ack is different from a message; so acks aren't acked.\n</p><p>\nInitially all channels are empty: there are no messages or acks in\ntransit along channels.\nLet\n    \\(x.num\\_unacked\\) be the number of \\(x\\)'s unacknowledge\n    messages, i.e, the number of messages that \\(x\\) has sent\n    minus the number of acks that \\(x\\) has received. \nWe can prove the invariant\nthat there are no messages in any of \\(x\\)'s outgoing channels when\n\\(x.num\\_unacked = 0\\).\n\n\n</p>", "3": "<h3 class=\"w3-text-teal\">A Rooted Tree</h3>\nFor an agent to become active there\n must be a chain of messages from the initiator to the\nagent.\nA data structure\nwith paths between the initiator and active agents is a tree,\nrooted at the initiator, and which spans active agents.\n For each agent \\(x\\) let \\(x.parent\\) be either \\(null\\) or an agent\nwhich is\n\\(x\\)'s parent in the tree. Agent \\(x\\) is not on the tree exactly\nwhen \\(x.parent = null\\).\n\n<p>\nWe will prove the following invariants.\n\n</p>", "4": "<h3 class=\"w3-text-teal\">Invariants</h3>\n<ol>\n<li>\n  The tree is rooted at the initiator, i.e.\n  if \\(x.parent \\neq null\\) then the initiator is an\n  <a href=\"https://en.wikipedia.org/wiki/Lowest_common_ancestor\">ancestor</a>\n  of \\(x\\).\n  </li>\n<li>\n  An agent is off the tree exactly when the agent is idle and the\n  agent has no unacknowledged messages:\n  <p>\n  \\(\n  (x.parent = null) \\quad \\equiv \\quad x.idle \\wedge\n  (x.num\\_unacked = 0)\n  \\)\n  </p></li>\n<li>\n  If \\(x\\)'s parent is not \\(null\\) then \\(x\\)'s parent has at least\n  one unacknowledged message.\n  <p>\n  \\(\n  x.parent \\neq null \\quad \\Rightarrow \\quad x.parent.num\\_unacked &gt; 0\n  \\)\n</p></li></ol>\nFrom invariant 3, it follows that an agent has no children if it has\nno unacknowledged messages. So, from invariant 1, if the initiator has\nno unacknowledged messages then all agents, apart from the initiator,\nare idle and have no messages in outgoing channels. Therefore\ncomputation has terminated when\n<p>\n\\(\ninitiator.idle \\wedge (initiator.num\\_unacked = 0)\n\\)\n</p><p>\nSo, the initiator detects that the computation has terminated when the\ninitiator is idle and no unacknowledged messages.\nNext we give an algorithm that maintains the above invariants.\n\n\n\n\n</p>", "5": "<h1 class=\"w3-text-teal\">Program for an agent</h1>\nNext we propose a program for an agent \\(x\\)\nother than the initiator.\n\n\n", "6": "<h4 class=\"w3-text-teal\">Program</h4>\n<pre>\n0: initially:\n      x.parent = null\n      x.idle = True\n      x.num_unacked = 0\n\n1. When x sends a message:\n      x.num_unacked = x.num_unacked + 1\n\n2. When x receives a message from y:\n      x.idle = False\n      if x.parent = null:\n         x.parent = y\n      else\n         send ack to y\n\n3. When x receives an ack:\n      x.num_unacked = x.num_unacked - 1\n      if (x.num_unacked == 0) and x.idle:\n              send ack to x.parent\n              x.parent = null\n\n4. When x becomes idle:\n       x.idle = True\n       if x.num_unacked == 0:\n              send ack to x.parent\n              x.parent = null\n</pre>\n<p class=\"w3-text-teal\">The Initiator</p>\nThe program for the initiator is the same except that initially the\ninitiator is active. Also, to keep the exposition uniform for the\ninitiator and the other agents, we assume that the initiator has a\nparent which is not one of the agents of the network. We call the initiator's parent\n\\(external\\). This agent plays no role other than to keep the proofs\nidentical for the invariant and other agents. \n<pre>\ninitiator.parent = external\ninitiator.idle = False\ninitiator.num_unacked = 0\nexternal.num_unacked = 1\n</pre>\n\n", "7": "<h2 class=\"w3-text-teal\">Proof of Correctness</h2>\n<p class=\"w3-text-teal\">Safety</p>\nThe proof that the invariants are satisfied is carried out by showing\nthat they hold initially and then verifying that\neach of the four commands maintains the invariants. The verification\nstep is straightforward if a bit laborious.\n\n<p class=\"w3-text-teal\">Progress</p>\n<p>\nIf all agents are idle and there are no messages in channels then\nthe underlying computation has terminated.\nWe will prove that if the underlying computation does terminate then\nthe detection algorithm terminates as well, i.e. the tree\nvanishes, and at that point the initiator detects that the computation has\nterminated.\nAfter the underlying computation terminates, the only action that\nexecutes is action 3: receiving an ack. \n\n</p><p>A variant function is\nthe following graph. A\nvertex \\(x\\) is in the graph exactly when \\(x.parent\\) is not null or\nthere is an ack in transit along the channel from \\(x\\) to its parent.\nDefine a partial order \\(&lt;\\) between graphs as \\(G \\leq G'\\)\nwhen \\(G\\) is a subgraph of \\(G'\\). This graph is a tree because the\nonly pending acks are from a vertex to its parent (rule 3).\n\n</p><p>\nNext we prove that every execution of an action while the variant\nfunction is not the empty graph reduces the variant function.\nWhen all acks are delivered to \\(y\\), it sends an ack to its parent.\nSo while the tree is not empty there is an ack in some channel.\nWhen \\(y\\) receives an ack from \\(x\\), the edge \\((x, y)\\) is\ndeleted from the tree. Therefore every\nexecution of action 3 reduces the variant function.\n\n</p>"}, "raw_webcrawl_data/DiffusingComputations/DiffusingComputationsSelfTest.html": {"0": "<h1 class=\"w3-text-teal\">Diffusing Computations Self Test</h1>\n\n\n", "1": "<h3 class=\"w3-text-teal\">Question 1</h3>\n\n    In this module we differentiate between acks and messages. An ack\n    travels along channels exactly like a message does; however, we\n    use acks only to acknowledge messages. So acks are not themselves\n    acknowledged. \n\n    \n    <p class=\"w3-text-teal\">Part a</p>\n    \n    When agent \\(x\\) gets a message from agent \\(y\\) then which of the\n    following is True\n    <ol>\n<li>\n      At that point agent \\(y\\) is on the tree, i.e., \\(y.parent \\neq\n      null\\)\n      </li>\n<li>\n      At that point agent \\(y\\) is not on the tree, i.e., \\(y.parent =\n      null\\)\n      </li>\n</ol>\n<p class=\"w3-text-teal\">Part b</p>\n    \n    When agent \\(x\\) gets a message from agent \\(y\\) then what action\n    does \\(x\\) take if \\(x.parent = null\\)?\n    \n    \n    <p class=\"w3-text-teal\">Part c</p>\n    \n    When agent \\(x\\) gets a message from agent \\(y\\) then what action\n    does \\(x\\) take if \\(x.parent \\neq null\\)?\n    \n    \n    <p class=\"w3-text-teal\">Part d</p>\n    If <code>x.num_unacked = 0</code> then which of the following is\n    True?\n    <ol>\n<li>\n      There are no messages in \\(x\\)'s outgoing channels.\n      </li>\n<li>\n      \\(x\\) has no descendants.\n      </li>\n<li>\n      There are no acks in \\(x\\)'s incoming channels\n      </li>\n</ol>\n<p class=\"w3-text-teal\">Part e</p>\n    True or False?\n    <p>\n    If \\(x.parent = y\\), then \\(x\\) has received more messages from\n    \\(y\\) than \\(x\\) has sent acks to \\(y\\).\n\n    </p><p class=\"w3-text-teal\">Part f</p>\n    True or False?\n    <p>\n    If \\(x.parent = y\\), and \\(w\\) is different from \\(x\\) and \\(y\\),\n    then \\(x\\) has received more messages from \\(w\\) than \\(x\\) has\n    sent acks to \\(w\\). \n\n    \n</p>", "2": ""}, "raw_webcrawl_data/DiffusingComputations/ScenarioActiveIdle.html": {"0": "<h1 class=\"w3-text-teal\">A Scenario of Active &amp; Idle Agents</h1>\n<figure>\n<img alt=\"Fig3\" src=\"raw_webcrawl_data/DiffusingComputations/DiffusingComputations/Slide04.jpg\" style=\"width:80%\"/>\n<figcaption>\n       Fig. 3. Scenario: Initial Step\n    </figcaption>\n</figure>\n<figure>\n<img alt=\"Fig4\" src=\"raw_webcrawl_data/DiffusingComputations/DiffusingComputations/Slide05.jpg\" style=\"width:80%\"/>\n<figcaption>\n       Scenario\n    </figcaption>\n</figure>\n<figure>\n<img alt=\"Fig5\" src=\"raw_webcrawl_data/DiffusingComputations/DiffusingComputations/Slide06.jpg\" style=\"width:80%\"/>\n<figcaption>\n       Scenario\n    </figcaption>\n</figure>\n<figure>\n<img alt=\"Fig6\" src=\"raw_webcrawl_data/DiffusingComputations/DiffusingComputations/Slide07.jpg\" style=\"width:80%\"/>\n<figcaption>\n       Scenario\n    </figcaption>\n</figure>\n<figure>\n<img alt=\"Fig7\" src=\"raw_webcrawl_data/DiffusingComputations/DiffusingComputations/Slide08.jpg\" style=\"width:80%\"/>\n<figcaption>\n       Scenario\n    </figcaption>\n</figure>\n<figure>\n<img alt=\"Fig8\" src=\"raw_webcrawl_data/DiffusingComputations/DiffusingComputations/Slide09.jpg\" style=\"width:80%\"/>\n<figcaption>\n       Scenario\n    </figcaption>\n</figure>\n<figure>\n<img alt=\"Fig9\" src=\"raw_webcrawl_data/DiffusingComputations/DiffusingComputations/Slide10.jpg\" style=\"width:80%\"/>\n<figcaption>\n       Scenario\n    </figcaption>\n</figure>\n<figure>\n<img alt=\"Fig10\" src=\"raw_webcrawl_data/DiffusingComputations/DiffusingComputations/Slide11.jpg\" style=\"width:80%\"/>\n<figcaption>\n       Scenario\n    </figcaption>\n</figure>\n<figure>\n<img alt=\"Fig11\" src=\"raw_webcrawl_data/DiffusingComputations/DiffusingComputations/Slide12.jpg\" style=\"width:80%\"/>\n<figcaption>\n       Scenario\n    </figcaption>\n</figure>\n<figure>\n<img alt=\"Fig12\" src=\"raw_webcrawl_data/DiffusingComputations/DiffusingComputations/Slide13.jpg\" style=\"width:80%\"/>\n<figcaption>\n       Scenario\n    </figcaption>\n</figure>\n<figure>\n<img alt=\"Fig13\" src=\"raw_webcrawl_data/DiffusingComputations/DiffusingComputations/Slide14.jpg\" style=\"width:80%\"/>\n<figcaption>\n       Scenario\n    </figcaption>\n</figure>\n<figure>\n<img alt=\"Fig14\" src=\"raw_webcrawl_data/DiffusingComputations/DiffusingComputations/Slide15.jpg\" style=\"width:80%\"/>\n<figcaption>\n       Scenario\n    </figcaption>\n</figure>\n<figure>\n<img alt=\"Fig15\" src=\"raw_webcrawl_data/DiffusingComputations/DiffusingComputations/Slide16.jpg\" style=\"width:80%\"/>\n<figcaption>\n       Scenario\n    </figcaption>\n</figure>\n<figure>\n<img alt=\"Fig16\" src=\"raw_webcrawl_data/DiffusingComputations/DiffusingComputations/Slide17.jpg\" style=\"width:80%\"/>\n<figcaption>\n       Scenario\n    </figcaption>\n</figure>\n\n"}, "raw_webcrawl_data/DiffusingComputations/ApplicationsDiffusingComputation.html": {"0": "<h1 class=\"w3-text-teal\">Applications of Diffusing\n    Computation</h1>\n", "1": "<h4 class=\"w3-text-teal\">\nThis module describes diffusing computation algorithms by which an\nagent learns properties of the network in which the agent is situated.\n</h4>\n\n    Diffusing computations are used by an agent to learn properties of\n    the network in which the agent is embedded. We will first look at\n    cases in which the network is static and then study an algorithm\n    in which an agent learns properties of the network while the\n    network is changing.\n    <p>\n    In this problem,  agents are in a static network. If there is an edge\n    from an agent \\(x\\) to an agent \\(y\\), in the network,  then there is an edge in the\n    reverse direction.\n\n    \n</p>", "2": "<h2 class=\"w3-text-teal\">Acquiring Graph Information</h2>\n    Associated with each agent \\(x\\) is some constant value, \\(x.v\\),\n    where \\(x.v\\) is arbitrary. For example \\(x.v\\) could\n    be: the coordinates of agent \\(x\\) in 3D space, or agent \\(x\\)'s\n    lists of assets and liabilities, or the files for which it has\n    exclusive locks.\n    <p>\n    An agent, called the\n    <i>initiator</i>, starts a computation by which it learns\n    the values of all the agents in the network. When the the\n    computation terminates, the initiator will have a set\n    \\(initiator.values\\) where the elements of the set\n    are pairs \\((x, x.v)\\), with one element for each agent in the system.\n\n    </p><p>\n    We first give the program and then prove its correctness. The\n    program is a simple version of a diffusing computation. The simplification\n    comes from the facts that:\n    </p><ol>\n<li>\n<i>Each agent becomes active only when it receives its first\n      message.</i> It doesn't change its active/idle state when it\n      receives subsequent messages.\n      So each agent changes from idle to active exactly once.\n      </li><li>\n      After an agent sets its parent to a non-null value, it does not\n      change its parents thereafter. So, the tree grows, and once an\n      edge of the tree is created that edge is never changed.\n      </li>\n<li>\n<i>For each channel \\(c\\), exactly one message and exactly one\n      ack traverses the channel in the algorithm</i>\n</li>\n</ol>\n<p>\n    By contrast, in a general diffusing computation an agent may\n    become active multiple times because each time an idle agent receives a\n    message it becomes active. In the general case, edges of the tree\n    are created and may be later deleted.\n\n</p><p>\nAssociated with each agent \\(x\\) is a local boolean variable\n\\(x.received\\_all\\_acks\\) which is True exactly when agent \\(x\\) has\nreceived acks for all the messages that it has sent.\n</p><p>\nEach ack has a field called <i>values</i> which is a set. The elements\n    of this set are pairs \\((x, x.v)\\) where \\(x\\) is an agent.\n    </p>", "3": "<h3 class=\"w3-text-teal\">Simpler Version</h3>\n\n    Initially, \\(x.parent = null\\) for all agents \\(x\\) other than the\n    initiator. \n<p>\n    The initiator starts the computation by executing the following\nprogram:\n</p><pre>\ninitiator.values = {(initiator, initiator.v)}\nsend a message to each neighbor of the initiator\n</pre>\n\n    Next we give the program for an agent \\(x\\)\nother than the initiator.\n    \n<pre>\n\n1: if (x.parent == null) and (x receives a message from y):\n       x.parent = y\n       // x.parent does not change from now onwards.\n       x.values = {(x, x.v)}\n       send a message to each neighbor of x other than x.parent\n\n2: if (x.parent != null) and (x receives a message from y):\n          send ack(values = {}) to y\n\n3: if x receives an ack\n          // set x.values to the union of x.values and ack.values.\n          x.values = union(x.values, ack.values)\n          if x.received_all_acks:\n               send ack(values=x.values) to x.parent\n</pre>\n", "4": "<h3 class=\"w3-text-teal\">Proof outline</h3>\nThe steps of the proof are as follows:\n<ol>\n<li>\n  An agent \\(x\\) changes \\(x.parent\\) exactly once from \\(null\\) to a\n  non-\\(null\\) value.\n  The initiator is the ancestor of all agents \\(x\\) for which\n  \\(x.parent\\) is not null.\n  The tree propagates to span all agents reachable from the initiator.\n  </li>\n<li>\n  For each agent \\(x\\), its value \\((x, x.v)\\) is propagated exactly once\n  to its parent. This value is then propagated along edges of\n  the tree through ancestors of \\(x\\) to the initiator.\n  </li>\n</ol>\n", "5": "<h2 style=\"color:red;\">Examples</h2>\nThe algorithm can be optimized for problems in\nwhich the initiator has to discover different kinds of information\nabout graphs. For example, if agents have colors and the initiator has\nto discover the numbers of agents of each color, then <i>values</i>\ncan be counts of agents of each color, and counts can be summed\ninstead of taking unions of sets.\n\n", "6": "<h4 style=\"color:red;\">Example</h4>\nNext, we look at an example of an algorithm in which an\nagent acquires a specific kind of graph information. \nAn agent has a color, either red or blue, and the color does not\nchange. The \ninitiator has to discover whether there exists at least one red agent\nin the network.\nAn optimization is that if the diffusing computation reaches a red\nagent then the red agent does not diffuse the\ncomputation even further. We give the optimized algorithm, <i>program\n2</i>, below. It's proof of correctness is similar to the proof of\nprogram  given above.\n\n", "7": "<h3 style=\"color:red;\">Program 2: Detecting a red agent</h3>\n<pre>\n1: if (x.parent == null) and  (x receives a message from y):\n        x.parent = y\n        if x.color == red:\n             send ack(color=red) to y\n        else:\n             send a message to all neighbors of x other than y\n\n2: if (x.parent != null) and x receives a message from y:\n          send ack(color=x.color) to y\n\n3: if (x receives an ack A) and (x has not sent an ack to x.parent):\n        if A.color == red:\n              send ack(color=red) to x.parent\n        elif x.received_all_acks:\n              send ack(color=blue) to x.parent\n</pre>\n<p>\nIf the initiator receives at least one red ack then the network has a\nred agent. If the initiator receives only blue acks then the network\nhas no red agent.\n\n</p>", "8": "<h2 class=\"w3-text-teal\">Termination Detection Revisited</h2>\nWe described the problem of detecting that a computation has terminated in\n<a href=\"../GlobalSnapshotApplications/GlobalSnapshotApplications.html\">\nthe module on global snapshot applications.</a> We also described an\nalgorithm to solve that problem in the module. Now we give another\nalgorithm to solve the same problem. This algorithm combines diffusing\ncomputation and the snapshot algorithm, and is a small modification of\n<i>program 2</i>. \n\n", "9": "<h5 class=\"w3-text-teal\">Problem Specification</h5>\n<p>\nFor convenience, the problem definition from the earlier\nmodule is repeated here.\n\n    </p><p>\n    An agent is either <i>idle</i> or <i>active</i>. An idle agent\n    remains idle until it receives a message. An idle agent does not\n    send messages. An active agent may send messages. An active agent\n    may become idle at any point. An idle agent becomes active when it\n    receives a message.\n    Initially all agents are active and all channels are empty.\n\n    </p><p>\n    A <i>terminated</i> state of the system is one in which all agents\n    are idle and all channels are empty. <i>terminated</i> is stable:\n    a system in a terminated state remains terminated thereafter. A\n    system may or may not enter a terminated state.\n\n    Our task is to develop an algorithm that detects that a system has\nentered a terminated state.\n\n</p>", "10": "<h5 class=\"w3-text-teal\">Algorithm</h5>\nIn analogy with <i>program 2</i>, let's give colors to agents.\nLet's specify that an agent that is\nidle throughout the snapshot algorithm is colored blue. An agent that\nis active at any point during the snapshot algorithm is colored red. The initiator\nhas to discover whether any agent is red.\n\n<p>\nThe differences between\ntermination detection and the problem of detecting a red agent\nare that (1) <i>agent\ncolors may change</i> during termination detection, and (2)\ntermination detection has to detect properties of channels --- namely\nwhether channels are empty --- in addition to detecting properties of\nagents. A surprising\nresult is that program 2, with the addition of a statement to deal\nwith channels, can be\nused for termination detection.\n\n</p><p>\nIn the modified algorithm, the initiator takes its local snapshot when\nit initiates the algorithm. All \nother agents take their local snapshot when they set their parents to\nnon-null values.\n\n</p><p>\nThe statement that we add is:\n\n</p><pre>\n4: if (x.parent != null) and x receives a message:\n          if x has not sent an ack to x.parent:\n              send ack(color=red) to x.parent\n</pre>\n", "11": "<h4 class=\"w3-text-teal\">Proof outline</h4>\nConsider the following modification to the snapshot algorithm: replace \nthe <i>ack</i> and <i>message</i> of program 2 by\nmarkers of the snapshot algorithm. The resulting program is identical\nto the snapshot algorithm with two differences:\n<ol>\n<li>\n  An ack in program 2 may be sent later than a marker is sent in a\n  snapshot algorithm. In the snapshot algorithm, when an agent receives a\n  marker for the first time it sends markers immediately.\n  In the modified\n  algorithm, however, an agent may not immediately send an ack when it\n  receives a command. This is because of the <i>elif</i> clause in\n  statement 3: an agent sends an ack only after receiving acks for all the\n  messages that it sent.\n  </li>\n<li>\n  The snapshot algorithm takes snapshots of channels as well as\n  agents. The modified algorithm doesn't mention channels.\n  </li>\n</ol>\n<p>\nNext, we address these two differences.\n\n</p><p>\nLate arrivals of markers don't matter for the purposes of detecting\ntermination. If computation has terminated then agents will be idle\nwhen markers arrive, regardless of when they arrive. Likewise, the\nalgorithm records channel states as empty, regardless of when markers\narrive.\n\n</p><p>\nIf \\(x\\) receives no messages after it takes its local snapshot and\nbefore receiving markers on all its incoming channels then all its\nincoming channels are empty. Statement 4 identifies the computation as\nnot terminated when \\(x\\) receives a message.\n\n</p>", "12": "", "13": "<h5 class=\"w3-text-teal\">Problem Specification</h5>\n<p>\nFor convenience, the problem definition from the earlier\nmodule is repeated here.\n\n    </p><p>\n    An agent is either <i>idle</i> or <i>active</i>. An idle agent\n    remains idle until it receives a message. An idle agent does not\n    send messages. An active agent may send messages. An active agent\n    may become idle at any point. An idle agent becomes active when it\n    receives a message.\n    Initially all agents are active and all channels are empty.\n\n    </p><p>\n    A <i>terminated</i> state of the system is one in which all agents\n    are idle and all channels are empty. <i>terminated</i> is stable:\n    a system in a terminated state remains terminated thereafter. A\n    system may or may not enter a terminated state.\n\n    Our task is to develop an algorithm that detects that a system has\nentered a terminated state.\n\n</p><h5 class=\"w3-text-teal\">Algorithm</h5>\nIn analogy with <i>program 2</i>, let's give colors to agents.\nLet's specify that an agent that is\nidle throughout the snapshot algorithm is colored blue. An agent that\nis active at any point during the snapshot algorithm is colored red. The initiator\nhas to discover whether any agent is red.\n\n<p>\nThe differences between\ntermination detection and the problem of detecting a red agent\nare that (1) <i>agent\ncolors may change</i> during termination detection, and (2)\ntermination detection has to detect properties of channels --- namely\nwhether channels are empty --- in addition to detecting properties of\nagents. A surprising\nresult is that program 2, with the addition of a statement to deal\nwith channels, can be\nused for termination detection.\n\n</p><p>\nIn the modified algorithm, the initiator takes its local snapshot when\nit initiates the algorithm. All \nother agents take their local snapshot when they set their parents to\nnon-null values.\n\n</p><p>\nThe statement that we add is:\n\n</p><pre>\n4: if (x.parent != null) and x receives a message:\n          if x has not sent an ack to x.parent:\n              send ack(color=red) to x.parent\n</pre>\n<h4 class=\"w3-text-teal\">Proof outline</h4>\nConsider the following modification to the snapshot algorithm: replace \nthe <i>ack</i> and <i>message</i> of program 2 by\nmarkers of the snapshot algorithm. The resulting program is identical\nto the snapshot algorithm with two differences:\n<ol>\n<li>\n  An ack in program 2 may be sent later than a marker is sent in a\n  snapshot algorithm. In the snapshot algorithm, when an agent receives a\n  marker for the first time it sends markers immediately.\n  In the modified\n  algorithm, however, an agent may not immediately send an ack when it\n  receives a command. This is because of the <i>elif</i> clause in\n  statement 3: an agent sends an ack only after receiving acks for all the\n  messages that it sent.\n  </li>\n<li>\n  The snapshot algorithm takes snapshots of channels as well as\n  agents. The modified algorithm doesn't mention channels.\n  </li>\n</ol>\n<p>\nNext, we address these two differences.\n\n</p><p>\nLate arrivals of markers don't matter for the purposes of detecting\ntermination. If computation has terminated then agents will be idle\nwhen markers arrive, regardless of when they arrive. Likewise, the\nalgorithm records channel states as empty, regardless of when markers\narrive.\n\n</p><p>\nIf \\(x\\) receives no messages after it takes its local snapshot and\nbefore receiving markers on all its incoming channels then all its\nincoming channels are empty. Statement 4 identifies the computation as\nnot terminated when \\(x\\) receives a message.\n\n</p><h2 class=\"w3-text-teal\">Communication Deadlock Detection</h2>\nCommunication deadlock is identical to termination detection with one\ndifference. An idle agent in \ntermination detection becomes active if it receives a message from <i>any</i>\nagent. By contrast, in the communication deadlock problem, an idle\nagent becomes active only if receives <i>a message from a set of agents\nfor which it is waiting.</i>\n\nAn earlier\n<a href=\"../DeadlockDetection_And/DeadlockDetection_And.html\">module\non deadlock detection</a> developed algorithms to detect deadlock when\nan idle agent became active only if it received messages or resources\nfrom <i>all</i> the agents for which it is waiting. In the\ncommunication-deadlock case, an\n idle agent became active if it receives messages\nfrom <i>any</i> of the agents for which it is waiting.\n\n<h5 class=\"w3-text-teal\">Problem Specification</h5>\nAn agent is either <i>active</i> or <i>waiting</i>.\nAssociated with each waiting agent \\(x\\) is a non-empty set\n\\(x.waits\\) of agents;\n\\(x\\) becomes active if it receives a message\nfrom any agent in \\(x.waits\\), and \\(x\\) remains waiting if it\nreceives a message from an agent that is not in \\(x.waits\\).\nA waiting agent does not send messages.\nActive agents may send messages.\n\n", "14": "<h5 class=\"w3-text-teal\">The Algorithm</h5>\nThe algorithm for communication deadlock detection is the same as that\nfor termination detection except that the network is restricted to\nagents that are waiting. The network has a channel between a pair of\nagents if either agent in the pair is waiting for the other.\n\n"}, "raw_webcrawl_data/DiffusingComputations/ApplicationsDiffusingComputationExample.html": {"0": "<h1 class=\"w3-text-teal\">Examples: Combining Algorithms</h1>\n\n    These examples show how to <i>compose</i> or combine\n    algorithms.\n\n    \n", "1": "<h2 class=\"w3-text-teal\">Snapshot Termination Detection</h2>\n\n\n    This algorithm combines the global snapshot algorithm and\n    diffusing computations to get an algorithm that is initiated by a\n    <i>single agent which detects when the snapshot algorithm has\n    terminated</i>.\n    \n    <p class=\"w3-text-teal\">Single initiator of snapshot</p>\n<a href=\"https://kmchandy.github.io/DiffusingComputations/DiffusingComputations.html\"> \n    An earlier module gave the rules for the global snapshot\n    algorithm. </a>\n    These rules allow multiple agents to start recording\n    their states either independently or when they receive markers\n    from other agents who have recorded their own states. Now,\n    let's design an algorithm in which a single agent, called the\n    <i>initiator</i> is the only agent that records its state\n    independently, and all other agents record their \n    states only when they receive markers.\n    \n    <p class=\"w3-text-teal\">Initiator detects termination of snapshot</p>\n\n    A global snapshot is complete when all agents have recorded their\n    own states and the states of all incoming channels.\n    The rules for the snapshot algorithm don't specify how an agent\n    detects that the global snapshot is complete.\n    Now let's look at an algorithm by which the initiator\n    detects that the snapshot algorithm is over.\n    In this algorithm the initiator does not get the local snapshots\n    of all the agents; the initiator merely detects that the states of\n    all agents and channels have been recorded.\n\n    \n\n    \n", "2": "<h4 class=\"w3-text-teal\">Problem Specification</h4>\n\n<p>\n    A system is as specified in diffusing computations. There is a\n    channel from agent \\(x\\) to agent \\(y\\) if and only there is a\n    channel from \\(y\\) to \\(x\\). All agents are reachable, by a\n    sequence of channels from an agent called the <i>initiator.</i>\n</p><p>\n    Modify the global snapshot algorithm so that it is started by a\n    single agent which detects that the algorithm has\n    terminated. \n\n    \n</p>", "3": "<h4 class=\"w3-text-teal\">Solution: Combine Algorithms</h4>\n\nThe solution is a diffusing computation algorithm which takes global\nsnapshots.\n<p>\nA message in the diffusing computation is a marker in the\nsnapshot algorithm.\nWhen a message (i.e. marker) is\n    received by an idle agent the agent becomes active, sends messages\n    (i.e., markers)\nspecified by the snapshot algorithm, and becomes idle.\n\n</p><p>\nThe diffusing computation terminates when all agents are idle and all\nchannels are empty.\nSo the computation terminates when each agent has completed its step\n    of the snapshot algorithm, and when all markers have been\nreceived.\nThis implies that at termination the states of all\nagents and channels have finished\nbeing recorded.\n\n\n\n\n    \n</p>"}}
{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "448cad6e-3a22-484a-872d-920e1269999f",
   "metadata": {},
   "source": [
    "# Data preprocessing\n",
    "## Exp1: Code doesnt exist anymore. I overwrote it with Exp2 code.\n",
    "Tried to load FAQ text and Course text in separate search DB, without any preprocessing. FAQ questions were not being answered correctly. Due to poor search results, and mismatch in chunking text.\n",
    "## Exp2: Code below\n",
    "FAQ data: Preprocessed FAQ documents to collect QnA pairs using GPT. Loaded only the Q's into search. When a student asks Q, we search in the Q's DB, and personalize the answer using GPT based on the student's question.\n",
    "Course data: Loaded in search engine as overlapping chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72c59997-8f16-4286-9b8c-0637ec3d105e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the model required for search\n",
    "from langchain.embeddings.huggingface import HuggingFaceEmbeddings\n",
    "embeddings = HuggingFaceEmbeddings(model_name=\"all-MiniLM-L6-v2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a5be074-1308-4b65-932d-cc7c0ed38a98",
   "metadata": {},
   "outputs": [],
   "source": [
    "# FAQ parser class\n",
    "from langchain.schema import (\n",
    "    AIMessage,\n",
    "    HumanMessage,\n",
    "    SystemMessage\n",
    ")\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.output_parsers import PydanticOutputParser, OutputFixingParser\n",
    "from pydantic import BaseModel, Field\n",
    "from enum import Enum\n",
    "from langchain_openai import ChatOpenAI\n",
    "from typing import List\n",
    "\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "class QnA(BaseModel):\n",
    "    question: str = Field(description=\"one specific question from the given 'faq document'\")\n",
    "    answer: str = Field(description=\"answer to the above question from the given 'faq document'. Do not generate an answer, simply copy-paste the entire the text of the answer as-is.\")\n",
    "\n",
    "class QnAList(BaseModel):\n",
    "    faq: List[QnA] = Field(description=\"list all the question and answer pairs from the given 'document'\")\n",
    "\n",
    "class FAQProcessor():\n",
    "    def __init__(self):\n",
    "        \n",
    "        self.model = ChatOpenAI(\n",
    "            model_name='gpt-3.5-turbo',\n",
    "            openai_api_key=os.getenv(\"OPENAI_API_KEY\"),\n",
    "            openai_organization=os.getenv(\"OPENAI_ORGANIZATION\"),\n",
    "        )\n",
    "\n",
    "        self.parser = PydanticOutputParser(pydantic_object=QnAList)\n",
    "        self.fix_parser = OutputFixingParser.from_llm(parser=self.parser, llm=self.model)\n",
    "        self.prompt = PromptTemplate(\n",
    "            template =  '''\n",
    "                You are a bot helping with text parsing. \n",
    "                Given an 'FAQ document', parse the list of question and answer pairs.\\n\n",
    "                The 'FAQ document' can be noisy, with some unrelated text, make sure to ignore this unrealted text.\n",
    "                \n",
    "                {format_instructions}\\n\n",
    "                \n",
    "                ***\n",
    "                'FAQ document' : {faq_doc}\n",
    "                ***\n",
    "\n",
    "\n",
    "                I am reminding you again, do not add any new questions or facts in asnwers that are not given in the 'FAQ document'.\n",
    "            ''',\n",
    "                input_variables=[\"faq_doc\"],\n",
    "                partial_variables={\n",
    "                    \"format_instructions\": self.parser.get_format_instructions(),\n",
    "                },\n",
    "            )    \n",
    "        self.search_conf_thresh = 1\n",
    "        \n",
    "    def parse(self, faq_doc):\n",
    "        response = self.model([HumanMessage(self.prompt.format_prompt(faq_doc=faq_doc).to_string()) ])\n",
    "\n",
    "        response_output = None\n",
    "        try:\n",
    "            response_output = self.parser.parse(response.content)\n",
    "        except Exception as e:\n",
    "            response_output = self.fix_parser.parse(response.content)                \n",
    "        return response_output\n",
    "        \n",
    "fp = FAQProcessor()        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4657353f-7583-4011-9551-78a3c3e5fe72",
   "metadata": {},
   "outputs": [],
   "source": [
    "#load all the contents of the course\n",
    "from langchain.document_loaders import DirectoryLoader\n",
    "from langchain.document_loaders import TextLoader\n",
    "from langchain.docstore.document import Document\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from bs4 import BeautifulSoup, Comment  \n",
    "import re\n",
    "import os\n",
    "import json\n",
    "import csv\n",
    "import shutil\n",
    "import logging \n",
    "def remove_html_tags(text):\n",
    "    clean_text = re.sub(r'<.*?>', '', text)\n",
    "    return clean_text  \n",
    "    \n",
    "def remove_html_comments(html_content):\n",
    "    # Parse the HTML using BeautifulSoup\n",
    "    soup = BeautifulSoup(html_content, 'html.parser')\n",
    "    \n",
    "    # Find all comment nodes and remove them\n",
    "    comments = soup.find_all(text=lambda text: isinstance(text, Comment))\n",
    "    for comment in comments:\n",
    "        comment.extract()\n",
    "    \n",
    "    # Get the HTML content without comments\n",
    "    html_content_without_comments = str(soup)\n",
    "    \n",
    "    return html_content_without_comments\n",
    "\n",
    "def split_to_sections(html_content):\n",
    "    # Parse the HTML using BeautifulSoup\n",
    "    soup = BeautifulSoup(html_content, 'html.parser')\n",
    "    \n",
    "    # Find all headers and their corresponding sections\n",
    "    header_list = []\n",
    "    index_list = []\n",
    "    for header in soup.find_all(['h1', 'h2', 'h3', 'h4', 'h5', 'h6']):    \n",
    "        header_list.append(header)\n",
    "        index_list.append(html_content.find(str(header)))\n",
    "    \n",
    "    sections = []\n",
    "    for ind, index in enumerate(index_list[:-1]):\n",
    "        sections.append(remove_html_comments(html_content[index:index_list[ind+1]]))\n",
    "    return sections\n",
    "\n",
    "def extract_image_info(html):\n",
    "    # Parse HTML\n",
    "    soup = BeautifulSoup(html, 'html.parser')\n",
    "\n",
    "    # Find all image tags\n",
    "    img_tags = soup.find_all(['img', 'figure'])\n",
    "\n",
    "    # Extract image URLs and titles\n",
    "    image_data = []\n",
    "    used_img_src = set()\n",
    "    for tag in img_tags:\n",
    "        if tag.name == 'img' and tag.get('src') not in used_img_src:\n",
    "            url = tag.get('src')\n",
    "            title = tag.get('alt', '')\n",
    "            image_data.append({'url': url, 'title': title})\n",
    "            used_img_src.add(url)\n",
    "            # Remove image tags from HTML\n",
    "            tag.extract()\n",
    "        elif tag.name == 'figure':\n",
    "            figcaption = tag.find('figcaption')\n",
    "            if figcaption:\n",
    "                title = figcaption.get_text()\n",
    "            else:\n",
    "                title = ''\n",
    "            img_tag = tag.find('img')\n",
    "            if img_tag and img_tag.get('src') not in used_img_src:\n",
    "                url = img_tag.get('src')\n",
    "                image_data.append({'url': url, 'title': title})\n",
    "                used_img_src.add(url)\n",
    "                # Remove image tags from HTML\n",
    "                tag.extract()\n",
    "\n",
    "    # Get modified HTML text\n",
    "    modified_html = str(soup)\n",
    "\n",
    "    return image_data, modified_html\n",
    "\n",
    "# Read the HTML file\n",
    "def read_html(html_filename):\n",
    "    html_content = None\n",
    "    with open(html_filename, 'r', encoding='utf-8') as f:\n",
    "        html_content = f.read()\n",
    "    return html_content\n",
    "\n",
    "def parse_html(html_content):\n",
    "    sections_with_images = split_to_sections(html_content)\n",
    "\n",
    "    sections_without_images = []\n",
    "    # Example usage\n",
    "    for section in sections_with_images:\n",
    "        image_info, html_without_images = extract_image_info(section)\n",
    "        sections_without_images.append('\\n{}'.format(html_without_images))\n",
    "        # [TODO] Solve for images later\n",
    "        # for info in image_info:\n",
    "        #     print(\"\\t\\tImage Info$$$$$$$$$$$$$:\")\n",
    "        #     print(\"\\t\\tURL:\", info['url'])\n",
    "        #     print(\"\\t\\tTitle:\", info['title'])\n",
    "    return sections_with_images, sections_without_images\n",
    "\n",
    "\n",
    "# location of the coursework. Expects a list of text files in this location.\n",
    "course_dir = '''raw_webcrawl_data'''\n",
    "loader = DirectoryLoader(course_dir, glob=\"**/*.html\", loader_cls=TextLoader)\n",
    "documents = loader.load()\n",
    "print('Loaded {} documents'.format(len(documents)))\n",
    "\n",
    "## output\n",
    "faq_processed_dir = '{}_faq_processed'.format(course_dir)\n",
    "os.makedirs(faq_processed_dir, exist_ok=True)\n",
    "\n",
    "\n",
    "#save the full text in a different DB for QnA on it\n",
    "all_sections = []\n",
    "all_sections_html = {}\n",
    "course_db = {}\n",
    "qna_dict = {}\n",
    "docs_procesessed_cnt = 0\n",
    "section_procesessed_cnt = 0\n",
    "qna_cnt = 0\n",
    "faq_cnt = 0\n",
    "for doc_cnt, doc in enumerate(documents):\n",
    "    if 'faq' not in documents[doc_cnt].metadata['source'] and 'FAQ' not in documents[doc_cnt].metadata['source']:\n",
    "        sections_with_images, sections_without_images = parse_html(documents[doc_cnt].page_content)    \n",
    "        docs_procesessed_cnt +=1\n",
    "        for section_cnt, section in enumerate(sections_without_images):\n",
    "            # save text without html tags for retrieval purposes. embedding model doesnt do good job with html tags\n",
    "            all_sections.append(Document(page_content=remove_html_tags(section), metadata={\"source\": documents[doc_cnt].metadata['source'], \"split\":section_cnt}))\n",
    "\n",
    "            # save text in html format for equations, since equations are written with html+latex tags. ChatGPT does a good job reading and comprehending this.\n",
    "            if documents[doc_cnt].metadata['source'] not in course_db:\n",
    "                course_db[documents[doc_cnt].metadata['source']] = {}\n",
    "            if section_cnt not in course_db[documents[doc_cnt].metadata['source']]:\n",
    "                course_db[documents[doc_cnt].metadata['source']][section_cnt] = sections_with_images[section_cnt]\n",
    "                section_procesessed_cnt+=1\n",
    "    else:\n",
    "        # load document, and compute QnA pairs using gpt\n",
    "        faq_list = fp.parse(documents[doc_cnt].page_content)\n",
    "        _, faq_file = os.path.split(documents[doc_cnt].metadata['source'])\n",
    "        faq_cnt += 1\n",
    "        with open(os.path.join(faq_processed_dir, faq_file.replace('.html','.csv')), 'w') as csvfile:\n",
    "            faqwriter = csv.writer(csvfile)\n",
    "            for faq in faq_list.faq:\n",
    "                faqwriter.writerow([faq.question, faq.answer])\n",
    "                qna_dict[faq.question] = [faq.answer]\n",
    "                qna_cnt+=1\n",
    "\n",
    "# save the documents in a dict\n",
    "json.dump(course_db, open('./chroma/course_db', 'w'))\n",
    "print('Processed {} documents, with {} sections'.format(docs_procesessed_cnt, section_procesessed_cnt))\n",
    "\n",
    "# split the rest of the documents into chunks that can be handled by embedding model \n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=512, chunk_overlap=256, length_function=len, is_separator_regex=False,)\n",
    "\n",
    "split_docs = text_splitter.split_documents(all_sections)\n",
    "print('Split the {} sections into {} splits'.format(len(all_sections), len(split_docs)))\n",
    "\n",
    "# load split coursework and fat into separate Chroma search db\n",
    "db_dir = \"./chroma/db\"\n",
    "shutil.rmtree(db_dir)\n",
    "os.makedirs(db_dir, exist_ok=True)\n",
    "db = Chroma.from_documents(split_docs, embeddings, collection_name=\"course\", persist_directory=db_dir)\n",
    "print('Loaded the splits to search engine')\n",
    "\n",
    "\n",
    "# save the QnA dict\n",
    "json.dump(qna_dict, open('./chroma/qna_dict', 'w'))\n",
    "db_faq_dir = \"./chroma/db_faq\"\n",
    "shutil.rmtree(db_faq_dir)\n",
    "os.makedirs(db_faq_dir, exist_ok=True)\n",
    "db_faq = Chroma.from_texts(list(qna_dict.keys()), embeddings, collection_name=\"faq\", persist_directory=db_faq_dir)\n",
    "print('Processed {} FAQ documents, with {} QnA pairs, and loaded it to search engine.'.format(faq_cnt, qna_cnt))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ca3c03b-a6ff-4a27-9283-6aadc996b5dd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Quickly test if search is working\n",
    "from langchain.embeddings.huggingface import HuggingFaceEmbeddings\n",
    "from langchain.vectorstores import Chroma\n",
    "\n",
    "query = \"Why use such a simplistic model?\"\n",
    "\n",
    "test_embedding_function = HuggingFaceEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n",
    "test_db = Chroma(persist_directory=\"./chroma/db\", embedding_function=test_embedding_function, collection_name=\"course\")\n",
    "test_db_faq = Chroma(persist_directory=\"./chroma/db_faq\", embedding_function=test_embedding_function, collection_name=\"faq\")\n",
    "\n",
    "results = test_db.similarity_search_with_score(query, k=3)\n",
    "results_faq = test_db_faq.similarity_search_with_score(query, k=3)\n",
    "\n",
    "\n",
    "# search in faq\n",
    "print('Retrieved {} FAQ docs'.format(len(results_faq)))\n",
    "for i, val in enumerate(results_faq):\n",
    "    print('\\nDocument {}'.format(i))\n",
    "    print('Score: {}'.format(val[1])) #lower the score, more relevant is the answer\n",
    "    print('Chapter: {}'.format(val[0].metadata))\n",
    "    print('Section: {}'.format(val[0].page_content))\n",
    "\n",
    "\n",
    "print('-------------------------------------------------------')\n",
    "\n",
    "# search in course work\n",
    "print('Retrieved {} docs'.format(len(results)))\n",
    "for i, val in enumerate(results):\n",
    "    print('\\nDocument {}'.format(i))\n",
    "    print('Score: {}'.format(val[1])) #lower the score, more relevant is the answer\n",
    "    print('Chapter: {}'.format(val[0].metadata))\n",
    "    print('Section: {}'.format(val[0].page_content))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b5f8934-caf4-46ee-8cf7-f53fded3a21b",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# FAQ Bot v1\n",
    "Step1: Search in a list of Qs, to find the nearest Q and personalize the answer with GPT.\n",
    "\n",
    "Step2: If no good FAQ is found, search in the course content and answer with GPT.\n",
    "\n",
    "Step3: If no good answer is found, send a static message  - I dont think I know the answer for this, let me check with the professor.\n",
    "\n",
    "This works fine. But sometimes it feels like the answer could have been well rounded, and the FAQ doesnt have enough context and information. Hence tried V2 below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "828685b3-a630-4646-b166-4071c81cb541",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.schema import (\n",
    "    AIMessage,\n",
    "    HumanMessage,\n",
    "    SystemMessage\n",
    ")\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.output_parsers import PydanticOutputParser, OutputFixingParser\n",
    "from pydantic import BaseModel, Field\n",
    "from enum import Enum\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "class IsAnswerable(Enum):\n",
    "    YES = \"YES - the given 'question' can be confidently answered using the given 'context'\"\n",
    "    NO = \"NO - the given 'question' cannot be answered with the given 'context'\"  \n",
    "\n",
    "class AnswerStatus(BaseModel):\n",
    "    status: IsAnswerable = Field(description=\"\")\n",
    "    answer: str = Field(description=\"answer the student's 'question' based solely on the given 'context'.\")\n",
    "\n",
    "class FAQBot():\n",
    "    def __init__(self):\n",
    "        \n",
    "        self.model = ChatOpenAI(\n",
    "            model_name='gpt-3.5-turbo',\n",
    "            openai_api_key=os.getenv(\"OPENAI_API_KEY\"),\n",
    "            openai_organization=os.getenv(\"OPENAI_ORGANIZATION\"),\n",
    "        )\n",
    "        \n",
    "        self.parser = PydanticOutputParser(pydantic_object=AnswerStatus)\n",
    "        self.fix_parser = OutputFixingParser.from_llm(parser=self.parser, llm=self.model, max_retries=3)\n",
    "        self.prompt = PromptTemplate(\n",
    "            template =  '''\n",
    "                You're a helpful teaching assistant for a technical course on {course}. You will only answer student's 'question' based on the given 'context' of the course.\\n\n",
    "                The 'context' can be noisy, with some unrelated text. Make sure to ignore this unrelated text while answering. \n",
    "                Provide as detailed answer as possible, with sufficient examples that are part of the 'context' text.\n",
    "                \n",
    "                {format_instructions}\\n\n",
    "                \n",
    "                ***\n",
    "                'query' : {question}\n",
    "                ***\n",
    "                \n",
    "                $$$\n",
    "                'context' : {context}\n",
    "                $$$\n",
    "\n",
    "                I am reminding you again, you are a teaching assistant, do not add any facts into the answer that is not given in the 'context'\n",
    "            ''',\n",
    "                input_variables=[\"question\", \"context\", \"course\"],\n",
    "                partial_variables={\n",
    "                    \"format_instructions\": self.parser.get_format_instructions(),\n",
    "                },\n",
    "            )    \n",
    "        self.search_conf_thresh = 1\n",
    "        \n",
    "    def get_completion_from_messages(self, question, verbose=True):\n",
    "        # Check if FAQ has answer.\n",
    "                \n",
    "        ## search in faq\n",
    "        if verbose:\n",
    "            print('Search in FAQ')\n",
    "\n",
    "        results_faq = db_faq.similarity_search_with_score(question, k=3)\n",
    "\n",
    "        ### save only the high confidence search results\n",
    "        if verbose:\n",
    "            print('\\tanswers retrieved')\n",
    "        retrieved_answers = ''\n",
    "        for i, val in enumerate(results_faq):\n",
    "            if verbose:\n",
    "                print('\\t\\ttext: {}\\n\\tconf:{}\\n'.format(val[0].page_content, val[1]))\n",
    "            if val[1] < self.search_conf_thresh: \n",
    "                # collect the corresponding answers of the qna pair for gpt \n",
    "                retrieved_answers += ' Question:{}\\n Answer:{}\\n'.format(val[0].page_content, qna_dict[val[0].page_content])    \n",
    "\n",
    "        #### if there is atleast one search result ask GPT to answer\n",
    "        if len(retrieved_answers):\n",
    "            # ask GPT to answer\n",
    "            prompt_string = self.prompt.format_prompt(question=question, context=retrieved_answers, course = 'Distributed Algorithms').to_string()\n",
    "\n",
    "            if verbose:\n",
    "                print(prompt_string)\n",
    "            response = self.model([\n",
    "                HumanMessage(\n",
    "                    prompt_string\n",
    "                ) \n",
    "            ])\n",
    "\n",
    "            if verbose:\n",
    "                print('\\t\\t\\tRaw GPT response: {}\\n'.format(response))\n",
    "                \n",
    "            faq_response = None\n",
    "            try:\n",
    "                faq_response = self.parser.parse(response.content)\n",
    "            except Exception as e:\n",
    "                faq_response = self.fix_parser.parse(response.content)                \n",
    "\n",
    "            if verbose:\n",
    "                print('\\t\\t\\tfinal response: {}\\n'.format(faq_response))\n",
    "\n",
    "            if faq_response != None and faq_response.status == IsAnswerable.YES:\n",
    "                return faq_response.answer\n",
    "            else:\n",
    "                ## search in coursework\n",
    "                if verbose:\n",
    "                    print('Search in coursework')            \n",
    "                results_faq = db.similarity_search_with_score(question, k=5)\n",
    "        \n",
    "                ### save only the high confidence search results\n",
    "                if verbose:\n",
    "                    print('\\tanswers retrieved')\n",
    "                retrieved_answers = ''\n",
    "                for i, val in enumerate(results_faq):\n",
    "                    if verbose:\n",
    "                        print('\\t\\ttext: {}\\n\\tconf:{}\\n'.format(val[0].page_content, val[1]))\n",
    "                    if val[1] < self.search_conf_thresh: \n",
    "                        retrieved_answers += ' {}'.format(val[0].page_content)    \n",
    "        \n",
    "                #### if there is atleast one search result ask GPT to answer\n",
    "                if len(retrieved_answers):\n",
    "                    # ask GPT to answer\n",
    "                    prompt_string = self.prompt.format_prompt(question=question, context=retrieved_answers, course = 'Distributed Algorithms').to_string()\n",
    "        \n",
    "                    if verbose:\n",
    "                        print(prompt_string)\n",
    "                    response = self.model([\n",
    "                        HumanMessage(\n",
    "                            prompt_string\n",
    "                        ) \n",
    "                    ])\n",
    "        \n",
    "                    if verbose:\n",
    "                        print('\\t\\t\\tRaw GPT response: {}\\n'.format(response))\n",
    "                        \n",
    "                    faq_response = None\n",
    "                    try:\n",
    "                        faq_response = self.parser.parse(response.content)\n",
    "                    except Exception as e:\n",
    "                        faq_response = self.fix_parser.parse(response.content)                \n",
    "        \n",
    "                    if verbose:\n",
    "                        print('\\t\\t\\tfinal response: {}\\n'.format(faq_response))\n",
    "                        \n",
    "                if faq_response != None:\n",
    "                    if faq_response.status == IsAnswerable.YES:\n",
    "                        return faq_response.answer\n",
    "                    else:\n",
    "                        return ''' I dont think I know the answer for this, let me check with the professor.'''\n",
    "                else:\n",
    "                    return ''' I dont think I know the answer for this, let me check with the professor.'''\n",
    "        else:\n",
    "            return ''' I dont think I know the answer for this, let me check with the professor.'''\n",
    "                                \n",
    "        \n",
    "fb = FAQBot( )        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b1a4b74-835c-4505-89ff-519c82baeee7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Have the viva \n",
    "while True:\n",
    "    query = input(\"\\nUser: \").strip()\n",
    "    print()\n",
    "    if query==\"\":\n",
    "        break\n",
    "    answer = fb.get_completion_from_messages(query)\n",
    "    print('bot answer: {}'.format(answer))\n",
    "    print('----------------------\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d2a985f-b3a9-45c0-8dd4-84e02cf2ed25",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# FAQ Bot v2\n",
    "Step1: Search both in FAQ (Search in a list of Qs, to find the nearest Q and personalize the answer with GPT) and coursework.\n",
    "\n",
    "Step2: Answer based on both FAQ and coursework.\n",
    "\n",
    "Step3: If no good answer is found, send a static message  - I dont think I know the answer for this, let me check with the professor.\n",
    "\n",
    "I am not sure if this is better than v1. Need to test with more examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6def94fa-060e-40ac-8b93-b9fb896ab4e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.schema import (\n",
    "    AIMessage,\n",
    "    HumanMessage,\n",
    "    SystemMessage\n",
    ")\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.output_parsers import PydanticOutputParser, OutputFixingParser\n",
    "from pydantic import BaseModel, Field\n",
    "from enum import Enum\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "class IsAnswerable(Enum):\n",
    "    YES = \"YES - the given 'question' can be confidently answered using the given 'context'\"\n",
    "    NO = \"NO - the given 'question' cannot be answered with the given 'context'\"  \n",
    "\n",
    "class AnswerStatus(BaseModel):\n",
    "    status: IsAnswerable = Field(description=\"\")\n",
    "    answer: str = Field(description=\"answer the student's 'question' based solely on the given 'context'.\")\n",
    "\n",
    "class FAQBot():\n",
    "    def __init__(self):\n",
    "        \n",
    "        self.model = ChatOpenAI(\n",
    "            model_name='gpt-3.5-turbo',\n",
    "            openai_api_key=os.getenv(\"OPENAI_API_KEY\"),\n",
    "            openai_organization=os.getenv(\"OPENAI_ORGANIZATION\"),\n",
    "        )\n",
    "\n",
    "        self.parser = PydanticOutputParser(pydantic_object=AnswerStatus)\n",
    "        self.fix_parser = OutputFixingParser.from_llm(parser=self.parser, llm=self.model, max_retries=3)\n",
    "        self.prompt = PromptTemplate(\n",
    "            template =  '''\n",
    "                You're a helpful teaching assistant for a technical course on {course}. You will only answer student's 'question' based on the given 'context' of the course.\\n\n",
    "                The 'context' can be noisy, with some unrelated text. Make sure to ignore this unrelated text while answering. \n",
    "                \n",
    "                Apply chain of thoughts, and think before you answer. The answer you are providing is it really helping the student to understand the concept completely?\n",
    "                \n",
    "                {format_instructions}\\n\n",
    "                \n",
    "                ***\n",
    "                'query' : {question}\n",
    "                ***\n",
    "                \n",
    "                $$$\n",
    "                'context' : {context}\n",
    "                $$$\n",
    "\n",
    "                I am reminding you again, you are a teaching assistant, do not add any facts into the answer that is not given in the 'context'\n",
    "            ''',\n",
    "                input_variables=[\"question\", \"context\", \"course\"],\n",
    "                partial_variables={\n",
    "                    \"format_instructions\": self.parser.get_format_instructions(),\n",
    "                },\n",
    "            )    \n",
    "        self.search_conf_thresh = 1\n",
    "        \n",
    "    def get_completion_from_messages(self, question, verbose=True):\n",
    "        \n",
    "        retrieved_answers = ''\n",
    "        ## search in faq\n",
    "        if verbose:\n",
    "            print('Search in FAQ')\n",
    "        results_faq = db_faq.similarity_search_with_score(question, k=3)\n",
    "\n",
    "        ### save only the high confidence search results\n",
    "        if verbose:\n",
    "            print('\\tanswers retrieved')\n",
    "        \n",
    "        for i, val in enumerate(results_faq):\n",
    "            if verbose:\n",
    "                print('\\t\\ttext: {}\\n\\t\\tChapter: {}\\n\\t\\tconf:{}\\n'.format(val[0].page_content, val[0].metadata, val[1]))\n",
    "\n",
    "            if val[1] < self.search_conf_thresh: \n",
    "                # collect the corresponding answers of the qna pair for gpt \n",
    "                retrieved_answers += ' Question:{}\\n Answer:{}\\n'.format(val[0].page_content, qna_dict[val[0].page_content])    \n",
    "\n",
    "\n",
    "\n",
    "        ## search in coursework\n",
    "        if verbose:\n",
    "            print('Search in coursework')            \n",
    "        results_faq = db.similarity_search_with_score(question, k=5)\n",
    "\n",
    "        ### save only the high confidence search results\n",
    "        if verbose:\n",
    "            print('\\tanswers retrieved')\n",
    "            \n",
    "        for i, val in enumerate(results_faq):\n",
    "            if verbose:\n",
    "                print('\\t\\ttext: {}\\n\\t\\tChapter: {}\\n\\t\\tconf:{}\\n'.format(val[0].page_content, val[0].metadata, val[1]))\n",
    "            if val[1] < self.search_conf_thresh: \n",
    "                retrieved_answers += ' {}'.format(val[0].page_content)    \n",
    "\n",
    "\n",
    "\n",
    "        \n",
    "        #### if there is atleast one search result ask GPT to answer\n",
    "        if len(retrieved_answers):\n",
    "            # ask GPT to answer\n",
    "            prompt_string = self.prompt.format_prompt(question=question, context=retrieved_answers, course = 'Distributed Algorithms').to_string()\n",
    "\n",
    "            if verbose:\n",
    "                print(prompt_string)\n",
    "            response = self.model([\n",
    "                HumanMessage(\n",
    "                    prompt_string\n",
    "                ) \n",
    "            ])\n",
    "\n",
    "            if verbose:\n",
    "                print('\\t\\t\\tRaw GPT response: {}\\n'.format(response))\n",
    "                \n",
    "            faq_response = None\n",
    "            try:\n",
    "                faq_response = self.parser.parse(response.content)\n",
    "            except Exception as e:\n",
    "                faq_response = self.fix_parser.parse(response.content)                \n",
    "\n",
    "            if verbose:\n",
    "                print('\\t\\t\\tfinal response: {}\\n'.format(faq_response))\n",
    "\n",
    "            if faq_response != None and faq_response.status == IsAnswerable.YES:\n",
    "                return faq_response.answer\n",
    "            else:\n",
    "                return ''' I dont think I know the answer for this, let me check with the professor.'''\n",
    "        else:\n",
    "            return ''' I dont think I know the answer for this, let me check with the professor.'''\n",
    "                                \n",
    "        \n",
    "fb = FAQBot( )        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d908b4cd-80a8-4128-ba89-f838bfa917b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Have the viva \n",
    "while True:\n",
    "    query = input(\"\\nUser: \").strip()\n",
    "    print()\n",
    "    if query==\"\":\n",
    "        break\n",
    "    answer = fb.get_completion_from_messages(query)\n",
    "    print('bot answer: {}'.format(answer))\n",
    "    print('----------------------\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c02ec77-15b1-4bac-8509-0849db2b7da1",
   "metadata": {},
   "source": [
    "# FAQ Bot v3\n",
    "\n",
    "Step1: Search both in FAQ (Search in a list of Qs, to find the nearest Q and personalize the answer with GPT) and coursework.\n",
    "\n",
    "Step2: Answer based on both FAQ and coursework. Unlike v1 and v2, as the output of coursework search we use the extended version of the snippets, ie, we extend the context by ~512 characters from either ends of all the retrieved snippet. This is to give GPT more context to answer. \n",
    "#TODO: This extended context still has unrelated text, since we are randomly choosing ~512 characters. In the future, before indexing the coursework, we need to parse the documents using offline cheaper models, to identify sections and sub-sections in a document, and use the entire section/sub-section as the context, instead of chunks with random start and end index. During this preprocessing, we can also identify tables/images relevant to these sections, and show them when we generate an answer from the relevant section.\n",
    "\n",
    "Step3: If no good answer is found, send a static message  - I dont think I know the answer for this, let me check with the professor.\n",
    "\n",
    "I am not sure if this is better than v1. Need to test with more examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53ad5f69-05a7-4b86-84e7-73f71228988f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.schema import (\n",
    "    AIMessage,\n",
    "    HumanMessage,\n",
    "    SystemMessage\n",
    ")\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.output_parsers import PydanticOutputParser, OutputFixingParser\n",
    "from pydantic import BaseModel, Field\n",
    "from enum import Enum\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.embeddings.huggingface import HuggingFaceEmbeddings\n",
    "from langchain.vectorstores import Chroma\n",
    "import json\n",
    "\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "class IsAnswerable(Enum):\n",
    "    YES = \"YES - the given 'question' can be confidently answered using the given 'context'\"\n",
    "    NO = \"NO - the given 'question' cannot be answered with the given 'context'\"  \n",
    "\n",
    "class AnswerStatus(BaseModel):\n",
    "    status: IsAnswerable = Field(description=\"\")\n",
    "    answer: str = Field(description=\"answer the student's 'question' based solely on the given 'context'. Answer only in HTML format, include images if relevant, and use math style for equations. \")\n",
    "\n",
    "class FAQBot():\n",
    "    def __init__(self):\n",
    "        \n",
    "        self.model = ChatOpenAI(\n",
    "            model_name='gpt-3.5-turbo',\n",
    "            openai_api_key=os.getenv(\"OPENAI_API_KEY\"),\n",
    "            openai_organization=os.getenv(\"OPENAI_ORGANIZATION\"),\n",
    "        )\n",
    "\n",
    "        embedding_function = HuggingFaceEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n",
    "        self.db = Chroma(persist_directory=\"./chroma/db\", embedding_function=embedding_function, collection_name=\"course\")\n",
    "        self.db_faq = Chroma(persist_directory=\"./chroma/db_faq\", embedding_function=embedding_function, collection_name=\"faq\")\n",
    "        self.qna_dict = json.load(open('./chroma/qna_dict'))\n",
    "        self.course_db = json.load(open('./chroma/course_db'))\n",
    "\n",
    "        self.parser = PydanticOutputParser(pydantic_object=AnswerStatus)\n",
    "        self.fix_parser = OutputFixingParser.from_llm(parser=self.parser, llm=self.model, max_retries=3)\n",
    "        self.prompt = PromptTemplate(\n",
    "            template =  '''\n",
    "                You're a helpful teaching assistant for a technical course on {course}. You will only answer student's 'question' based on the given 'context' of the course.\\n\n",
    "                The 'context' is a combination of two things - 1) Previous question and answers on the {course} that are similar to the student's question, and 2) some snippets of text from the course contents that are relevant to the student's 'question'.\n",
    "                \n",
    "                {format_instructions}\\n\n",
    "                \n",
    "                ***\n",
    "                'query' : {question}\n",
    "                ***\n",
    "                \n",
    "                $$$\n",
    "                'context' : {context}\n",
    "                $$$\n",
    "                I am reminding you again, you are a teaching assistant, do not add any facts into the answer that is not given in the 'context'. \n",
    "                Answer only in HTML format, include images if relevant, and use math style for equations.\n",
    "            ''',\n",
    "                input_variables=[\"question\", \"context\", \"course\"],\n",
    "                partial_variables={\n",
    "                    \"format_instructions\": self.parser.get_format_instructions(),\n",
    "                },\n",
    "            )    \n",
    "        self.search_conf_thresh = 1\n",
    "        self.excuse_me_msg = '''<p>I dont think I know the answer for this, let me check with the professor.</p>'''\n",
    "        \n",
    "    def ask_question(self, question, verbose=False):\n",
    "        \n",
    "        retrieved_answers = ''\n",
    "        ## search in faq\n",
    "        if verbose:\n",
    "            print('Search in FAQ')\n",
    "        results_faq = self.db_faq.similarity_search_with_score(question, k=3)\n",
    "\n",
    "        ### save only the high confidence search results\n",
    "        if verbose:\n",
    "            print('\\tanswers retrieved')\n",
    "\n",
    "        is_faq_title_printed = False\n",
    "        for i, val in enumerate(results_faq):\n",
    "            if verbose:\n",
    "                print('\\t\\ttext: {}\\n\\t\\tChapter: {}\\n\\t\\tconf:{}\\n'.format(val[0].page_content, val[0].metadata, val[1]))\n",
    "\n",
    "            if val[1] < self.search_conf_thresh: \n",
    "                if not is_faq_title_printed:\n",
    "                    retrieved_answers += '''Question and Answers from the past that are similar to the student's question\\n-----------------\\n'''\n",
    "                    is_faq_title_printed = True\n",
    "                # collect the corresponding answers of the qna pair for gpt \n",
    "                retrieved_answers += ' Question:{}\\n Answer:{}\\n'.format(val[0].page_content, self.qna_dict[val[0].page_content])    \n",
    "\n",
    "\n",
    "\n",
    "        ## search in coursework\n",
    "        if verbose:\n",
    "            print('Search in coursework')            \n",
    "        results = self.db.similarity_search_with_score(question, k=5)\n",
    "\n",
    "        ### save only the high confidence search results\n",
    "        if verbose:\n",
    "            print('\\tanswers retrieved')\n",
    "\n",
    "        is_snippet_title_printed = False\n",
    "        max_chapters = 3\n",
    "        neighboring_sections = 2 # + or -\n",
    "        len_extended_text_context = 512\n",
    "        chapter_cnt = 0\n",
    "        seen_chapters = []\n",
    "        for i, val in enumerate(results):\n",
    "            if verbose:\n",
    "                print('\\t\\ttext: {}\\n\\t\\tChapter: {}\\n\\t\\tSection: {}\\n\\t\\tconf:{}\\n'.format(val[0].page_content, val[0].metadata['source'], val[0].metadata['split'], val[1]))\n",
    "                print(self.course_db[val[0].metadata['source']].keys())\n",
    "            if val[1] < self.search_conf_thresh: \n",
    "                if not is_snippet_title_printed:\n",
    "                    retrieved_answers += '''\\n$$$$$$$$$$\\nSnippets of text from the course that are relevant to the student's question\\n-----------------\\n'''\n",
    "                    is_snippet_title_printed = True\n",
    "\n",
    "                if val[0].metadata['source'] not in seen_chapters and chapter_cnt<max_chapters:\n",
    "                    \n",
    "                    html_str = self.course_db[val[0].metadata['source']][str((val[0].metadata['split']))]\n",
    "                    extended_context = ''\n",
    "                    for ind in range(val[0].metadata['split']-neighboring_sections, val[0].metadata['split']+neighboring_sections):\n",
    "                        if str(ind) in self.course_db[val[0].metadata['source']]:\n",
    "                            extended_context += '\\n{}'.format(self.course_db[val[0].metadata['source']][str(ind)])\n",
    "                    \n",
    "                    retrieved_answers += '\\n Relevant text snippet {}: {}\\n\\n '.format(chapter_cnt, extended_context) \n",
    "                    if verbose:\n",
    "                        print('\\n\\t\\tlength:({}, {})'.format(len(html_str), len(extended_context)))\n",
    "\n",
    "                    seen_chapters.append(val[0].metadata['source'])\n",
    "                    chapter_cnt += 1\n",
    "                    if len(retrieved_answers)>2000:\n",
    "                        if verbose:\n",
    "                            print('retrieved_answers length greater than 2000 : {}'.format(len(retrieved_answers)))\n",
    "                        break\n",
    "                    \n",
    "\n",
    "\n",
    "\n",
    "        \n",
    "        #### if there is atleast one search result ask GPT to answer\n",
    "        if len(retrieved_answers):\n",
    "            # ask GPT to answer\n",
    "            prompt_string = self.prompt.format_prompt(question=question, context=retrieved_answers, course = 'Distributed Algorithms').to_string()\n",
    "\n",
    "            if verbose:\n",
    "                print(prompt_string)\n",
    "            response = self.model([\n",
    "                HumanMessage(\n",
    "                    prompt_string\n",
    "                ) \n",
    "            ])\n",
    "\n",
    "            if verbose:\n",
    "                print('\\t\\t\\tRaw GPT response: {}\\n'.format(response))\n",
    "                \n",
    "            faq_response = None\n",
    "            try:\n",
    "                faq_response = self.parser.parse(response.content)\n",
    "            except Exception as e:\n",
    "                faq_response = self.fix_parser.parse(response.content)                \n",
    "\n",
    "            if verbose:\n",
    "                print('\\t\\t\\tfinal response: {}\\n'.format(faq_response))\n",
    "\n",
    "            if faq_response != None and faq_response.status == IsAnswerable.YES:\n",
    "                return faq_response.answer\n",
    "            else:\n",
    "                return self.excuse_me_msg\n",
    "        else:\n",
    "            return self.excuse_me_msg\n",
    "                                \n",
    "        \n",
    "fb = FAQBot( )        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a51b9bde-1fba-4385-b6a9-f98d3cb60720",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Have the viva \n",
    "while True:\n",
    "    query = input(\"\\nUser: \").strip()\n",
    "    print()\n",
    "    if query==\"\":\n",
    "        break\n",
    "    answer = fb.ask_question(query)\n",
    "    print('bot answer: {}'.format(answer))\n",
    "    print('----------------------\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "815821a9-6f79-41a1-a128-375aef2b84f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# html_filename = 'tutorial/mani/DISTRIBUTED_SYSTEM_MODELS/DistributedSystemModels.html'\n",
    "html_filename = 'tutorial/mani/Byzantine/ByzantineOral.html'"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
